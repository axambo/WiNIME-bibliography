@inproceedings{Mase2001,
abstract = {In this paper, we introduce our research challenges for creating new musical instruments using everyday-life media with intimate interfaces, such as the self-body, clothes, water and stuffed toys. Various sensor technologies including image processing and general touch sensitive devices are employed to exploit these interaction media. The focus of our effort is to provide user-friendly and enjoyable experiences for new music and sound performances. Multimodality of musical instruments is explored in each attempt. The degree of controllability in the performance and the richness of expressions are also discussed for each installation.},
address = {Seattle, WA},
author = {Mase, Kenji and Yonezawa, Tomoko},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {New interface,dance,image processing,music controller,stuffed toy,water interface},
pages = {34--37},
title = {{Body , Clothes , Water and Toys : Media Towards Natural Music Expressions with Digital Sounds}},
url = {http://www.nime.org/proceedings/2001/nime2001{\_}034.pdf},
year = {2001}
}
@inproceedings{McElligott:2002,
address = {Dublin, Ireland},
author = {McElligott, Lisa and Dixon, Edward and Dillon, Michelle},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {3D sensing pressure,Gesture,control device,effort,expression,force,input.,intent,movement,music,resolution,sensor,sound,weight distribution},
pages = {126--130},
title = {{`PegLegs in Music' Processing the Effort Generated by Levels of Expressive Gesturing in Music}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}126.pdf},
year = {2002}
}
@inproceedings{Bongers:2002,
address = {Dublin, Ireland},
author = {Bongers, Bert and Harris, Yolande},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
pages = {18--23},
title = {{A Structured Instrument Design Approach: The Video-Organ}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}018.pdf},
year = {2002}
}
@inproceedings{Pardue:2002,
abstract = {Passive RF Tagging can provide an attractive medium for development of free-gesture musical interfaces. This was initially explored in our Musical Trinkets installation, which used magnetically-coupled resonant LC circuits to identify and track the position of multiple objects in real-time. Manipulation of these objects in free space over a read coil triggered simple musical interactions. Musical Navigatrics builds upon this success with new more sensitive and stable sensing, multi-dimensional response, and vastly more intricate musical mappings that enable full musical exploration of free space through the dynamic use and control of arpeggiatiation and effects. The addition of basic sequencing abilities also allows for the building of complex, layered musical interactions in a uniquely easy and intuitive manner.},
address = {Dublin, Ireland},
author = {Pardue, Laurel S and Paradiso, Joseph A},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {music sequencer interface,passive tag,position tracking},
pages = {145--147},
title = {{Musical Navigatrics: New Musical Interactions with Passive Magnetic Tags}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}145.pdf},
year = {2002}
}
@inproceedings{Gunther:2002,
abstract = {This paper presents a novel coupling of haptics technology and music, introducing the notion of tactile composition or aesthetic composition for the sense of touch. A system that facilitates the composition and perception of intricate, musically structured spatio-temporal patterns of vibration on the surface of the body is described. An initial test of the system in a performance context is discussed. The fundamental building blocks of a compositional language for touch are considered.},
address = {Dublin, Ireland},
author = {Gunther, Eric and Davenport, Glorianna and O'Modhrain, Sile},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {multi-modal,music,tactile composition,vibrotactile},
pages = {73--79},
title = {{Cutaneous Grooves: Composing for the Sense of Touch}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}073.pdf},
year = {2002}
}
@inproceedings{Smyth:2002,
abstract = {The cicada uses a rapid sequence of buckling ribs to initiate and sustain vibrations in its tymbal plate (the primary mechanical resonator in the cicada's sound production system). The tymbalimba, a music controller based on this same mechanism, has a row of 4 convex aluminum ribs (ason the cicada's tymbal) arranged much like the keys on a calimba. Each rib is spring loaded and capable of snapping down into a V-shape (a motion referred to as buckling), under the downward force of the user's finger. This energy generated by the buckling motion is measured by an accelerometer located under each rib and used as the input to a physical model.},
address = {Dublin, Ireland},
author = {Smyth, Tamara and Smith, Julius O},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Bioacoustics,Buckling mechanism.,Cicada,Controllers,Physical Modeling},
pages = {24--27},
title = {{Creating Sustained Tones with the Cicada's Rapid Sequential Buckling Mechanism}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}161.pdf},
year = {2002}
}
@inproceedings{Hasan:2002,
abstract = {We have created a new electronic musical instrument, referred to as the Termenova (Russian for "daughter of Theremin") that combines a free-gesture capacitive sensing device with an optical sensing system that detects the reflection of a hand when it intersects a beam of an array of red lasers. The laser beams, which are made visible by a thin layer of theatrical mist, provide visual feedback and guidance to the performer to alleviate the difficulties of using a non-contact interface as well as adding an interesting component for the audience to observe. The system uses capacitive sensing to detect the proximity of the player's hands; this distance is mapped to pitch, volume, or other continuous effect. The laser guide positions are calibrated before play with position controlled servo motors interfaced to a main controller board; the location of each beam corresponds to the position where the performer should move his or her hand to achieve a pre-specified pitch and/or effect. The optical system senses the distance of the player's hands from the source of each laser beam, providing an additional dimension of musical control.},
address = {Dublin, Ireland},
author = {Hasan, Leila and Yu, Nicholas and Paradiso, Joseph A},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Theremin,capacitive sensing,gesture interface,laser harp,musical controller,optical proximity sensing,servo control},
pages = {82--87},
title = {{The Termenova : A Hybrid Free-Gesture Interface}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}082.pdf},
year = {2002}
}
@inproceedings{Marshall:2002,
abstract = {This paper introduces a subtle interface, which evolved from the design of an alternative gestural controller in the development of a performance interface. The conceptual idea used is based on that of the traditional Bodhran instrument, an Irish frame drum. The design process was user-centered and involved professional Bodhran players and through prototyping and user testing the resulting Vodhran emerged.},
address = {Dublin, Ireland},
author = {Marshall, Mark T and Rath, Matthias and Moynihan, Breege},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Virtual instrument,gesture,sound modeling,user-centered design},
pages = {118--119},
title = {{The Virtual Bodhran -- The Vodhran}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}118.pdf},
year = {2002}
}
@inproceedings{Blaine:2002,
abstract = {This paper discusses the Jam-O-Drum multi-player musical controller and its adaptation into a gaming controller interface known as the Jam-O-Whirl. The Jam-O-World project positioned these two controller devices in a dedicated projection environment that enabled novice players to participate in immersive musical gaming experiences. Players' actions, detected via embedded sensors in an integrated tabletop surface, control game play, real-time computer graphics and musical interaction. Jam-O-World requires physical and social interaction as well as collaboration among players.},
address = {Dublin, Ireland},
author = {Blaine, Tina and Forlines, Clifton},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Collaboration,computer graphics,embedded sensors,gaming controller,immersive musical gaming experiences,multi-player,musical controller,novice,social interaction.},
pages = {12--17},
title = {{JAM-O-WORLD: Evolution of the Jam-O-Drum Multi-player Musical Controller into the Jam-O-Whirl Gaming Interface}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}012.pdf},
year = {2002}
}
@inproceedings{Wilkerson:2002,
abstract = {The Mutha Rubboard is a musical controller based on the rubboard, washboard or frottoir metaphor commonly used in the Zydeco music genre of South Louisiana. It is not onlya metamorphosis of a traditional instrument, but a modern bridge of exploration into a rich musical heritage. It uses capacitive and piezo sensing technology to output MIDI and raw audio data.This new controller reads the key placement in two parallel planes by using radio capacitive sensing circuitry expanding greatly on the standard corrugated metal playing surface. The percussive output normally associated with the rubboard is captured through piezo contact sensors mounted directly on the keys (the playing implements). Additionally,mode functionality is controlled by discrete switching on the keys.This new instrument is meant to be easily played by both experienced players and those new to the rubboard. It lends itself to an expressive freedom by placing the control surface on the chest and allowing the hands to move uninhibited about it or by playing it in the usual way, preserving its musical heritage.},
address = {Dublin, Ireland},
author = {Wilkerson, Carr and Serafin, Stefania and Ng, Carmen},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Louisiana heritage,MIDI controllers,Zydeco music,bowl resonators.,computer music,electronic musical instrument,human computer interface,interactive music,physical modeling},
pages = {195--198},
title = {{The Mutha Rubboard Controller}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}195.pdf},
year = {2002}
}
@inproceedings{Young:2002,
address = {Dublin, Ireland},
author = {Young, Diana},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Hyperbow,Hyperinstrument,Hyperviolin,accelerometer,bow,position sensor,strain sensor,violin},
pages = {201--206},
title = {{The Hyperbow Controller: Real-Time Dynamics Measurement of Violin Performance}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}201.pdf},
year = {2002}
}
@inproceedings{Baumann:2002,
abstract = {In this paper we will have a short overview of some of the systems we have been developing as an independent company over the last years. We will focus especially on our latest experiments in developing wireless gestural systems using the camera as an interactive tool to generate 2D and 3D visuals and music.},
address = {Dublin, Ireland},
author = {Baumann, Alain and S{\'{a}}nchez, Rosa},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {interdisciplinary applications of,mixed media instruments},
pages = {5--9},
title = {{Interdisciplinary Applications of New Instruments}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}005.pdf},
year = {2002}
}
@inproceedings{Griffith:2002,
abstract = {The use of free gesture in making music has usually been confined to instruments that use direct mappings between movement and sound space. Here we demonstrate the use of categories of gesture as the basis of musical learning and performance collaboration. These are used in a system that reinterprets the approach to learning through performance that is found in many musical cultures and discussed here through the example of Kpelle music.},
address = {Dublin, Ireland},
author = {Griffith, Niall J and O'Leary, Sean and O'Shea, Donagh and Hammond, Ed and O'Modhrain, Sile},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Collaboration,Gesture,Metaphor,Performance},
pages = {71--72},
title = {{Circles and Seeds: Adapting Kpelle Ideas about Music Performance for Collaborative Digital Music performance}},
url = {http://www.nime.org/proceedings/2002/nime2002{\_}071.pdf},
year = {2002}
}
@inproceedings{Young2003a,
address = {Montreal, Canada},
author = {Young, Diana and Serafin, Stefania},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
pages = {104--108},
title = {{Playability Evaluation of a Virtual Bowed String Instrument}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}104.pdf},
year = {2003}
}
@inproceedings{Nishimoto2003,
abstract = {In this paper, we discuss a design principle for the musicalinstruments that are useful for both novices and professionalmusicians and that facilitate musically rich expression. Webelieve that the versatility of conventional musicalinstruments causes difficulty in performance. By dynamicallyspecializing a musical instrument for performing a specific(genre of) piece, the musical instrument could become moreuseful for performing the piece and facilitates expressiveperformance. Based on this idea, we developed two new typesof musical instruments, i.e., a "given-melody-based musicalinstrument" and a "harmonic-function-based musicalinstrument." From the experimental results using twoprototypes, we demonstrate the efficiency of the designprinciple.},
address = {Montreal, Canada},
author = {Nishimoto, Kazushi and Oshima, Chika and Miyagawa, Yohei},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
pages = {164--169},
title = {{Why Always Versatile? Dynamically Customizable Musical Instruments Facilitate Expressive Performances}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}164.pdf},
year = {2003}
}
@inproceedings{Cadoz2003,
address = {Montreal, Canada},
author = {Cadoz, Claude and Luciani, Annie and Florens, Jean-Loup and Castagn{\'{e}}, Nicolas},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
pages = {235--246},
title = {{{\{}AC{\}}ROE - {\{}ICA{\}} Artistic Creation and Computer Interactive Multisensory Simulation Force Feedback Gesture Transducers}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}235.pdf},
year = {2003}
}
@inproceedings{Hewitt2003,
address = {Montreal, Canada},
author = {Hewitt, Donna and Stevenson, Ian},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Alternate controller,electronic music.,gesture,microphone technique,performance interface,vocal performance},
pages = {122--128},
title = {{E-mic: Extended Mic-stand Interface Controller}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}122.pdf},
year = {2003}
}
@inproceedings{Traube2003,
address = {Montreal, Canada},
author = {Traube, Caroline and Depalle, Philippe and Wanderley, Marcelo M},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
pages = {42--47},
title = {{Indirect Acquisition of Instrumental Gesture Based on Signal , Physical and Perceptual Information}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}042.pdf},
year = {2003}
}
@inproceedings{Young2003,
address = {Montreal, Canada},
author = {Young, Diana and Essl, Georg},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
pages = {9--14},
title = {{HyperPuja: A Tibetan Singing Bowl Controller}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}009.pdf},
year = {2003}
}
@inproceedings{Baalman2003,
abstract = {The STRIMIDILATOR is an instrument that uses the deviation and the vibration of strings as MIDI-controllers. Thismethod of control gives the user direct tactile force feedbackand allows for subtle control. The development of the instrument and its different functions are described.},
address = {Montreal, Canada},
author = {Baalman, Marije A},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {MIDI controllers,strings. Figure The STRIMIDILATOR,tactile force feedback},
pages = {19--23},
title = {{The STRIMIDILATOR: a String Controlled MIDI-Instrument}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}019.pdf},
year = {2003}
}
@inproceedings{Gaye2003,
abstract = {In the project Sonic City, we have developed a system thatenables users to create electronic music in real time by walkingthrough and interacting with the urban environment. Weexplore the use of public space and everyday behaviours forcreative purposes, in particular the city as an interface andmobility as an interaction model for electronic music making.A multi-disciplinary design process resulted in theimplementation of a wearable, context-aware prototype. Thesystem produces music by retrieving information aboutcontext and user action and mapping it to real-time processingof urban sounds. Potentials, constraints, and implications ofthis type of music creation are discussed.},
address = {Montreal, Canada},
author = {Gaye, Lalya and Maz{\'{e}}, Ramia and Holmquist, Lars E},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
pages = {109--115},
title = {{Sonic City: The Urban Environment as a Musical Interface}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}109.pdf},
year = {2003}
}
@inproceedings{Shiraiwa2003,
address = {Montreal, Canada},
author = {Terasawa, Hiroko and Segnini, Rodrigo and Woo, Vivian},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Applied chemistry,Battery Controller.,Chemical music},
pages = {83--86},
title = {{Sound Kitchen: Designing a Chemically Controlled Musical Performance}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}083.pdf},
year = {2003}
}
@inproceedings{Cannon2003,
abstract = {In this paper we present a design for the EpipE, a newexpressive electronic music controller based on the IrishUilleann Pipes, a 7-note polyphonic reeded woodwind. Thecore of this proposed controller design is a continuouselectronic tonehole-sensing arrangement, equally applicableto other woodwind interfaces like those of the flute, recorder orJapanese shakuhachi. The controller will initially be used todrive a physically-based synthesis model, with the eventualgoal being the development of a mapping layer allowing theEpipE interface to operate as a MIDI-like controller of arbitrarysynthesis models.},
address = {Montreal, Canada},
author = {Cannon, Cormac and Hughes, Stephen and O'Modhrain, Sile},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Controllers,Irish bagpipe,conical bore,continuous woodwind tonehole sensor,double reed,physical modelling,tonehole.,uilleann pipes},
pages = {3--8},
title = {{EpipE: Exploration of the Uilleann Pipes as a Potential Controller for Computer-based Music}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}003.pdf},
year = {2003}
}
@inproceedings{Nakra2003,
abstract = {This paper describes the artistic projects undertaken at ImmersionMusic, Inc. (www.immersionmusic.org) during its three-yearexistence. We detail work in interactive performance systems,computer-based training systems, and concert production.},
address = {Montreal, Canada},
author = {Nakra, Teresa M},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Conductor's Jacket,Digital Baton,Interactive computer music systems,gestural interaction},
pages = {151--152},
title = {{Immersion Music: a Progress Report}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}151.pdf},
year = {2003}
}
@inproceedings{Blaine2003,
abstract = {We explore a variety of design criteria applicable to thecreation of collaborative interfaces for musical experience. Themain factor common to the design of most collaborativeinterfaces for novices is that musical control is highlyrestricted, which makes it possible to easily learn andparticipate in the collective experience. Balancing this tradeoff is a key concern for designers, as this happens at theexpense of providing an upward path to virtuosity with theinterface. We attempt to identify design considerationsexemplified by a sampling of recent collaborative devicesprimarily oriented toward novice interplay. It is our intentionto provide a non-technical overview of design issues inherentin configuring multiplayer experiences, particularly for entrylevel players.},
address = {Montreal, Canada},
author = {Blaine, Tina and Fels, Sidney},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Design,collaborative interface,multiplayer,musical control.,musical experience,novice},
pages = {129--134},
title = {{Contexts of Collaborative Musical Experiences}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}129.pdf},
year = {2003}
}
@inproceedings{PalacioQuintin2003,
address = {Montreal, Canada},
author = {Palacio-Quintin, Cl{\'{e}}o},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
pages = {206--207},
title = {{The Hyper-Flute}},
url = {http://www.nime.org/proceedings/2003/nime2003{\_}206.pdf},
year = {2003}
}
@inproceedings{Nelson:2004,
address = {Hamamatsu, Japan},
author = {Nelson, Mark and Thom, Belinda},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
pages = {35--38},
title = {{A Survey of Real-Time MIDI Performance}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}035.pdf},
year = {2004}
}
@inproceedings{Takahata:2004,
address = {Hamamatsu, Japan},
author = {Takahata, Masami and Shiraki, Kensuke and Sakane, Yutaka and Takebayashi, Yoichi},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Karate,Learning environment,Sound feedback,Wearable device},
pages = {13--18},
title = {{Sound Feedback for Powerful Karate Training}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}013.pdf},
year = {2004}
}
@inproceedings{Won:2004,
abstract = {In this paper, we describe a new MIDI controller, the LightPipes. The Light Pipes are a series of pipes that respond toincident light. The paper will discuss the design of theinstrument, and the prototype we built. A piece was composedfor the instrument using algorithms designed in Pure Data.},
address = {Hamamatsu, Japan},
author = {Won, Sook Y and Chan, Humane and Liu, Jeremy},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Controllers,MIDI,Pure Data.,light sensors},
pages = {209--210},
title = {{Light Pipes: A Light Controlled MIDI Instrument}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}209.pdf},
year = {2004}
}
@inproceedings{Shatin:2004,
abstract = {In this report, we discuss Tree Music, an interactive computermusic installation created using GAIA (Graphical Audio InterfaceApplication), a new open-source interface for controlling theRTcmix synthesis and effects processing engine. Tree Music,commissioned by the University of Virginia Art Museum, used awireless camera with a wide-angle lens to capture motion andocclusion data from exhibit visitors. We show how GAIA wasused to structure and navigate the compositional space, and howthis program supports both graphical and text-based programmingin the same application. GAIA provides a GUI which combinestwo open-source applications: RTcmix and Perl.},
address = {Hamamatsu, Japan},
author = {Shatin, Judith and Topper, David},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Composition,GUI controllers,Real time audio,interactive systems,new interfaces,open source,video tracking},
pages = {51--54},
title = {{Tree Music: Composing with GAIA}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}051.pdf},
year = {2004}
}
@inproceedings{Morris:2004,
abstract = {This paper describes the SillyTone Squish Factory, a haptically engaging musical interface. It contains the motivation behind the device's development, a description of the interface, various mappings of the interface to musical applications, details of its construction, and the requirements to demo the interface.},
address = {Hamamatsu, Japan},
author = {Morris, Geoffrey C and Leitman, Sasha and Kassianidou, Marina},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
pages = {201--202},
title = {{SillyTone Squish Factory}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}201.pdf},
year = {2004}
}
@inproceedings{OModhrain:2004,
abstract = {The PebbleBox and the CrumbleBag are examples of a granular interaction paradigm, in which the manipulation ofphysical grains of arbitrary material becomes the basis forinteracting with granular sound synthesis models. The soundsmade by the grains as they are manipulated are analysed,and parameters such as grain rate, grain amplitude andgrain density are extracted. These parameters are then usedto control the granulation of arbitrary sound samples in realtime. In this way, a direct link is made between the haptic sensation of interacting with grains and the control ofgranular sounds.},
address = {Hamamatsu, Japan},
author = {O'Modhrain, Sile and Essl, Georg},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Musical instrument,granular synthesis,haptic},
pages = {74--79},
title = {{PebbleBox and CrumbleBag: Tactile Interfaces for Granular Synthesis}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}074.pdf},
year = {2004}
}
@inproceedings{Silva:2004,
abstract = {We describe a simple, computationally light, real-time system for tracking the lower face and extracting informationabout the shape of the open mouth from a video sequence.The system allows unencumbered control of audio synthesismodules by action of the mouth. We report work in progressto use the mouth controller to interact with a physical modelof sound production by the avian syrinx.},
address = {Hamamatsu, Japan},
author = {de Silva, Gamhewage C and Smyth, Tamara and Lyons, Michael J},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Bioacoustics,Face Tracking,Mouth Controller},
pages = {169--172},
title = {{A Novel Face-tracking Mouth Controller and its Application to Interacting with Bioacoustic Models}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}169.pdf},
year = {2004}
}
@inproceedings{Serafin:2004,
address = {Hamamatsu, Japan},
author = {Serafin, Stefania and Young, Diana},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
pages = {108--111},
title = {{Toward a Generalized Friction Controller: from the Bowed String to Unusual Musical Instruments}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}108.pdf},
year = {2004}
}
@inproceedings{Hashida:2004,
abstract = {This paper proposes an interface for improvisational ensemble plays which synthesizes musical sounds and graphical images on the floor from people's act of "walking." The aim of this paper is to develop such a system that enables nonprofessional people in our public spaces to play good contrapuntal music without any knowledge of music theory. The people are just walking. This system is based on the i-trace system [1] which can capture the people's behavior and give some visual feedback.},
address = {Hamamatsu, Japan},
author = {Hashida, Tomoko and Kakehi, Yasuaki and Naemura, Takeshi},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Contrapuntal Music,Human Tracking,Improvisational Ensemble Play,Spatially Augmented Reality,Traces},
pages = {215--216},
title = {{Ensemble System with i-trace}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}215.pdf},
year = {2004}
}
@inproceedings{Gaye:2004,
abstract = {Sonic City is a wearable system enabling the use of the urban environment as an interface for real-time electronic music making, when walking through and interacting with a city. The device senses everyday interactions and surrounding contexts, and maps this information in real time to the sound processing of urban sounds. We conducted a short-term study with various participants using our prototype in everyday settings. This paper describes the course of the study and preliminary results in terms of how the participants used and experienced the system. These results showed that the city was perceived as the main performer but that the user improvised different tactics and ad hoc interventions to actively influence and participate in how the music was created.},
address = {Hamamatsu, Japan},
author = {Gaye, Lalya and Holmquist, Lars E},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {User study,context-awareness.,interactive music,mobility,new interface for musical expression,wearable computing},
pages = {161--164},
title = {{In Duet with Everyday Urban Settings: A User Study of Sonic City}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}161.pdf},
year = {2004}
}
@inproceedings{Fels:2004,
abstract = {The Tooka was created as an exploration of two personinstruments. We have worked with two Tooka performers toenhance the original experimental device to make a musicalinstrument played and enjoyed by them. The main additions tothe device include: an additional button that behaves as amusic capture button, a bend sensor, an additional thumbactuated pressure sensor for vibrato, additional musicalmapping strategies, and new interfacing hardware. Thesedevelopments a rose through exper iences andrecommendations from the musicians playing it. In addition tothe changes to the Tooka, this paper describes the learningprocess and experiences of the musicians performing with theTooka.},
address = {Hamamatsu, Japan},
author = {Fels, Sidney and Kaastra, Linda and Takahashi, Sachiyo and Mccaig, Graeme},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Musician-centred design,two-person musical instrument.},
pages = {1--6},
title = {{Evolving Tooka: from Experiment to Instrument}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}001.pdf},
year = {2004}
}
@inproceedings{Young:2004,
address = {Hamamatsu, Japan},
author = {Young, Diana and Fujinaga, Ichiro},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {bluetooth,drum stick,japanese drum,taiko,wireless},
pages = {23--26},
title = {{AoBachi: A New Interface for {\{}Japan{\}}ese Drumming}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}023.pdf},
year = {2004}
}
@inproceedings{Hornof:2004,
abstract = {Though musical performers routinely use eye movements to communicate with each other during musical performances, very few performers or composers have used eye tracking devices to direct musical compositions and performances. EyeMusic is a system that uses eye movements as an input to electronic music compositions. The eye movements can directly control the music, or the music can respond to the eyes moving around a visual scene. EyeMusic is implemented so that any composer using established composition software can incorporate prerecorded eye movement data into their musical compositions.},
address = {Hamamatsu, Japan},
author = {Hornof, Anthony J and Sato, Linda},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Electronic music composition,Max/MSP.,eye movements,eye tracking,human-computer interaction},
pages = {185--188},
title = {{EyeMusic: Making Music with the Eyes}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}185.pdf},
year = {2004}
}
@inproceedings{Havel:2004,
abstract = {This paper presents a project involving a percussionist playing on a virtual percussion. Both artistic and technical aspects of the project are developed. Especially, a method forstrike recognition using the Flock of Birds is presented, aswell as its use for artistic purpose.},
address = {Hamamatsu, Japan},
author = {Havel, Christophe and Desainte-Catherine, Myriam},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Gesture analysis,strike recognition.,virtual percussion},
pages = {31--34},
title = {{Modeling an Air Percussion for Composition and Performance}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}031.pdf},
year = {2004}
}
@inproceedings{Hiraga:2004,
address = {Hamamatsu, Japan},
author = {Hiraga, Rumi and Bresin, Roberto and Hirata, Keiji and Katayose, Haruhiro},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Musical Expression,Performance Ren- dering,Rencon,Turing Test},
pages = {120--123},
title = {{Rencon 2004: Turing Test for Musical Expression}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}120.pdf},
year = {2004}
}
@inproceedings{Hughes:2004,
abstract = {The Epipe is a novel electronic woodwind controller with continuous tonehole coverage sensing, an initial design for which was introduced at NIME '03. Since then, we have successfully completed two fully operational prototypes. This short paper describes some of the issues encountered during the design and construction of this controller. It also details our own early experiences and impressions of the interface as well as its technical specifications.},
address = {Hamamatsu, Japan},
author = {Hughes, Stephen and Cannon, Cormac and O'Modhrain, Sile},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {MIDI,capacitive sensing,variable tonehole control,woodwind controller},
pages = {199--200},
title = {{Epipe : A Novel Electronic Woodwind Controller}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}199.pdf},
year = {2004}
}
@inproceedings{Lee:2004,
abstract = {This paper describes the first system designed to allow children to conduct an audio and video recording of an orchestra. No prior music experience is required to control theorchestra, and the system uses an advanced algorithm totime stretch the audio in real-time at high quality and without altering the pitch. We will discuss the requirements andchallenges of designing an interface to target our particularuser group (children), followed by some system implementation details. An overview of the algorithm used for audiotime stretching will also be presented. We are currently using this technology to study and compare professional andnon-professional conducting behavior, and its implicationswhen designing new interfaces for multimedia. You're theConductor is currently a successful exhibit at the Children'sMuseum in Boston, USA.},
address = {Hamamatsu, Japan},
author = {Lee, Eric and Nakra, Teresa M and Borchers, Jan},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {conducting systems,design patterns,gesture recogni-,interactive exhibits,real-time audio stretching,tion},
pages = {68--73},
title = {{You're The Conductor: A Realistic Interactive Conducting System for Children}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}068.pdf},
year = {2004}
}
@inproceedings{Essl2005,
address = {Vancouver, BC, Canada},
author = {Essl, Georg and O'Modhrain, Sile},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
pages = {70--75},
title = {{Scrubber: An Interface for Friction-induced Sounds}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}070.pdf},
year = {2005}
}
@inproceedings{Deutscher2005,
address = {Vancouver, BC, Canada},
author = {Deutscher, Meghan and Fels, Sidney and Hoskinson, Reynald and Takahashi, Sachiyo},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
pages = {274},
title = {{Echology}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}274.pdf},
year = {2005}
}
@inproceedings{Chew2005,
abstract = {In the Expression Synthesis Project (ESP), we propose adriving interface for expression synthesis. ESP aims toprovide a compelling metaphor for expressive performance soas to make high-level expressive decisions accessible to nonexperts. In ESP, the user drives a car on a virtual road thatrepresents the music with its twists and turns; and makesdecisions on how to traverse each part of the road. The driver'sdecisions affect in real-time the rendering of the piece. Thepedals and wheel provide a tactile interface for controlling thecar dynamics and musical expression, while the displayportrays a first person view of the road and dashboard from thedriver's seat. This game-like interface allows non-experts tocreate expressive renderings of existing music without havingto master an instrument, and allows expert musicians toexperiment with expressive choice without having to firstmaster the notes of the piece. The prototype system has beentested and refined in numerous demonstrations. This paperpresents the concepts underlying the ESP system and thearchitectural design and implementation of a prototype.},
address = {Vancouver, BC, Canada},
author = {Chew, Elaine and Fran{\c{c}}ois, Alexandre R and Liu, Jie and Yang, Aaron},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Music expression synthesis system,driving interface.},
pages = {224--227},
title = {{ESP: A Driving Interface for Expression Synthesis}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}224.pdf},
year = {2005}
}
@inproceedings{Birnbaum2005,
address = {Vancouver, BC, Canada},
author = {Birnbaum, David and Fiebrink, Rebecca and Malloch, Joseph and Wanderley, Marcelo M},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {design space analysis,human-computer interaction,interfaces for musical expression,new},
pages = {192--195},
title = {{Towards a Dimension Space for Musical Devices}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}192.pdf},
year = {2005}
}
@inproceedings{Yonezawa2005,
abstract = {The HandySinger system is a personified tool developedto naturally express a singing voice controlled by the gestures of a hand puppet. Assuming that a singing voice is akind of musical expression, natural expressions of the singingvoice are important for personification. We adopt a singingvoice morphing algorithm that effectively smoothes out thestrength of expressions delivered with a singing voice. Thesystem's hand puppet consists of a glove with seven bendsensors and two pressure sensors. It sensitively capturesthe user's motion as a personified puppet's gesture. Tosynthesize the different expressional strengths of a singingvoice, the ``normal'' (without expression) voice of a particular singer is used as the base of morphing, and three different expressions, ``dark,'' ``whisper'' and ``wet,'' are used asthe target. This configuration provides musically expressedcontrols that are intuitive to users. In the experiment, weevaluate whether 1) the morphing algorithm interpolatesexpressional strength in a perceptual sense, 2) the handpuppet interface provides gesture data at sufficient resolution, and 3) the gestural mapping of the current systemworks as planned.},
address = {Vancouver, BC, Canada},
author = {Yonezawa, Tomoko and Suzuki, Takahiko and Mase, Kenji and Kogure, Kiyoshi},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Hand-puppet Interface,Personified Expression,Singing Voice Morphing,Voice Ex- pressivity},
pages = {121--126},
title = {{HandySinger : Expressive Singing Voice Morphing using Personified Hand-puppet Interface}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}121.pdf},
year = {2005}
}
@inproceedings{Taylor2005,
abstract = {We present a real-time system which allows musicians tointeract with synthetic virtual characters as they perform.Using Max/MSP to parameterize keyboard and vocal input, meaningful features (pitch, amplitude, chord information, and vocal timbre) are extracted from live performancein real-time. These extracted musical features are thenmapped to character behaviour in such a way that the musician's performance elicits a response from the virtual character. The system uses the ANIMUS framework to generatebelievable character expressions. Experimental results arepresented for simple characters.},
address = {Vancouver, BC, Canada},
author = {Taylor, Robyn and Torres, Daniel and Boulanger, Pierre},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Music,advanced man-machine inter- faces,artistic in- stallations,behavioural systems,immersive entertainment,interaction tech- niques,synthetic characters,virtual reality,visualization},
pages = {220--223},
title = {{Using Music to Interact with a Virtual Character}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}220.pdf},
year = {2005}
}
@inproceedings{Marinelli2005,
abstract = {Mocean is an immersive environment that creates sensoryrelationships between natural media, particularly exploringthe potential of water as an emotive interface.},
address = {Vancouver, BC, Canada},
author = {Marinelli, Maia and Lamenzo, Jared and Borissov, Liubo},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {New interface,PIC microcontroller,human computer interface.,natural media,pipe organ,water,wind instrument},
pages = {272},
title = {{Mocean}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}272.pdf},
year = {2005}
}
@inproceedings{Melo2005,
abstract = {The Swayway is an audio/MIDI device inspired by the simpleconcept of the wind chime.This interactive sculpture translates its swaying motion,triggered by the user, into sound and light. Additionally, themotion of the reeds contributes to the visual aspect of thepiece, converting the whole into a sensory and engagingexperience.},
address = {Vancouver, BC, Canada},
author = {Melo, Mauricio and Fan, Doria},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Interactive sound sculpture,LEDs,flex sensors,midi chimes,sound installation.},
pages = {262--263},
title = {{Swayway - Midi Chimes}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}262.pdf},
year = {2005}
}
@inproceedings{Fox2005,
abstract = {This paper describes the design of SoniMime, a system forthe sonification of hand movement for real-time timbre shaping. We explore the application of the tristimulus timbremodel for the sonification of gestural data, working towardthe goals of musical expressivity and physical responsiveness. SoniMime uses two 3-D accelerometers connected toan Atmel microprocessor which outputs OSC control messages. Data filtering, parameter mapping, and sound synthesis take place in Pd running on a Linux computer.},
address = {Vancouver, BC, Canada},
author = {Fox, Jesse and Carlile, Jennifer},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Human Computer Interac- tion,Musical Controller,Sonification},
pages = {242--243},
title = {{SoniMime: Movement Sonification for Real-Time Timbre Shaping}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}242.pdf},
year = {2005}
}
@inproceedings{Pellarin2005,
address = {Vancouver, BC, Canada},
author = {Pellarin, Lars and B{\"{o}}ttcher, Niels and Olsen, Jakob M and Gregersen, Ole and Serafin, Stefania and Guglielmi, Michel},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Motion tracking,mapping strategies,multiple participants music interfaces.,public installation},
pages = {152--155},
title = {{Connecting Strangers at a Train Station}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}152.pdf},
year = {2005}
}
@inproceedings{Carlile2005,
address = {Vancouver, BC, Canada},
author = {Carlile, Jennifer and Hartmann, Bj{\"{o}}rn},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Collaborative Control,Haptic Interfaces,Musical Controller},
pages = {250--251},
title = {{{\{}OR{\}}OBORO: A Collaborative Controller with Interpersonal Haptic Feedback}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}250.pdf},
year = {2005}
}
@inproceedings{Carter2005,
abstract = {In this paper, we describe a course of research investigating thepotential for new types of music made possible by locationtracking and wireless technologies. Listeners walk arounddowntown Culver City, California and explore a new type ofmusical album by mixing together songs and stories based ontheir movement. By using mobile devices as an interface, wecan create new types of musical experiences that allowlisteners to take a more interactive approach to an album.},
address = {Vancouver, BC, Canada},
author = {Carter, William and Liu, Leslie S},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Augmented Reality,Digital Soundscape,Interactive Music,Location-Based Entertainment,Mobile Music,Mobility},
pages = {176--179},
title = {{Location33: A Mobile Musical}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}176.pdf},
year = {2005}
}
@inproceedings{Blaine2005,
abstract = {This paper will investigate a variety of alternate controllers that are making an impact in interactive entertainment, particularly in the video game industry. Since the late 1990's, the surging popularity of rhythmic and musical performance games in Japanese arcades has led to the development of new interfaces and alternate controllers for the consumer market worldwide. Rhythm action games such as Dance Dance Revolution, Taiko No Tatsujin (Taiko: Drum Master), and Donkey Konga are stimulating collaborative gameplay and exposing consumers to custom controllers designed specifically for musical and physical interaction. We are witnessing the emergence and acceptance of these breakthrough controllers and models for gameplay as an international cultural phenomenon penetrating the video game and toy markets in record numbers. Therefore, it is worth considering the potential benefits to developers of musical interfaces, electronic devices and alternate controllers in light of these new and emerging opportunities, particularly in the realm of video gaming, toy development, arcades, and other interactive entertainment experiences.},
address = {Vancouver, BC, Canada},
author = {Blaine, Tina},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
pages = {27--33},
title = {{The Convergence of Alternate Controllers and Musical Interfaces in Interactive Entertainment}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}027.pdf},
year = {2005}
}
@inproceedings{Birchfield2005,
abstract = {This paper details the motivations, design, and realization of Sustainable, a dynamic, robotic sound installation that employs a generative algorithm for music and sound creation. The piece is comprised of seven autonomous water gong nodes that are networked together by water tubes to distribute water throughout the system. A water resource allocation algorithm guides this distribution process and produces an ever-evolving sonic and visual texture. A simple set of behaviors govern the individual gongs, and the system as a whole exhibits emergent properties that yield local and large scale forms in sound and light.},
address = {Vancouver, BC, Canada},
author = {Birchfield, David and Lorig, David and Phillips, Kelly},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {computing,dynamic systems,evolutionary,generative arts,installation art,music,robotics,sculpture,sound},
pages = {160--163},
title = {{Sustainable: a dynamic, robotic, sound installation}},
url = {http://www.nime.org/proceedings/2005/nime2005{\_}160.pdf},
year = {2005}
}
@inproceedings{Burns2006,
address = {Paris, France},
author = {Burns, Anne-Marie and Wanderley, Marcelo M},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {finger-tracking,gesture,guitar fingering,hough transform},
pages = {196--199},
title = {{Visual Methods for the Retrieval of Guitarist Fingering}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}196.pdf},
year = {2006}
}
@inproceedings{Serafin2006,
abstract = {In this paper we introduce the Croaker, a novel input deviceinspired by Russolo's Intonarumori. We describe the components of the controller and the sound synthesis engine whichallows to reproduce several everyday sounds.},
address = {Paris, France},
author = {Serafin, Stefania and de G{\"{o}}tzen, Amalia and B{\"{o}}ttcher, Niels and Gelineck, Steven},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Noise machines,everyday sounds,physical models.},
pages = {240--245},
title = {{Synthesis and Control of Everyday Sounds Reconstructing Russolo's Intonarumori}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}240.pdf},
year = {2006}
}
@inproceedings{Gaye2006,
address = {Paris, France},
author = {Gaye, Lalya and Holmquist, Lars E and Behrendt, Frauke and Tanaka, Atau},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
pages = {22--25},
title = {{Mobile Music Technology: Report on an Emerging Community}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}022.pdf},
year = {2006}
}
@inproceedings{Smyth2006,
address = {Paris, France},
author = {Smyth, Tamara},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {khaen,mapping,musical acoustics,sound synthesis control},
pages = {314--317},
title = {{Handheld Acoustic Filter Bank for Musical Control}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}314.pdf},
year = {2006}
}
@inproceedings{Francois2006,
address = {Paris, France},
author = {Fran{\c{c}}ois, Alexandre R and Chew, Elaine},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Interactive Systems,Music soft- ware,Software Architecture},
pages = {150--155},
title = {{An Architectural Framework for Interactive Music Systems}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}150.pdf},
year = {2006}
}
@inproceedings{Kimura2006,
abstract = {This is a description of a demonstration, regarding theuse of auditory illusions and psycho-acoustic phenomenonused in the interactive work of Jean-Claude Risset, writtenfor violinist Mari Kimura.},
address = {Paris, France},
author = {Kimura, Mari and Risset, Jean-Claude},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Kimura.,Risset,Violin,auditory illusions,psycho-acoustic phenomena,sig- nal processing,subharmonics},
pages = {407--408},
title = {{Auditory Illusion and Violin: Demonstration of a Work by Jean-Claude Risset Written for Mari Kimura}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}407.pdf},
year = {2006}
}
@inproceedings{Young2006,
abstract = {In this paper we present progress of an ongoingcollaboration between researchers at the MIT MediaLaboratory and the Royal Academy of Music (RAM). The aimof this project is to further explore the expressive musicalpotential of the Hyperbow, a custom music controller firstdesigned for use in violin performance. Through the creationof new repertoire, we hope to stimulate the evolution of thisinterface, advancing its usability and refining itscapabilities. In preparation for this work, the Hyperbowsystem has been adapted for cello (acoustic and electric)performance. The structure of our collaboration is described,and two of the pieces currently in progress are presented.Feedback from the performers is also discussed, as well asfuture plans.},
address = {Paris, France},
author = {Young, Diana and Nunn, Patrick and Vassiliev, Artem},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Cello,bow,composition.,controller,electroacoustic music},
pages = {396--401},
title = {{Composing for Hyperbow: A Collaboration Between {\{}MIT{\}} and the Royal Academy of Music}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}396.pdf},
year = {2006}
}
@inproceedings{Bonardi2006,
abstract = {In this article, we present the first step of our research work todesign a Virtual Assistant for Performers and Stage Directors,able to give a feedback from performances. We use amethodology to automatically construct fuzzy rules in a FuzzyRule-Based System that detects contextual emotions from anactor's performance during a show.We collect video data from a lot of performances of the sameshow from which it should be possible to visualize all theemotions and intents or more precisely "intent graphs". Toperform this, the collected data defining low-level descriptorsare aggregated and converted into high-level characterizations.Then, depending on the retrieved data and on their distributionon the axis, we partition the universes into classes. The last stepis the building of the fuzzy rules that are obtained from theclasses and that permit to give conclusions to label the detectedemotions.},
address = {Paris, France},
author = {Bonardi, Alain and Truck, Isis and Akdag, Herman},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Emotion detector,Fuzzy Classes,Intents,Performance.,Stage Director,Virtual Assistant},
pages = {326--329},
title = {{Towards a Virtual Assistant for Performers and Stage Directors}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}326.pdf},
year = {2006}
}
@inproceedings{Wang2006,
address = {Paris, France},
author = {Wang, Ge and Misra, Ananya and Cook, Perry R},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Graphical interfaces,collaborative performance,computer music ensemble,education.,emergence,networking,visualization},
pages = {49--52},
title = {{Building Collaborative Graphical interFaces in the Audicle}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}049.pdf},
year = {2006}
}
@inproceedings{Favilla2006,
address = {Paris, France},
author = {Favilla, Stuart and Cannon, Joanne},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
pages = {370--375},
title = {{Children of Grainger: Leather Instruments for Free Music}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}370.pdf},
year = {2006}
}
@inproceedings{Breinbjerg2006,
abstract = {In this paper we describe the intentions, the design and functionality of an Acousmatic Composition Environment that allows children or musical novices to educate their auditory curiosity by recording, manipulating and mixing sounds of everyday life. The environment consists of three stands: A stand for sound recording with a soundproof box that ensure good recording facilities in a noisy environment; a stand for sound manipulation with five simple, tangible interfaces; a stand for sound mixing with a graphical computer interface presented on two touch screens.},
address = {Paris, France},
author = {Breinbjerg, Morten and Caprani, Ole and Lunding, Rasmus and Kramhoft, Line},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Acousmatic listening,aesthetics,tangible interfaces.},
pages = {334--337},
title = {{An Acousmatic Composition Environment}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}334.pdf},
year = {2006}
}
@inproceedings{Bottoni2006,
address = {Paris, France},
author = {Bottoni, Paolo and Faralli, Stefano and Labella, Anna and Pierro, Mario},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Max/MSP,agent,mapping,planning},
pages = {322--325},
title = {{Mapping with Planning Agents in the Max/MSP Environment: the GO/Max Language}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}322.pdf},
year = {2006}
}
@inproceedings{Bevilacqua2006,
address = {Paris, France},
author = {Bevilacqua, Fr{\'{e}}d{\'{e}}ric and Rasamimanana, Nicolas and Fl{\'{e}}ty, Emmanuel and Lemouton, Serge and Baschet, Florence},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
pages = {402--406},
title = {{The Augmented Violin Project: Research, Composition and Performance Report}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}402.pdf},
year = {2006}
}
@inproceedings{Schiesser2006,
address = {Paris, France},
author = {Schiesser, S{\'{e}}bastien and Traube, Caroline},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {augmented instrument,gestural control,live electronics,perfor- mance,saxophone},
pages = {308--313},
title = {{On Making and Playing an Electronically-augmented Saxophone}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}308.pdf},
year = {2006}
}
@inproceedings{Beilharz2006,
abstract = {Hyper-shaku (Border-Crossing) is an interactive sensor environment that uses motion sensors to trigger immediate responses and generative processes augmenting the Japanese bamboo shakuhachi in both the auditory and visual domain. The latter differentiates this process from many hyper-instruments by building a performance of visual design as well as electronic music on top of the acoustic performance. It utilizes a combination of computer vision and wireless sensing technologies conflated from preceding works. This paper outlines the use of gesture in these preparatory sound and audio-visual performative, installation and sonification works, leading to a description of the Hyper-shaku environment integrating sonification and generative elements.},
address = {Paris, France},
author = {Beilharz, Kirsty and Jakovich, Joanne and Ferguson, Sam},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Gesture-controllers,hyper-instrument,sonification},
pages = {352--357},
title = {{Hyper-shaku (Border-crossing): Towards the Multi-modal Gesture-controlled Hyper-Instrument}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}352.pdf},
year = {2006}
}
@inproceedings{Dimitrov2006,
address = {Paris, France},
author = {Dimitrov, Smilen and Serafin, Stefania},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
pages = {184--187},
title = {{A Simple Practical Approach to a Wireless Data Acquisition Board}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}184.pdf},
year = {2006}
}
@inproceedings{Freed2006a,
abstract = {Software and hardware enhancements to an electric 6-stringcello are described with a focus on a new mechanical tuningdevice, a novel rotary sensor for bow interaction and controlstrategies to leverage a suite of polyphonic soundprocessing effects.},
address = {Paris, France},
author = {Freed, Adrian and Wessel, David and Zbyszy{\'{n}}ski, Michael and Uitti, Frances M},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Cello,Double Bowing,FSR,Rotary Absolute Position Encoder,chordophone,convolution.,double stops,triple stops},
pages = {409--413},
title = {{Augmenting the Cello}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}409.pdf},
year = {2006}
}
@inproceedings{Tanaka2006,
abstract = {This paper presents the concepts and techniques used in afamily of location based multimedia works. The paper hasthree main sections: 1.) to describe the architecture of anaudio-visual hardware/software framework we havedeveloped for the realization of a series of locative mediaartworks, 2.) to discuss the theoretical and conceptualunderpinnings motivating the design of the technicalframework, and 3.) to elicit from this, fundamental issuesand questions that can be generalized and applicable to thegrowing practice of locative media.},
address = {Paris, France},
author = {Tanaka, Atau and Gemeinboeck, Petra},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Mobile music,locative media.,urban fiction},
pages = {26--30},
title = {{A Framework for Spatial Interaction in Locative Media}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}026.pdf},
year = {2006}
}
@inproceedings{Birchfield2006,
abstract = {Physically situated public art poses significant challenges for the design and realization of interactive, electronic sound works. Consideration of diverse audiences, environmental sensitivity, exhibition conditions, and logistics must guide the artwork. We describe our work in this area, using a recently installed public piece, Transition Soundings, as a case study that reveals a specialized interface and open-ended approach to interactive music making. This case study serves as a vehicle for examination of the real world challenges posed by public art and its outcomes.},
address = {Paris, France},
author = {Birchfield, David and Phillips, Kelly and Kidan{\'{e}}, Assegid and Lorig, David},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Arts,Embedded Electronics.,Installation Art,Interactivity,Music,Network Systems,Public Art,Sculpture,Sound},
pages = {43--48},
title = {{Interactive Public Sound Art: a case study}},
url = {http://www.nime.org/proceedings/2006/nime2006{\_}043.pdf},
year = {2006}
}
@inproceedings{Baalman2007,
abstract = {This paper describes work on a newly created large-scaleinteractive theater performance entitled Schwelle (Thresholds). The , , authors discuss an innovative approach towardsthe conception, development and implementation of dynamicand responsive audio scenography : a constantly evolving,multi-layered sound design generated by continuous inputfrom a series of distributed wireless sensors deployed bothon the body of a performer and placed within the physicalstage environment. The paper is divided into conceptualand technological parts. We first describe the project's dramaturgical and conceptual context in order to situate theartistic framework that has guided the technological systemdesign. Specifically, this framework discusses the team's approach in combining techniques from situated computing,theatrical sound design practice and dynamical systems inorder to create a new kind of adaptive audio scenographicenvironment augmented by wireless, distributed sensing foruse in live theatrical performance. The goal of this adaptive sound design is to move beyond both existing playbackmodels used in theatre sound as well as the purely humancentered, controller-instrument approach used in much current interactive performance practice.},
address = {New York City, NY, United States},
author = {Baalman, Marije A and Moody-Grigsby, Daniel and Salter, Christopher L},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Interactive performance,adaptive audio scenography,audio dramaturgy,dynamical systems,situated computing,sound design,wireless sens- ing},
pages = {178--184},
title = {{Schwelle : Sensor Augmented, Adaptive Sound Design for Live Theatrical Performance}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}178.pdf},
year = {2007}
}
@inproceedings{Nakamoto2007,
abstract = {We proposed a circle canon system for enjoying a musical ensemble supported by a computer and network. Using the song "Frog round", which is a popular circle canon chorus originated from a German folk song, we produced a singing ensemble opportunity where everyone plays the music together at the same time. The aim of our system is that anyone can experience the joyful feeling of actually playing the music as well as sharing it with others.},
address = {New York City, NY, United States},
author = {Nakamoto, Misako and Kuhara, Yasuo},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Chorus,Circle canon,Ensemble,Frog round,Internet,Max/MSP,MySQL database.,Song},
pages = {409--410},
title = {{Circle Canon Chorus System Used To Enjoy A Musical Ensemble Singing "Frog Round"}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}409.pdf},
year = {2007}
}
@inproceedings{Corness2007,
address = {New York City, NY, United States},
author = {Seo, Jinsil and Corness, Greg},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {nime07},
pages = {431},
title = {{nite{\_}aura : An Audio-Visual Interactive Immersive Installation}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}431.pdf},
year = {2007}
}
@inproceedings{Moriwaki2007,
abstract = {In this paper the , , authors present the MIDI Scrapyard Challenge (MSC) workshop, a one-day hands-on experience which asks participants to create musical controllers out of cast-off electronics, found materials and junk. The workshop experience, principles, and considerations are detailed, along with sample projects which have been created in various MSC workshops. Observations and implications as well as future developments for the workshop are discussed.},
address = {New York City, NY, United States},
author = {Moriwaki, Katherine and Brucken-Cohen, Jonah},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {nime07},
pages = {168--172},
title = {{MIDI Scrapyard Challenge Workshops}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}168.pdf},
year = {2007}
}
@inproceedings{Paine2007,
address = {New York City, NY, United States},
author = {Paine, Garth and Stevenson, Ian and Pearce, Angela},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Mapping,Musical Instrument Design,Musicianship,evaluation,testing.},
pages = {70--77},
title = {{The Thummer Mapping Project (ThuMP)}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}070.pdf},
year = {2007}
}
@inproceedings{Bennett2007,
abstract = {This paper proposes that the physicality of an instrument be considered an important aspect in the design of new interfaces for musical expression. The use of Laban's theory of effort in the design of new effortful interfaces, in particular looking at effortspace modulation, is investigated, and a platform for effortful interface development (named the DAMPER) is described. Finally, future work is described and further areas of research are highlighted.},
address = {New York City, NY, United States},
author = {Bennett, Peter and Ward, Nicholas and O'Modhrain, Sile and Rebelo, Pedro},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Effortful Interaction. Haptics. Laban Analysis. Ph},
pages = {273--276},
title = {{DAMPER : A Platform for Effortful Interface Development}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}273.pdf},
year = {2007}
}
@inproceedings{Fiebrink2007,
address = {New York City, NY, United States},
author = {Fiebrink, Rebecca and Wang, Ge and Cook, Perry R},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {nime07},
pages = {164--167},
title = {{Don't Forget the Laptop : Using Native Input Capabilities for Expressive Musical Control}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}164.pdf},
year = {2007}
}
@inproceedings{Hollinger2007,
abstract = {This paper presents an electronic piano keyboard and computer mouse designed for use in a magnetic resonance imaging scanner. The interface allows neuroscientists studying motor learning of musical tasks to perform functional scans of a subject's brain while synchronizing the scanner, auditory and visual stimuli, and auditory feedback with the onset, offset, and velocity of the piano keys. The design of the initial prototype and environment-specific issues are described, as well as prior work in the field. Preliminary results are positive and were unable to show the existence of image artifacts caused by the interface. Recommendations to improve the optical assembly are provided in order to increase the robustness of the design.},
address = {New York City, NY, United States},
author = {Hollinger, Avrum and Steele, Christopher and Penhune, Virginia and Zatorre, Robert and Wanderley, Marcelo M},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Input device,MRI-compatible,fMRI,motor learning,optical sensing.},
pages = {246--249},
title = {{fMRI-Compatible Electronic Controllers}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}246.pdf},
year = {2007}
}
@inproceedings{Jakovich2007,
address = {New York City, NY, United States},
author = {Jakovich, Joanne and Beilharz, Kirsty},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Architecture,adaptation,engagement.,granular synthesis,installation,interaction},
pages = {185--190},
title = {{ParticleTecture : Interactive Granular Soundspaces for Architectural Design}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}185.pdf},
year = {2007}
}
@inproceedings{Sa2007,
address = {New York City, NY, United States},
author = {S{\'{a}}, Adriana},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {nime07},
pages = {428},
title = {{Thresholds}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}428.pdf},
year = {2007}
}
@inproceedings{Bell2007,
abstract = {We present the Multimodal Music Stand (MMMS) for the untethered sensing of performance gestures and the interactive control of music. Using e-field sensing, audio analysis, and computer vision, the MMMS captures a performer's continuous expressive gestures and robustly identifies discrete cues in a musical performance. Continuous and discrete gestures are sent to an interactive music system featuring custom designed software that performs real-time spectral transformation of audio.},
address = {New York City, NY, United States},
author = {Bell, Bo and Kleban, Jim and Overholt, Dan and Putnam, Lance and Thompson, John and Kuchera-Morin, JoAnn},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Multimodal,computer vision,e-field sensing,interactivity,untethered control.},
pages = {62--65},
title = {{The Multimodal Music Stand}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}062.pdf},
year = {2007}
}
@inproceedings{Lee2007a,
abstract = {We present REXband, an interactive music exhibit for collaborative improvisation to medieval music. This audio-only system consists of three digitally augmented medieval instrument replicas: thehurdy gurdy, harp, and frame drum. The instruments communicatewith software that provides users with both musical support andfeedback on their performance using a "virtual audience" set in amedieval tavern. REXband builds upon previous work in interactivemusic exhibits by incorporating aspects of e-learning to educate, inaddition to interaction design patterns to entertain; care was alsotaken to ensure historic authenticity. Feedback from user testingin both controlled (laboratory) and public (museum) environmentshas been extremely positive. REXband is part of the RegensburgExperience, an exhibition scheduled to open in July 2007 to showcase the rich history of Regensburg, Germany.},
address = {New York City, NY, United States},
author = {Lee, Eric and Wolf, Marius and Jansen, Yvonne and Borchers, Jan},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {augmented instru- ments,e-learning,education,interactive music exhibits,medieval music},
pages = {172--177},
title = {{REXband : A Multi-User Interactive Exhibit for Exploring Medieval Music}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}172.pdf},
year = {2007}
}
@inproceedings{Bottoni2007,
abstract = {This paper reports our experiments on using a dual-coreDSP processor in the construction of a user-programmablemusical instrument and controller called the TouchBox.},
address = {New York City, NY, United States},
author = {Bottoni, Paolo and Caporali, Riccardo and Capuano, Daniele and Faralli, Stefano and Labella, Anna and Pierro, Mario},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {DSP,controller,dual-core,synthesizer,touch-screen},
pages = {394--395},
title = {{Use of a Dual-Core {\{}DSP{\}} in a Low-Cost, Touch-Screen Based Musical Instrument}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}394.pdf},
year = {2007}
}
@inproceedings{Castellano2007,
abstract = {In this paper we describe a system which allows users to use their full-body for controlling in real-time the generation of an expressive audio-visual feedback. The system extracts expressive motion features from the user's full-body movements and gestures. The values of these motion features are mapped both onto acoustic parameters for the real-time expressive rendering of a piece of music, and onto real-time generated visual feedback projected on a screen in front of the user.},
address = {New York City, NY, United States},
author = {Castellano, Ginevra and Bresin, Roberto and Camurri, Antonio and Volpe, Gualtiero},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Expressive interaction,i,multimodal environments},
pages = {390--391},
title = {{Expressive Control of Music and Visual Media by Full-Body Movement}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}390.pdf},
year = {2007}
}
@inproceedings{Chuchacz2007,
abstract = {A novel electronic percussion synthesizer prototype is presented. Our ambition is to design an instrument that will produce a high quality, realistic sound based on a physical modelling sound synthesis algorithm. This is achieved using a real-time Field Programmable Gate Array (FPGA) implementation of the model coupled to an interface that aims to make efficient use of all the subtle nuanced gestures of the instrumentalist. It is based on a complex physical model of the vibrating plate - the source of sound in the majority of percussion instruments. A Xilinx Virtex II pro FPGA core handles the sound synthesis computations with an 8 billion operations per second performance and has been designed in such a way to allow a high level of control and flexibility. Strategies are also presented to that allow the parametric space of the model to be mapped to the playing gestures of the percussionist.},
address = {New York City, NY, United States},
author = {Chuchacz, Katarzyna and O'Modhrain, Sile and Woods, Roger},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Electronic Percussion Instrument,FPGA.,Physical Model},
pages = {37--40},
title = {{Physical Models and Musical Controllers -- Designing a Novel Electronic Percussion Instrument}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}037.pdf},
year = {2007}
}
@inproceedings{Chang2007,
abstract = {FigureWe present Zstretch, a textile music controller that supports expressive haptic interactions. The musical controller takes advantage of the fabric's topological constraints to enable proportional control of musical parameters. This novel interface explores ways in which one might treat music as a sheet of cloth. This paper proposes an approach to engage simple technologies for supporting ordinary hand interactions. We show that this combination of basic technology with general tactile movements can result in an expressive musical interface. a},
address = {New York City, NY, United States},
author = {Chang, Angela and Ishii, Hiroshi},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Tangible interfaces,musical expressivity,tactile design,textiles},
pages = {46--49},
title = {{Zstretch : A Stretchy Fabric Music Controller}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}046.pdf},
year = {2007}
}
@inproceedings{Court2007,
address = {New York City, NY, United States},
author = {Grant, Laura and Grant, Billy and Grillo, Joe and Osborn, Owen and Kucinski, Christopher},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {nime07},
pages = {421},
title = {{Miller}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}421.pdf},
year = {2007}
}
@inproceedings{Hashida2007,
abstract = {This paper introduces jPop-E (java-based PolyPhrase Ensemble), an assistant system for the Pop-E performancerendering system. Using this assistant system, MIDI dataincluding expressive tempo changes or velocity control canbe created based on the user's musical intention. Pop-E(PolyPhrase Ensemble) is one of the few machine systemsdevoted to creating expressive musical performances thatcan deal with the structure of polyphonic music and theuser's interpretation of the music. A well-designed graphical user interface is required to make full use of the potential ability of Pop-E. In this paper, we discuss the necessaryelements of the user interface for Pop-E, and describe theimplemented system, jPop-E.},
address = {New York City, NY, United States},
author = {Hashida, Mitsuyo and Nagata, Noriko and Katayose, Haruhiro},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Ensemble Music Ex- pression,Performance Rendering,User Interface},
pages = {313--316},
title = {{jPop-E : An Assistant System for Performance Rendering of Ensemble Music}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}313.pdf},
year = {2007}
}
@inproceedings{Ojanen2007,
abstract = {This paper presents a line of historic electronic musical instruments designed by Erkki Kurenniemi in the 1960's and1970's. Kurenniemi's instruments were influenced by digitallogic and an experimental attitude towards user interfacedesign. The paper presents an overview of Kurenniemi'sinstruments and a detailed description of selected devices.Emphasis is put on user interface issues such as unconventional interactive real-time control and programming methods.},
address = {New York City, NY, United States},
author = {Ojanen, Mikko and Suominen, Jari and Kallio, Titti and Lassfolk, Kai},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Digital electronics,Dimi,Erkki Kurenniemi,Synthesizer,User interface design},
pages = {88--93},
title = {{Design Principles and User Interfaces of Erkki Kurenniemi's Electronic Musical Instruments of the 1960's and 1970's}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}088.pdf},
year = {2007}
}
@inproceedings{Hashida2007a,
abstract = {This paper introduces a system for improvisational musicalexpression that enables all users, novice and experienced, toperform intuitively and expressively. Users can generate musically consistent results through intuitive action, inputtingrhythm in a decent tempo. We demonstrate novel mappingways that reect user's input information more interactivelyand eectively in generating the music. We also present various input devices that allow users more creative liberty.},
address = {New York City, NY, United States},
author = {Hashida, Tomoko and Naemura, Takeshi and Sato, Takao},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Improvisation,a sense of tempo,interactive music},
pages = {407--408},
title = {{A System for Improvisational Musical Expression Based on Player ' s Sense of Tempo}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}407.pdf},
year = {2007}
}
@inproceedings{Cartwright2007,
abstract = {This report presents the design and construct ion of Rage in Conjunction with the Machine, a simple but novel pairing of musical interface and sound sculpture. The , , authors discuss the design and creation of this instrument , focusing on the unique aspects of it, including the use of physical systems, large gestural input, scale, and the electronic coupling of a physical input to a physical output.},
address = {New York City, NY, United States},
author = {Cartwright, Mark and Jones, Matt and Terasawa, Hiroko},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {audience participation,inflatable,instrume nt design,instrume nt size,mapping,musical,new musical instrument,nime07,physical systems,sound scultpure},
pages = {224--227},
title = {{Rage in Conjunction with the Machine}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}224.pdf},
year = {2007}
}
@inproceedings{Franinovic2007,
abstract = {The distinctive features of interactive sound installations inpublic space are considered, with special attention to therich, if undoubtedly difficult, environments in which theyexist. It is argued that such environments, and the socialcontexts that they imply, are among the most valuable features of these works for the approach that we have adoptedto creation as research practice. The discussion is articulated through case studies drawn from two of our installations, Recycled Soundscapes (2004) and Skyhooks (2006).Implications for the broader design of new musical instruments are presented.},
address = {New York City, NY, United States},
author = {Franinovic, Karmen and Visell, Yon},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {architecture,interaction,music,nime07,sound in-,urban design},
pages = {191--196},
title = {{New Musical Interfaces in Context : Sonic Interaction Design in the Urban Setting}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}191.pdf},
year = {2007}
}
@inproceedings{Francois2007,
abstract = {This paper describes the design of Mimi, a multi-modal interactive musical improvisation system that explores the potential and powerful impact of visual feedback in performermachine interaction. Mimi is a performer-centric tool designed for use in performance and teaching. Its key andnovel component is its visual interface, designed to providethe performer with instantaneous and continuous information on the state of the system. For human improvisation,in which context and planning are paramount, the relevantstate of the system extends to the near future and recentpast. Mimi's visual interface allows for a peculiar blendof raw reflex typically associated with improvisation, andpreparation and timing more closely affiliated with scorebased reading. Mimi is not only an effective improvisationpartner, it has also proven itself to be an invaluable platformthrough which to interrogate the mental models necessaryfor successful improvisation.},
address = {New York City, NY, United States},
author = {Fran{\c{c}}ois, Alexandre R and Chew, Elaine and Thurmond, Dennis},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Performer-machine interaction,machine improvisation,visualization design},
pages = {277--280},
title = {{Visual Feedback in Performer-Machine Interaction for Musical Improvisation}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}277.pdf},
year = {2007}
}
@inproceedings{Young2007,
abstract = {This paper presents a newly created database containingcalibrated gesture and audio data corresponding to variousviolin bowstrokes, as well as video and motion capture datain some cases. The database is web-accessible and searchable by keywords and subject. It also has several importantfeatures designed to improve accessibility to the data and tofoster collaboration between researchers in fields related tobowed string synthesis, acoustics, and gesture.},
address = {New York City, NY, United States},
author = {Young, Diana and Deshmane, Anagha},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {audio,bowed string,bowing,bowing parame- ters,bowstroke,gesture,technique,violin},
pages = {352--357},
title = {{Bowstroke Database : A Web-Accessible Archive of Violin Bowing Data}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}352.pdf},
year = {2007}
}
@inproceedings{B2007,
abstract = {In this paper we describe the design and implementation ofthe PHYSMISM: an interface for exploring the possibilitiesfor improving the creative use of physical modelling soundsynthesis.The PHYSMISM is implemented in a software and hardware version. Moreover, four different physical modellingtechniques are implemented, to explore the implications ofusing and combining different techniques.In order to evaluate the creative use of physical models,a test was performed using 11 experienced musicians as testsubjects. Results show that the capability of combining thephysical models and the use of a physical interface engagedthe musicians in creative exploration of physical models.},
address = {New York City, NY, United States},
author = {B{\"{o}}ttcher, Niels and Gelineck, Steven and Serafin, Stefania},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Physical models,excitation,hybrid instruments,resonator.},
pages = {31--36},
title = {{PHYSMISM : A Control Interface for Creative Exploration of Physical Models}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}031.pdf},
year = {2007}
}
@inproceedings{Hauert2007,
address = {New York City, NY, United States},
author = {Hauert, Sibylle and Reichmuth, Daniel and B{\"{o}}hm, Volker},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {nime07},
pages = {422},
title = {{Instant City, a Music Building Game Table}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}422.pdf},
year = {2007}
}
@inproceedings{Historia2007,
address = {New York City, NY, United States},
author = {{\'{A}}lvarez-Fern{\'{a}}ndez, Miguel and Kersten, Stefan and Piascik, Asia},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {nime07},
pages = {432},
title = {{Soundanism}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}432.pdf},
year = {2007}
}
@inproceedings{Rigler2007,
abstract = {The Music Cre8tor is an interactive music composition systemcontrolled by motion sensors specifically designed forchildren with disabilities although not exclusively for thispopulation. The player(s) of the Music Cre8tor can either holdor attach accelerometer sensors to trigger a variety ofcomputer-generated sounds, MIDI instruments and/or prerecorded sound files. The sensitivity of the sensors can bemodified for each unique individual so that even the smallestmovement can control a sound. The flexibility of the systemis such that either four people can play simultaneously and/orone or more players can use up to four sensors. The originalgoal of this program was to empower students with disabilitiesto create music and encourage them to perform with othermusicians, however this same goal has expanded to includeother populations.},
address = {New York City, NY, United States},
author = {Rigler, Jane and Seldess, Zachary},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Music Education,disabilities,interactive performance.,motion sensors,music composition,special education},
pages = {415--416},
title = {{The Music Cre8tor : an Interactive System for Musical Exploration and Education}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}415.pdf},
year = {2007}
}
@inproceedings{Biggs2007,
address = {New York City, NY, United States},
author = {Biggs, Betsey},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {nime07},
pages = {424},
title = {{The Tipping Point}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}424.pdf},
year = {2007}
}
@inproceedings{Sirguy2007,
abstract = {Eowave and Ircam have been deeply involved into gestureanalysis and sensing for a few years by now, as severalartistic projects demonstrate (1). In 2004, Eowave has beenworking with Ircam on the development of the Eobodysensor system, and since that, Eowave's range of sensors hasbeen increased with new sensors sometimes developed innarrow collaboration with artists for custom sensor systemsfor installations and performances. This demo-paperdescribes the recent design of a new USB/MIDI-to-sensorinterface called Eobody2.},
address = {New York City, NY, United States},
author = {Sirguy, Marc and Gallin, Emmanuelle},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Computer music,Gestural controller,MIDI,Motors,Relays,Robots,Sensor,USB,Wireless.},
pages = {401--402},
title = {{Eobody2 : A Follow-up to Eobody's Technology}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}401.pdf},
year = {2007}
}
@inproceedings{Kim2007,
address = {New York City, NY, United States},
author = {Kim, Juno and Schiemer, Greg and Narushima, Terumi},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {1,algorithmic composition,expressive control interfaces,eye movement recording,microtonal tuning,midi,nime07,pure data,video},
pages = {50--55},
title = {{Oculog : Playing with Eye Movements}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}050.pdf},
year = {2007}
}
@inproceedings{Bull2007,
address = {New York City, NY, United States},
author = {Bull, Steve and Gresham-Lancaster, Scot and Mintchev, Kalin and Svoboda, Terese},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {nime07},
pages = {420},
title = {{Cellphonia : WET}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}420.pdf},
year = {2007}
}
@inproceedings{Camurri2007a,
abstract = {EyesWeb XMI (for eXtended Multimodal Interaction) is the new version of the well-known EyesWeb platform. It has a main focus on multimodality and the main design target of this new release has been to improve the ability to process and correlate several streams of data. It has been used extensively to build a set of interactive systems for performing arts applications for Festival della Scienza 2006, Genoa, Italy. The purpose of this paper is to describe the developed installations as well as the new EyesWeb features that helped in their development.},
address = {New York City, NY, United States},
author = {Camurri, Antonio and Coletta, Paolo and Varni, Giovanna and Ghisio, Simone},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {EyesWeb,multimodal interactive systems,performing arts.},
pages = {305--308},
title = {{Developing Multimodal Interactive Systems with EyesWeb XMI}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}305.pdf},
year = {2007}
}
@inproceedings{Bencina2008,
address = {Genoa, Italy},
author = {Bencina, Ross and Wilde, Danielle and Langley, Somaya},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {gestural control,mapping,nime08,prototyping,three-axis accelerometers,vocal,wii remote},
pages = {197--202},
title = {{Gesture=Sound Experiments : Process and Mappings}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}197.pdf},
year = {2008}
}
@inproceedings{Roma2008,
abstract = {We present an audio waveform editor that can be operated in real time through a tabletop interface. The systemcombines multi-touch and tangible interaction techniques inorder to implement the metaphor of a toolkit that allows direct manipulation of a sound sample. The resulting instrument is well suited for live performance based on evolvingloops.},
address = {Genoa, Italy},
author = {Roma, Gerard and Xamb{\'{o}}, Anna},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {interaction techniques,musical performance,tabletop interface,tangible interface},
pages = {249--252},
title = {{A Tabletop Waveform Editor for Live Performance}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}249.pdf},
year = {2008}
}
@inproceedings{Favilla2008,
abstract = {This demonstration presents three new augmented and metasaxophone interface/instruments, built by the Bent LeatherBand. The instruments are designed for virtuosic liveperformance and make use of Sukandar Kartadinata's Gluion[OSC] interfaces. The project rationale and research outcomesfor the first twelve months is discussed. Instruments/interfacesdescribed include the Gluisop, Gluialto and Leathersop.},
address = {Genoa, Italy},
author = {Favilla, Stuart and Cannon, Joanne and Hicks, Tony and Chant, Dale and Favilla, Paris},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Augmented saxophone,Gluion,OSC,virtuosic performance systems},
pages = {366--369},
title = {{Gluisax : Bent Leather Band's Augmented Saxophone Project}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}366.pdf},
year = {2008}
}
@inproceedings{Bouenard2008,
abstract = {A new interface for visualizing and analyzing percussion gestures is presented, proposing enhancements of existing motion capture analysis tools. This is achieved by offering apercussion gesture analysis protocol using motion capture.A virtual character dynamic model is then designed in order to take advantage of gesture characteristics, yielding toimprove gesture analysis with visualization and interactioncues of different types.},
address = {Genoa, Italy},
author = {Bou{\"{e}}nard, Alexandre and Gibet, Sylvie and Wanderley, Marcelo M},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Gesture and sound,interaction.,interface,percussion gesture,virtual character},
pages = {38--43},
title = {{Enhancing the Visualization of Percussion Gestures by Virtual Character Animation}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}038.pdf},
year = {2008}
}
@inproceedings{Schedel2008,
abstract = {The Color of Waiting is an interactive theater workwith music, dance, and video which was developed atSTEIM in Amsterdam and further refined at CMMASin Morelia Mexico with funding from Meet theComposer. Using Max/MSP/ Jitter a cellist is able tocontrol sound and video during the performancewhile performing a structured improvisation inresponse to the dancer's movement. In order toensure. repeated performances of The Color o fWaiting , Kinesthetech Sense created the scorecontained in this paper. Performance is essential tothe practice of time-based art as a living form, buthas been complicated by the unique challenges ininterpretation and re-creation posed by worksincorporating technology. Creating a detailed scoreis one of the ways artists working with technologycan combat obsolescence.},
address = {Genoa, Italy},
author = {Schedel, Margaret and Rootberg, Alison and de Martelly, Elizabeth},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {nime08},
pages = {339--342},
title = {{Scoring an Interactive, Multimedia Performance Work}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}339.pdf},
year = {2008}
}
@inproceedings{Kallblad2008,
abstract = {It started with an idea to create an empty space in which you activated music and light as you moved around. In responding to the music and lighting you would activate more or different sounds and thereby communicate with the space through your body. This led to an artistic research project in which children's spontaneous movement was observed, a choreography made based on the children's movements and music written and recorded for the choreography. This music was then decomposed and choreographed into an empty space at Botkyrka konsthall creating an interactive dance installation. It was realized using an interactive sound and light system in which 5 video cameras were detecting the motion in the room connected to a 4-channel sound system and a set of 14 light modules. During five weeks people of all ages came to dance and move around in the installation. The installation attracted a wide range of people of all ages and the tentative evaluation indicates that it was very positively received and that it encouraged free movement in the intended way. Besides observing the activity in the installation interviews were made with schoolchildren age 7 who had participated in the installation.},
address = {Genoa, Italy},
author = {K{\"{a}}llblad, Anna and Friberg, Anders and Svensson, Karl and Edelholm, Elisabet S},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Installation,children's movement,dance,interactive multimedia,video recognition},
pages = {128--133},
title = {{Hoppsa Universum -- An Interactive Dance Installation for Children}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}128.pdf},
year = {2008}
}
@inproceedings{Dimitrov2008,
abstract = {This paper reports on a Short-Term Scientific Mission (STSM)sponsored by the Sonic Interaction Design (SID) EuropeanCOST Action IC601.Prototypes of objects for the novel instrument Reactablewere developed, with the goal of studying sonification ofmovements on this platform using physical models. A physical model of frictional interactions between rubbed dry surfaces was used as an audio generation engine, which alloweddevelopment in two directions - a set of objects that affordsmotions similar to sliding, and a single object aiming tosonify contact friction sound. Informal evaluation was obtained from a Reactable expert user, regarding these sets ofobjects. Experiments with the objects were also performed- related to both audio filtering, and interfacing with otherobjects for the Reactable.},
address = {Genoa, Italy},
author = {Dimitrov, Smilen and Alonso, Marcos and Serafin, Stefania},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Reactable,contact fric- tion,motion sonification,physical model},
pages = {211--214},
title = {{Developing Block-Movement, Physical-Model Based Objects for the Reactable}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}211.pdf},
year = {2008}
}
@inproceedings{Bau2008,
abstract = {We combine two concepts, the musical instrument as metaphorand technology probes, to explore how tangible interfaces canexploit the semantic richness of sound. Using participatorydesign methods from Human-Computer Interaction (HCI), wedesigned and tested the A20, a polyhedron-shaped, multichannel audio input/output device. The software maps soundaround the edges and responds to the user's gestural input,allowing both aural and haptic modes of interaction as well asdirect manipulation of media content. The software is designedto be very flexible and can be adapted to a wide range ofshapes. Our tests of the A20's perceptual and interactionproperties showed that users can successfully detect soundplacement, movement and haptic effects on this device. Ourparticipatory design workshops explored the possibilities of theA20 as a generative tool for the design of an extended,collaborative personal music player. The A20 helped users toenact scenarios of everyday mobile music player use and togenerate new design ideas.},
address = {Genoa, Italy},
author = {Bau, Olivier and Tanaka, Atau and Mackay, Wendy E},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Generative design tools,Instrument building,Multi-faceted audio,Personal music devices,Tangible user interfaces,Technology probes},
pages = {91--96},
title = {{The A20 : Musical Metaphors for Interface Design}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}091.pdf},
year = {2008}
}
@inproceedings{Endo2008,
abstract = {We developed a rhythmic instruments ensemble simulator generating animation using game controllers. The motion of a player is transformed into musical expression data of MIDI to generate sounds, and MIDI data are transformed into animation control parameters to generate movies. These animations and music are shown as the reflection of player performance. Multiple players can perform a musical ensemble to make more varied patterns of animation. Our system is so easy that everyone can enjoy performing a fusion of music and animation.},
address = {Genoa, Italy},
author = {Endo, Ayaka and Kuhara, Yasuo},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Flash movie,Gesture music and animation.,MIDI,Max/MSP,Wii Remote,Wireless game controller},
pages = {345--346},
title = {{Rhythmic Instruments Ensemble Simulator Generating Animation Movies Using {\{}Bluetooth{\}} Game Controller}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}345.pdf},
year = {2008}
}
@inproceedings{Platz2008,
abstract = {Many mobile devices, specifically mobile phones, come equipped with a microphone. Microphones are high-fidelity sensors that can pick up sounds relating to a range of physical phenomena. Using simple feature extraction methods,parameters can be found that sensibly map to synthesis algorithms to allow expressive and interactive performance.For example blowing noise can be used as a wind instrument excitation source. Also other types of interactionscan be detected via microphones, such as striking. Hencethe microphone, in addition to allowing literal recording,serves as an additional source of input to the developingfield of mobile phone performance.},
address = {Genoa, Italy},
author = {Misra, Ananya and Essl, Georg and Rohs, Michael},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {microphone,mobile music making,mobile-stk},
pages = {185--188},
title = {{Microphone as Sensor in Mobile Phone Performance}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}185.pdf},
year = {2008}
}
@inproceedings{Kamiyama2008,
address = {Genoa, Italy},
author = {Kamiyama, Yusuke and Tanaka, Mai and Tanaka, Hiroya},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {3D sound system,musical expression,sound generating device,sound-field arrangement.,umbrella},
pages = {352--353},
title = {{Oto-Shigure : An Umbrella-Shaped Sound Generator for Musical Expression}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}352.pdf},
year = {2008}
}
@inproceedings{Young2008,
address = {Genoa, Italy},
author = {Young, Diana},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {bowing,classification,gesture,playing technique,principal component anal- ysis},
pages = {44--48},
title = {{Classification of Common Violin Bowing Techniques Using Gesture Data from a Playable Measurement System}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}044.pdf},
year = {2008}
}
@inproceedings{Camurri2008,
address = {Genoa, Italy},
author = {Camurri, Antonio and Canepa, Corrado and Coletta, Paolo and Mazzarino, Barbara and Volpe, Gualtiero},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Active listening of music,collaborative environments,expressive interfaces,full-body motion analysis and expressive gesture,multimodal interactive systems for music and perf,social interaction.},
pages = {134--139},
title = {{Mappe per Affetti Erranti : a Multimodal System for Social Active Listening and Expressive Performance}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}134.pdf},
year = {2008}
}
@inproceedings{Lanzalone2008,
address = {Genoa, Italy},
author = {Lanzalone, Silvia},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {nime08},
pages = {273--276},
title = {{The 'Suspended Clarinet' with the 'Uncaused Sound' : Description of a Renewed Musical Instrument}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}273.pdf},
year = {2008}
}
@inproceedings{Sjuve2008,
abstract = {This paper describes the development of a wireless wearablecontroller, GO, for both sound processing and interactionwith wearable lights. Pure Data is used for sound processing.The GO prototype is built using a PIC microcontroller usingvarious sensors for receiving information from physicalmovements.},
address = {Genoa, Italy},
author = {Sjuve, Eva},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Gestural interface,Interactive Lights.,Pure Data,Wireless controller},
pages = {362--363},
title = {{Prototype GO : Wireless Controller for Pure Data}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}362.pdf},
year = {2008}
}
@inproceedings{Hashida2008,
abstract = {One of the advantages of case-based systems is that theycan generate expressions even if the user doesn't know howthe system applies expression rules. However, the systemscannot avoid the problem of data sparseness and do notpermit a user to improve the expression of a certain part ofa melody directly. After discussing the functions requiredfor user-oriented interface for performance rendering systems, this paper proposes a directable case-based performance rendering system, called Itopul. Itopul is characterized by 1) a combination of the phrasing model and thepulse model, 2) the use of a hierarchical music structure foravoiding from the data sparseness problem, 3) visualizationof the processing progress, and 4) music structures directlymodifiable by the user.},
address = {Genoa, Italy},
author = {Hashida, Mitsuyo and Ito, Yosuke and Katayose, Haruhiro},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Case-based Approach,Performance Rendering,User Interface},
pages = {277--280},
title = {{A Directable Performance Rendering System : Itopul}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}277.pdf},
year = {2008}
}
@inproceedings{Suzuki2008,
abstract = {This research aims to develop a novel instrument for sociomusical interaction where a number of participants can produce sounds by feet in collaboration with each other. Thedeveloped instrument, beacon, is regarded as embodied soundmedia product that will provide an interactive environmentaround it. The beacon produces laser beams lying on theground and rotating. Audio sounds are then produced whenthe beams pass individual performer's foot. As the performers are able to control the pitch and sound length accordingto the foot location and angles facing the instrument, theperformer's body motion and foot behavior can be translated into sound and music in an intuitive manner.},
address = {Genoa, Italy},
author = {Suzuki, Kenji and Kyoya, Miho and Kamatani, Takahiro and Uchiyama, Toshiaki},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Embodied sound media,Hyper-instrument,Laser beams},
pages = {360--361},
title = {{beacon : Embodied Sound Media Environment for Socio-Musical Interaction}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}360.pdf},
year = {2008}
}
@inproceedings{Butler2008,
abstract = {In this paper I discuss the importance of and need forpedagogical materials to support the development of newinterfaces and new instruments for electronic music. I describemy method for creating a graduated series of pedagogicaletudes composed using Max/MSP. The etudes will helpperformers and instrument designers learn the most commonlyused basic skills necessary to perform with interactiveelectronic music instruments. My intention is that the finalseries will guide a beginner from these initial steps through agraduated method, eventually incorporating some of the moreadvanced techniques regularly used by electronic musiccomposers.I describe the order of the series, and discuss the benefits (bothto performers and to composers) of having a logical sequence ofskill-based etudes. I also connect the significance of skilledperformers to the development of two essential areas that Iperceive are still just emerging in this field: the creation of acomposed repertoire and an increase in musical expressionduring performance.},
address = {Genoa, Italy},
author = {Butler, Jennifer},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {composition,etudes,max,msp,musical controllers,musical expression,nime08,pedagogy,repertoire},
pages = {77--80},
title = {{Creating Pedagogical Etudes for Interactive Instruments}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}077.pdf},
year = {2008}
}
@inproceedings{Kiefer2008,
abstract = {There is small but useful body of research concerning theevaluation of musical interfaces with HCI techniques. Inthis paper, we present a case study in implementing thesetechniques; we describe a usability experiment which evaluated the Nintendo Wiimote as a musical controller, andreflect on the effectiveness of our choice of HCI methodologies in this context. The study offered some valuable results,but our picture of the Wiimote was incomplete as we lackeddata concerning the participants' instantaneous musical experience. Recent trends in HCI are leading researchers totackle this problem of evaluating user experience; we reviewsome of their work and suggest that with some adaptation itcould provide useful new tools and methodologies for computer musicians.},
address = {Genoa, Italy},
author = {Kiefer, Chris and Collins, Nick and Fitzpatrick, Geraldine},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Evaluating Musical Interac- tion,HCI Methodology,Wiimote},
pages = {87--90},
title = {{HCI Methodology For Evaluating Musical Controllers : A Case Study}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}087.pdf},
year = {2008}
}
@inproceedings{Kimura2008,
abstract = {This paper describes the compositional process for creatingthe interactive work for violin entitled VITESSIMO using theAugmented Violin [1].},
address = {Genoa, Italy},
author = {Kimura, Mari},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Augmented Violin,gesture tracking,interactive performance},
pages = {219--220},
title = {{Making of VITESSIMO for Augmented Violin : Compositional Process and Performance}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}219.pdf},
year = {2008}
}
@inproceedings{Ward2008,
abstract = {This paper presents a comparison of the movement styles of two theremin players based on observation and analysis of video recordings. The premise behind this research is that a consideration of musicians' movements could form the basis for a new framework for the design of new instruments. Laban Movement Analysis is used to qualitatively analyse the movement styles of the musicians and to argue that the Recuperation phase of their phrasing is essential to achieve satisfactory performance.},
address = {Genoa, Italy},
author = {Ward, Nicholas and Penfield, Kedzie and O'Modhrain, Sile and Knapp, R Benjamin},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Effort Phrasing,Laban Movement Analysis,Recuperation,Theremin},
pages = {117--121},
title = {{A Study of Two Thereminists : Towards Movement Informed Instrument Design}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}117.pdf},
year = {2008}
}
@inproceedings{Flanigan2008,
abstract = {Plink Jet is a robotic musical instrument made from scavenged inkjet printers and guitar parts. We investigate the expressive capabilities of everyday machine technology by recontextualizing the relatively high-tech mechanisms of typical office debris into an electro-acoustic musical instrument. We also explore the performative relationship between human and machine.},
address = {Genoa, Italy},
author = {Flanigan, Lesley and Doro, Andrew},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Automation,DIY,Infra-Instrument,Interaction Design,Performing Technology,Repurposing of Consumer Technology,Robotics},
pages = {349--351},
title = {{Plink Jet}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}349.pdf},
year = {2008}
}
@inproceedings{Canazza2008,
abstract = {Musical open works can be often thought like sequences of musical structures, which can be arranged by anyone who had access to them and who wished to realize the work. This paper proposes an innovative agent-based system to model the information and organize it in structured knowledge; to create effective, graph-centric browsing perspectives and views for the user; to use , , authoring tools for the performance of open work of electro-acoustic music.},
address = {Genoa, Italy},
author = {Canazza, Sergio and Dattolo, Antonina},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Multimedia Information Systems,Musical Open Work,Software Agents,zz-structures.},
pages = {140--143},
title = {{New Data Structure for Old Musical Open Works}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}140.pdf},
year = {2008}
}
@inproceedings{PalacioQuintin2008,
abstract = {After eight years of practice on the first hyper-flute prototype (a flute extended with sensors), this article presentsa retrospective of its instrumental practice and the newdevelopments planned from both technological and musical perspectives. Design, performance skills, and mappingstrategies are discussed, as well as interactive compositionand improvisation.},
address = {Genoa, Italy},
author = {Palacio-Quintin, Cl{\'{e}}o},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {composition,gestural control,hyper-flute,hyper-instruments,improvisation,interactive music,mapping,nime08,sensors},
pages = {293--298},
title = {{Eight Years of Practice on the Hyper-Flute : Technological and Musical Perspectives}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}293.pdf},
year = {2008}
}
@inproceedings{Zoran2008,
address = {Genoa, Italy},
author = {Zoran, Amit and Maes, Pattie},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {nime08},
pages = {67--70},
title = {{Considering Virtual {\&} Physical Aspects in Acoustic Guitar Design}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}067.pdf},
year = {2008}
}
@inproceedings{Wang2009a,
address = {Pittsburgh, PA, United States},
author = {Wang, Ge and Fiebrink, Rebecca},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {nime09},
pages = {334},
title = {{PLOrk Beat Science 2.0}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}334.pdf},
year = {2009}
}
@inproceedings{McDonald2009,
abstract = {The Vibrobyte is a wireless haptic interface specialized forco-located musical performance. The hardware is designedaround the open source Arduino platform, with haptic control data encapsulated in OSC messages, and OSC/hardwarecommunications handled by Processing. The Vibrobyte wasfeatured at the International Computer Music Conference2008 (ICMC) in a telematic performance between ensembles in Belfast, Palo Alto (California, USA), and Troy (NewYork, USA).},
address = {Pittsburgh, PA, United States},
author = {McDonald, Kyle and Kouttron, Dane and Bahn, Curtis and Braasch, Jonas and Oliveros, Pauline},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {haptics,interface,nime09,performance,telematic},
pages = {41--42},
title = {{The Vibrobyte : A Haptic Interface for Co-Located Performance}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}041.pdf},
year = {2009}
}
@inproceedings{Ferguson2009,
address = {Pittsburgh, PA, United States},
author = {Ferguson, Sam and Beilharz, Kirsty},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Auditory Display.,Interactive Sonification,Sonification},
pages = {35--36},
title = {{An Interface for Live Interactive Sonification}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}035.pdf},
year = {2009}
}
@inproceedings{Johnston2009,
abstract = {In this paper we describe an interaction framework whichclassifies musicians' interactions with virtual musical instruments into three modes: instrumental, ornamental andconversational. We argue that conversational interactionsare the most difficult to design for, but also the most interesting. To illustrate our approach to designing for conversational interactions we describe the performance workPartial Reflections 3 for two clarinets and interactive software. This software uses simulated physical models to create a virtual sound sculpture which both responds to andproduces sounds and visuals.},
address = {Pittsburgh, PA, United States},
author = {Johnston, Andrew and Candy, Linda and Edmonds, Ernest},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Music,instruments,interaction.},
pages = {207--212},
title = {{Designing for Conversational Interaction}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}207.pdf},
year = {2009}
}
@inproceedings{Fiebrink2009,
abstract = {Supervised learning methods have long been used to allow musical interface designers to generate new mappings by example. We propose a method for harnessing machine learning algorithms within a radically interactive paradigm, in which the designer may repeatedly generate examples, train a learner, evaluate outcomes, and modify parameters in real-time within a single software environment. We describe our meta-instrument, the Wekinator, which allows a user to engage in on-the-fly learning using arbitrary control modalities and sound synthesis environments. We provide details regarding the system implementation and discuss our experiences using the Wekinator for experimentation and performance.},
address = {Pittsburgh, PA, United States},
author = {Fiebrink, Rebecca and Trueman, Dan and Cook, Perry R},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Machine learning,mapping,tools.},
pages = {280--285},
title = {{A Meta-Instrument for Interactive, On-the-Fly Machine Learning}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}280.pdf},
year = {2009}
}
@inproceedings{Siwiak2009,
abstract = {Catch Your Breath is an interactive audiovisual bio-feedbacksystem adapted from a project designed to reduce respiratory irregularity in patients undergoing 4D CT scans for oncological diagnosis. The system is currently implementedand assessed as a potential means to reduce motion-induceddistortion in CT images.A museum installation based on the same principle wascreated in which an inexpensive wall-mounted web camera tracks an IR sensor embedded into a pendant worn bythe user. The motion of the subjects breathing is trackedand interpreted as a real-time variable tempo adjustment toa stored musical file. The subject can then adjust his/herbreathing to synchronize with a separate accompanimentline. When the breathing is regular and is at the desiredtempo, the audible result sounds synchronous and harmonious. The accompaniment's tempo progresses and gradually decrease which causes the breathing to synchronize andslow down, thus increasing relaxation.},
address = {Pittsburgh, PA, United States},
author = {Siwiak, Diana and Berger, Jonathan and Yang, Yao},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {auditory display.,music,sensor},
pages = {153--154},
title = {{Catch Your Breath - Musical Biofeedback for Breathing Regulation}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}153.pdf},
year = {2009}
}
@inproceedings{StClair2009,
address = {Pittsburgh, PA, United States},
author = {{St. Clair}, Michael and Leitman, Sasha},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {expression,in doing so,installation,interactive,it also opens up,music,new territories for,nime09,play,playful physical motions into,playground,radical collaboration,real-time,the realm of artistic},
pages = {293--296},
title = {{PlaySoundGround : An Interactive Musical Playground}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}293.pdf},
year = {2009}
}
@inproceedings{Min2009,
abstract = {It is surely not difficult for anyone with experience in thesubject known as Music Theory to realize that there is avery definite and precise relationship between music andmathematics. This paper describes the SoriSu, a newelectronic musical instrument based on Sudoku puzzles,which probe the expressive possibilities of mathematicalconcepts in music. The concept proposes a new way ofmapping numbers to sound. This interface was designed toprovide easy and pleasing access to music for users whoare unfamiliar or uncomfortable with current musicaldevices. The motivation behind the project is presented, aswell as hardware and software design.},
address = {Pittsburgh, PA, United States},
author = {Min, Hye Ki},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Game Interfaces,Mathematics and Sound,Mathematics in Music,Numbers,Puzzles,Tangible User Interfaces.},
pages = {82--85},
title = {{SORISU : Sound with Numbers}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}082.pdf},
year = {2009}
}
@inproceedings{Baalman2009a,
address = {Pittsburgh, PA, United States},
author = {Baalman, Marije A and Smoak, Harry C and Salter, Christopher L and Malloch, Joseph and Wanderley, Marcelo M},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Data exchange,Max/MSP Introduction and Background The SenseWorl,Open- SoundControl,SuperCollider,collaborative performance,etc). While the,in- teractive performance,interactive art works,lighting,mechatronics,sensor data,show control},
pages = {131--134},
title = {{Sharing Data in Collaborative, Interactive Performances : the SenseWorld DataNetwork}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}131.pdf},
year = {2009}
}
@inproceedings{Nakra2009,
abstract = {The UBS Virtual Maestro is an interactive conducting system designed by Immersion Music to simulate the experience of orchestral conducting for the general public attending a classical music concert. The system utilizes the Wii Remote, which users hold and move like a conducting baton to affect the tempo and dynamics of an orchestral video/audio recording. The accelerometer data from the Wii Remote is used to control playback speed and volume in real-time. The system is housed in a UBSbranded kiosk that has toured classical performing arts venues throughout the United States and Europe in 2007 and 2008. In this paper we share our experiences in designing this standalone system for thousands of users, and lessons that we learned from the project.},
address = {Pittsburgh, PA, United States},
author = {Nakra, Teresa M and Ivanov, Yuri and Smaragdis, Paris and Ault, Chris},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Wii Remote,conducting,gesture,interactive installations},
pages = {250--255},
title = {{The UBS Virtual Maestro : an Interactive Conducting System}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}250.pdf},
year = {2009}
}
@inproceedings{Keith2009,
abstract = {Deviate generates multiple streams of melodic and rhythmic output in real-time, according to user-specified control parameters. This performance system has been implemented using Max 5 [1] within the genre of popular contemporary electronic music, incorporating techno, IDM, and related forms. The aim of this project is not musical style synthesis, but to construct an environment in which a range of creative and musical goals may be achieved. A key aspect is control over generative processes, as well as consistent yet varied output. An approach is described which frees the user from determining note-level output while allowing control to be maintained over larger structural details, focusing specifically on the melodic aspect of this system. Audio examples are located online at http://www.cetenbaath.com/cb/about-deviate/.},
address = {Pittsburgh, PA, United States},
author = {Keith, Sarah},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {generative,laptop,performance,popular music},
pages = {54--55},
title = {{Controlling Live Generative Electronic Music with Deviate}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}054.pdf},
year = {2009}
}
@inproceedings{Gillian2009a,
abstract = {This paper presents "Scratch-Off", a new musical multiplayer DJ game that has been designed for a mobile phone. We describe how the game is used as a test platform for experimenting with various types of multimodal feedback. The game uses movement gestures made by the players to scratch a record and control crossfades between tracks, with the objective of the game to make the correct scratch at the correct time in relation to the music. Gestures are detected using the devices built-in tri-axis accelerometer and multi-touch screen display. The players receive visual, audio and various types of vibrotactile feedback to help them make the correct scratch on the beat of the music track. We also discuss the results of a pilot study using this interface.},
address = {Pittsburgh, PA, United States},
author = {Gillian, Nicholas and O'Modhrain, Sile and Essl, Georg},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Mobile devices,audio games.,gesture},
pages = {308--311},
title = {{Scratch-Off : A Gesture Based Mobile Music Game with Tactile Feedback}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}308.pdf},
year = {2009}
}
@inproceedings{Nicolls2009,
address = {Pittsburgh, PA, United States},
author = {Nicolls, Sarah},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {gestural,interactive,motors,performance,piano,sensor,technology},
pages = {203--206},
title = {{Twenty-First Century Piano}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}203.pdf},
year = {2009}
}
@inproceedings{Pedrosa2009,
abstract = {Haptic feedback is an important element that needs to be carefully designed in computer music interfaces. This paper presents an evaluation of several force renderings for target acquisition in space when used to support a music related task. The study presented here addresses only one musical aspect: the need to repeat elements accurately in time and in content. Several force scenarios will be rendered over a simple 3D target acquisition task and users' performance will be quantitatively and qualitatively evaluated. The results show how the users' subjective preference for a particular kind of force support does not always correlate to a quantitative measurement of performance enhancement. We describe a way in which a control mapping for a musical interface could be achieved without contradicting the users' preferences as obtained from the study.},
address = {Pittsburgh, PA, United States},
author = {Pedrosa, Ricardo and Maclean, Karon E},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {comfort,force feedback,music interfaces,target acquisition.,tempo},
pages = {19--24},
title = {{Evaluation of {\{}3D{\}} Haptic Target Rendering to Support Timing in Music Tasks}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}019.pdf},
year = {2009}
}
@inproceedings{Torre2009,
address = {Pittsburgh, PA, United States},
author = {Torre, Giuseppe and Sazdov, Robert and Konczewska, Dorota},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {nime09},
pages = {330},
title = {{MOLITVA --- Composition for Voice, Live Electronics, Pointing-At Glove Device and {\{}3-D{\}} Setup of Speakers}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}330.pdf},
year = {2009}
}
@inproceedings{Weinberg2009a,
abstract = {This paper presents an interactive and improvisational jam session, including human players and two robotic musicians. The project was developed in an effort to create novel and inspiring music through human-robot collaboration. The jam session incorporates Shimon, a newly-developed socially-interactive robotic marimba player, and Haile, a perceptual robotic percussionist developed in previous work. The paper gives an overview of the musical perception modules, adaptive improvisation modes and human-robot musical interaction models that were developed for the session. The paper also addresses the musical output that can be created from increased interconnections in an expanded multiple-robot multiplehuman ensemble, and suggests directions for future work.},
address = {Pittsburgh, PA, United States},
author = {Weinberg, Gil and Blosser, Brian and Mallikarjuna, Trishul and Raman, Aparna},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Haile.,Robotic musicianship,Shimon},
pages = {70--73},
title = {{The Creation of a Multi-Human, Multi-Robot Interactive Jam Session}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}070.pdf},
year = {2009}
}
@inproceedings{Gillian2009,
abstract = {This paper presents the SARC EyesWeb Catalog (SEC), agroup of blocks designed for real-time gesture recognitionthat have been developed for the open source program EyesWeb. We describe how the recognition of real-time bodymovements can be used for musician-computer-interaction.},
address = {Pittsburgh, PA, United States},
author = {Gillian, Nicholas and Knapp, R Benjamin and O'Modhrain, Sile},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {SARC EyesWeb Catalog,gesture recognition},
pages = {60--61},
title = {{The {\{}SAR{\}}C EyesWeb Catalog : A Pattern Recognition Toolbox for Musician-Computer Interaction}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}060.pdf},
year = {2009}
}
@inproceedings{Jessop2009,
abstract = {This paper describes The Vocal Augmentation and Manipulation Prosthesis (VAMP) a gesture-based wearable controller for live-time vocal performance. This controller allows a singer to capture and manipulate single notes that he or she sings, using a gestural vocabulary developed from that of choral conducting. By drawing from a familiar gestural vocabulary, this controller and the associated mappings can be more intuitive and expressive for both performer and audience.},
address = {Pittsburgh, PA, United States},
author = {Jessop, Elena},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {conducting.,gestural control,musical expressivity,vocal performance},
pages = {256--259},
title = {{The Vocal Augmentation and Manipulation Prosthesis (VAMP): A Conducting-Based Gestural Controller for Vocal Performance}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}256.pdf},
year = {2009}
}
@inproceedings{Gallin2009,
address = {Pittsburgh, PA, United States},
author = {Gallin, Emmanuelle and Sirguy, Marc},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Computer Music,Controller,MIDI,Sensor,USB,ribbon cello.,ribbon controllers},
pages = {199--202},
title = {{Sensor Technology and the Remaking of Instruments from the Past}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}199.pdf},
year = {2009}
}
@inproceedings{Kanda2009,
address = {Pittsburgh, PA, United States},
author = {Kanda, Ryo and Hashida, Mitsuyo and Katayose, Haruhiro},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Interaction,audience,gesture.,performer,physical,sen- sor,visualize},
pages = {45--47},
title = {{Mims : Interactive Multimedia Live Performance System}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}045.pdf},
year = {2009}
}
@inproceedings{Gong2009,
abstract = {In this project, we have developed a real-time writing instrument for music control. The controller, MusicGrip, can capture the subtle dynamics of the user's grip while writing or drawing and map this to musical control signals and sonic outputs. This paper discusses this conversion of the common motor motion of handwriting into an innovative form of music expression. The presented example instrument can be used to integrate the composing aspect of music with painting and writing, creating a new art form from the resultant aural and visual representation of the collaborative performing process.},
address = {Pittsburgh, PA, United States},
author = {Gong, Nan-Wei and Laibowitz, Mat and Paradiso, Joseph A},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Interactive music control,MIDI,group performing activity.,pen controller,writing instrument},
pages = {74--77},
title = {{MusicGrip : A Writing Instrument for Music Control}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}074.pdf},
year = {2009}
}
@inproceedings{Dubois2009,
address = {Pittsburgh, PA, United States},
author = {DuBois, Luke and Flanigan, Lesley},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {nime09},
pages = {336},
title = {{Bioluminescence}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}336.pdf},
year = {2009}
}
@inproceedings{Lyon2009,
address = {Pittsburgh, PA, United States},
author = {Lyon, Eric and Knapp, R Benjamin and Ouzounian, Gascia},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {nime09},
pages = {327},
title = {{Biomuse Trio}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}327.pdf},
year = {2009}
}
@inproceedings{Fels2009,
address = {Pittsburgh, PA, United States},
author = {Fels, Sidney and Pritchard, Bob and Lenters, Allison},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {nime09},
pages = {274--275},
title = {{ForTouch : A Wearable Digital Ventriloquized Actor}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}274.pdf},
year = {2009}
}
@inproceedings{Gelineck2009,
abstract = {This paper presents a HCI inspired evaluation of simple physical interfaces used to control physical models. Specifically knobs and sliders are compared in a creative and exploratory framework, which simulates the natural environment in which an electronic musician would normally explore a new instrument. No significant difference was measured between using knobs and sliders for controlling parameters of a physical modeling electronic instrument. Thereported difference between the tested instruments were mostlydue to the sound synthesis models.},
address = {Pittsburgh, PA, United States},
author = {Gelineck, Steven and Serafin, Stefania},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Affordances.,Creativ- ity,Electronic Musicians,Evaluation,Exploration,Interfaces,Knobs,Physi- cal Modeling,Sliders},
pages = {13--18},
title = {{A Quantitative Evaluation of the Differences between Knobs and Sliders}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}013.pdf},
year = {2009}
}
@inproceedings{Baalman2009,
address = {Pittsburgh, PA, United States},
author = {Baalman, Marije A},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {nime09},
pages = {329},
title = {{Code LiveCode Live, or livecode Embodied}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}329.pdf},
year = {2009}
}
@inproceedings{Overholt2009,
address = {Pittsburgh, PA, United States},
author = {Overholt, Dan and Lahey, Byron and Hansen, Anne-Marie Skriver and Burleson, Winslow and {Norrgaard Jensen}, Camilla},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {nime09},
pages = {339},
title = {{Pendaphonics}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}339.pdf},
year = {2009}
}
@inproceedings{Kiefer2009,
address = {Pittsburgh, PA, United States},
author = {Kiefer, Chris and Collins, Nick and Fitzpatrick, Geraldine},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {nime09},
pages = {246--249},
title = {{Phalanger : Controlling Music Software With Hand Movement Using A Computer Vision and Machine Learning Approach}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}246.pdf},
year = {2009}
}
@inproceedings{Lai2009,
abstract = {Hands On Stage, designed from a percussionist's perspective, is a new performance interface designed for audiovisual improvisation. It comprises a custom-built table interface and a performance system programmed in two environments, SuperCollider 3 and Isadora. This paper traces the interface's evolution over matters of relevant technology, concept, construction, system design, and its creative outcomes.},
address = {Pittsburgh, PA, United States},
author = {Lai, Chi-Hsia},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {audiovisual,interface design,performance.},
pages = {39--40},
title = {{Hands On Stage : A Sound and Image Performance Interface}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}039.pdf},
year = {2009}
}
@inproceedings{Jones2009,
abstract = {The Fragmented Orchestra is a distributed musical instrument which combines live audio streams from geographically disparate sites, and granulates each according to thespike timings of an artificial spiking neural network. Thispaper introduces the work, outlining its historical context,technical architecture, neuronal model and network infrastructure, making specific reference to modes of interactionwith the public.},
address = {Pittsburgh, PA, United States},
author = {Jones, Daniel and Hodgson, Tim and Grant, Jane and Matthias, John and Outram, Nicholas and Ryan, Nick},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {distributed,emergent,environmental,installation,neural network,nime09,sound,streaming audio},
pages = {297--302},
title = {{The Fragmented Orchestra}},
url = {http://www.nime.org/proceedings/2009/nime2009{\_}297.pdf},
year = {2009}
}
@inproceedings{Fyfe2010,
abstract = {In this paper we discuss SurfaceMusic, a tabletop music system in which touch gestures are mapped to physical modelsof instruments. With physical models, parametric controlover the sound allows for a more natural interaction between gesture and sound. We discuss the design and implementation of a simple gestural interface for interactingwith virtual instruments and a messaging system that conveys gesture data to the audio system.},
address = {Sydney, Australia},
author = {Fyfe, Lawrence and Lynch, Sean and Hull, Carmen and Carpendale, Sheelagh},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Open Sound Control.,Tabletop,gesture,multi-touch,physical model},
pages = {360--363},
title = {{SurfaceMusic : Mapping Virtual Touch-based Instruments to Physical Models}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}360.pdf},
year = {2010}
}
@inproceedings{Stahl2010,
abstract = {Maintaining a sense of personal connection between increasingly synthetic performers and increasingly diffuse audiences is vital to storytelling and entertainment. Sonic intimacy is important, because voice is one of the highestbandwidth channels for expressing our real and imagined selves.New tools for highly focused spatialization could help improve acoustical clarity, encourage audience engagement, reduce noise pollution and inspire creative expression. We have a particular interest in embodied, embedded systems for vocal performance enhancement and transformation. This short paper describes work in progress on a toolkit for high-quality wearable sound suits. Design goals include tailored directionality and resonance, full bandwidth, and sensible ergonomics. Engineering details to accompany a demonstration of recent prototypes are presented, highlighting a novel magnetostrictive flextensional transducer. Based on initial observations we suggest that vocal acoustic output from the torso, and spatial perception of situated low frequency sources, are two areas deserving greater attention and further study.},
address = {Sydney, Australia},
author = {Stahl, Alex and Clemens, Patricia},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {magnetostrictive flextensional transducer,nime10,paralinguistics,sound reinforcement,spatialization,speech enhancement,transformation,voice,wearable systems},
pages = {427--430},
title = {{Auditory Masquing : Wearable Sound Systems for Diegetic Character Voices}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}427.pdf},
year = {2010}
}
@inproceedings{Bryan2010,
abstract = {The Mobile Music (MoMu) toolkit is a new open-sourcesoftware development toolkit focusing on musical interaction design for mobile phones. The toolkit, currently implemented for iPhone OS, emphasizes usability and rapidprototyping with the end goal of aiding developers in creating real-time interactive audio applications. Simple andunified access to onboard sensors along with utilities forcommon tasks found in mobile music development are provided. The toolkit has been deployed and evaluated in theStanford Mobile Phone Orchestra (MoPhO) and serves asthe primary software platform in a new course exploringmobile music.},
address = {Sydney, Australia},
author = {Bryan, Nicholas J and Herrera, Jorge and Oh, Jieun and Wang, Ge},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {iPhone,instrument design,mobile music,software develop- ment,toolkit},
pages = {174--177},
title = {{MoMu : A Mobile Music Toolkit}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}174.pdf},
year = {2010}
}
@inproceedings{BryanKinns2010,
abstract = {In this paper we outline the emerging field of Interactional Sound and Music which concerns itself with multi-person technologically mediated interactions primarily using audio. We present several examples of interactive systems in our group, and reflect on how they were designed and evaluated. Evaluation techniques for collective, performative, and task oriented activities are outlined and compared. We emphasise the importance of designing for awareness in these systems, and provide examples of different awareness mechanisms.},
address = {Sydney, Australia},
author = {Bryan-Kinns, Nick and Fencott, Robin and Metatla, Oussama and Nabavian, Shahin and Sheridan, Jennifer G},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Interactional,awareness.,collaboration,composition,improvisation,music,mutual engagement,sound},
pages = {403--406},
title = {{Interactional Sound and Music : Listening to CSCW, Sonification, and Sound Art}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}403.pdf},
year = {2010}
}
@inproceedings{Berthaut2010,
abstract = {We present Drile, a multiprocess immersive instrument built uponthe hierarchical live-looping technique and aimed at musical performance. This technique consists in creating musical trees whosenodes are composed of sound effects applied to a musical content.In the leaves, this content is a one-shot sound, whereas in higherlevel nodes this content is composed of live-recorded sequencesof parameters of the children nodes. Drile allows musicians tointeract efficiently with these trees in an immersive environment.Nodes are represented as worms, which are 3D audiovisual objects. Worms can be manipulated using 3D interaction techniques,and several operations can be applied to the live-looping trees. Theenvironment is composed of several virtual rooms, i.e. group oftrees, corresponding to specific sounds and effects. Learning Drileis progressive since the musical control complexity varies according to the levels in live-looping trees. Thus beginners may havelimited control over only root worms while still obtaining musically interesting results. Advanced users may modify the trees andmanipulate each of the worms.},
address = {Sydney, Australia},
author = {Berthaut, Florent and Desainte-Catherine, Myriam and Hachet, Martin},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {3D interac- tion,Drile,hierarchical live-looping,immersive instrument},
pages = {192--197},
title = {{DRILE : An Immersive Environment for Hierarchical Live-Looping}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}192.pdf},
year = {2010}
}
@inproceedings{Pan2010,
abstract = {This paper proposes a novel method to realize an initiativeexchange for robot. A humanoid robot plays vibraphone exchanging initiative with a human performer by perceivingmultimodal cues in real time. It understands the initiative exchange cues through vision and audio information.In order to achieve the natural initiative exchange betweena human and a robot in musical performance, we built thesystem and the software architecture and carried out the experiments for fundamental algorithms which are necessaryto the initiative exchange.},
address = {Sydney, Australia},
author = {Pan, Ye and Kim, Min-Gyu and Suzuki, Kenji},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Human-robot interaction,initiative exchange,prediction},
pages = {166--169},
title = {{A Robot Musician Interacting with a Human Partner through Initiative Exchange}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}166.pdf},
year = {2010}
}
@inproceedings{Nugroho2010,
abstract = {In this paper, we describe the shaping factors, which simplify and help us understand the multi-dimensional aspects of designing Wearable Expressions. These descriptive shaping factors contribute to both the design and user-experience evaluation of Wearable Expressions.},
address = {Sydney, Australia},
author = {Nugroho, Jeremiah and Beilharz, Kirsty},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Wearable expressions,body,user-centered design.},
pages = {327--330},
title = {{Understanding and Evaluating User Centred Design in Wearable Expressions}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}327.pdf},
year = {2010}
}
@inproceedings{LeGroux2010,
abstract = {Most new digital musical interfaces have evolved upon theintuitive idea that there is a causality between sonic outputand physical actions. Nevertheless, the advent of braincomputer interfaces (BCI) now allows us to directly accesssubjective mental states and express these in the physicalworld without bodily actions. In the context of an interactive and collaborative live performance, we propose to exploit novel brain-computer technologies to achieve unmediated brain control over music generation and expression.We introduce a general framework for the generation, synchronization and modulation of musical material from brainsignal and describe its use in the realization of Xmotion, amultimodal performance for a "brain quartet".},
address = {Sydney, Australia},
author = {{Le Groux}, Sylvain and Manzolli, J{\^{o}}natas and Verschure, Paul F and Sanchez, Marti and Luvizotto, Andre and Mura, Anna and V{\"{a}}ljam{\"{a}}e, Aleksander and Guger, Christoph and Prueckl, Robert and Bernardet, Ulysses},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Biosignals,Brain-computer Interface,Collaborative Musical Performance,Interactive Music Sys- tem},
pages = {309--314},
title = {{Disembodied and Collaborative Musical Interaction in the Multimodal Brain Orchestra}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}309.pdf},
year = {2010}
}
@inproceedings{Baba2010,
abstract = {"VirtualPhilharmony" (V.P.) is a conducting interface that enables users to perform expressive music with conducting action. Several previously developed conducting interfaces do not satisfy users who have conducting experience because the feedback from the conducting action does not always correspond with a natural performance. The tempo scheduler, which is the main engine of a conducting system, must be improved. V.P. solves this problem by introducing heuristics of conducting an orchestra in detecting beats, applying rules regarding the tempo expression in a bar, etc. We confirmed with users that the system realized a high "following" performance and had musical persuasiveness.},
address = {Sydney, Australia},
author = {Baba, Takashi and Hashida, Mitsuyo and Katayose, Haruhiro},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Conducting system,heuristics,sensor,template.},
pages = {263--270},
title = {{"VirtualPhilharmony" : A Conducting System with Heuristics of Conducting an Orchestra}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}263.pdf},
year = {2010}
}
@inproceedings{Roberts2010,
address = {Sydney, Australia},
author = {Roberts, Charles and Wright, Matthew and Kuchera-Morin, JoAnn and Putnam, Lance},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {AlloSphere,HCI,OSC,Vir- tual Reality,interactivity,mapping,multi-user,network,performance},
pages = {57--62},
title = {{Dynamic Interactivity Inside the AlloSphere}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}057.pdf},
year = {2010}
}
@inproceedings{Beilharz2010,
abstract = {In this paper we examine a wearable sonification and visualisation display that uses physical analogue visualisation and digital sonification to convey feedback about the wearer's activity and environment. Intended to bridge a gap between art aesthetics, fashionable technologies and informative physical computing, the user experience evaluation reveals the wearers' responses and understanding of a novel medium for wearable expression. The study reveals useful insights for wearable device design in general and future iterations of this sonification and visualisation display.},
address = {Sydney, Australia},
author = {Beilharz, Kirsty and {Vande Moere}, Andrew and Stiel, Barbara and Calo, Claudia and Tomitsch, Martin and Lombard, Adrian},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Wearable display,bimodal display,design aesthetics,multimodal expression,physical computing,sonification,visualisation},
pages = {323--326},
title = {{Expressive Wearable Sonification and Visualisation : Design and Evaluation of a Flexible Display}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}323.pdf},
year = {2010}
}
@inproceedings{Kang2010,
address = {Sydney, Australia},
author = {Kang, Laewoo and Chien, Hsin-Yi},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Interactive music interface,calligraphy,graphical music composing,sonification},
pages = {352--355},
title = {{H{\'{e}} : Calligraphy as a Musical Interface}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}352.pdf},
year = {2010}
}
@inproceedings{Mattek2010,
address = {Sydney, Australia},
author = {Mattek, Alison and Freeman, Mark and Humphrey, Eric},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Computer-Assisted Composition,Multi-touch Interfaces},
pages = {479--480},
title = {{Revisiting Cagean Composition Methodology with a Modern Computational Implementation}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}479.pdf},
year = {2010}
}
@inproceedings{Suiter2010,
abstract = {This paper introduces the concept of composing expressive music using the principles of Fuzzy Logic. The paper provides a conceptual model of a musical work which follows compositional decision making processes. Significant features of this Fuzzy Logic framework are its inclusiveness through the consideration of all the many and varied musical details, while also incorporating the imprecision that characterises musical terminology and discourse. A significant attribute of my Fuzzy Logic method is that it traces the trajectory of all musical details, since it is both the individual elements and their combination over time which is significant to the effectiveness of a musical work in achieving its goals. The goal of this work is to find a set of elements and rules, which will ultimately enable the construction of a genralised algorithmic compositional system which can produce expressive music if so desired.},
address = {Sydney, Australia},
author = {Suiter, Wendy},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {fuzzy logic,music composition,musical expression,nime10},
pages = {319--322},
title = {{Toward Algorithmic Composition of Expression in Music Using Fuzzy Logic}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}319.pdf},
year = {2010}
}
@inproceedings{Cannon2010,
address = {Sydney, Australia},
author = {Cannon, Joanne and Favilla, Stuart},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {augmented instruments,expressive spatial,nime10,playable instruments},
pages = {120--124},
title = {{Expression and Spatial Motion : Playable Ambisonics}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}120.pdf},
year = {2010}
}
@inproceedings{Humphrey2010,
abstract = {This paper articulates an interest in a kind of interactive musical instrument and artwork that defines the mechanisms for instrumental interactivity from the iconic morphologies of "ready-mades", casting historical utilitarian objects as the basis for performed musical experiences by spectators. The interactive repertoires are therefore partially pre-determined through enculturated behaviors that are associated with particular objects, but more importantly, inextricably linked to the thematic and meaningful assemblage of the work itself. Our new work epi-thet gathers data from individual interactions with common microscopes placed on platforms within a large space. This data is correlated with public domain genetic datasets obtained from micro-array analysis. A sonification algorithm generates unique compositions associated with the spectator "as measured" through their individual specification in performing an iconic measurement action. The apparatus is a receptacle for unique compositions in sound, and invites a participatory choreography of stillness that is available for reception as a live musical performance.},
address = {Sydney, Australia},
author = {Humphrey, Tim and Flynn, Madeleine and Stevens, Jesse},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Sonification installation spectator-choreography m},
pages = {69--71},
title = {{Epi-thet : A Musical Performance Installation and a Choreography of Stillness}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}069.pdf},
year = {2010}
}
@inproceedings{Ferguson2010,
abstract = {In this paper, we describe a comparison between parameters drawn from 3-dimensional measurement of a dance performance, and continuous emotional response data recorded from an audience present during this performance. A continuous time series representing the mean movement as the dance unfolds is extracted from the 3-dimensional data. The audiences' continuous emotional response data are also represented as a time series, and the series are compared. We concluded that movement in the dance performance directly influences the emotional arousal response of the audience.},
address = {Sydney, Australia},
author = {Ferguson, Sam and Schubert, Emery and Stevens, Catherine},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Continuous Response.,Dance,Emotion,Motion Capture},
pages = {481--484},
title = {{Movement in a Contemporary Dance Work and its Relation to Continuous Emotional Response}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}481.pdf},
year = {2010}
}
@inproceedings{Oh2010,
abstract = {In this paper, we describe the development of the Stanford Mobile Phone Orchestra (MoPhO) since its inceptionin 2007. As a newly structured ensemble of musicians withiPhones and wearable speakers, MoPhO takes advantageof the ubiquity and mobility of smartphones as well asthe unique interaction techniques offered by such devices.MoPhO offers a new platform for research, instrument design, composition, and performance that can be juxtaposedto that of a laptop orchestra. We trace the origins of MoPhO,describe the motivations behind the current hardware andsoftware design in relation to the backdrop of current trendsin mobile music making, detail key interaction conceptsaround new repertoire, and conclude with an analysis onthe development of MoPhO thus far.},
address = {Sydney, Australia},
author = {Oh, Jieun and Herrera, Jorge and Bryan, Nicholas J and Dahl, Luke and Wang, Ge},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {iPhone,live performance,mobile music,mobile phone orchestra},
pages = {82--87},
title = {{Evolving The Mobile Phone Orchestra}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}082.pdf},
year = {2010}
}
@inproceedings{Martin2010a,
abstract = {In 2009 the cross artform group, Last Man to Die, presenteda series of performances using new interfaces and networkedperformance to integrate the three artforms of its members(actor, Hanna Cormick, visual artist, Benjamin Forster andpercussionist, Charles Martin). This paper explains ourartistic motivations and design for a computer vision surfaceand networked heartbeat sensor as well as the experience ofmounting our first major work, Vital LMTD.},
address = {Sydney, Australia},
author = {Martin, Charles and Forster, Benjamin and Cormick, Hanna},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {cross-artform performance,networked performance,physi- cal computing},
pages = {204--207},
title = {{Cross-Artform Performance Using Networked Interfaces : Last Man to Die's Vital LMTD}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}204.pdf},
year = {2010}
}
@inproceedings{Martin2010,
abstract = {Many musical instruments have interfaces which emphasisethe pitch of the sound produced over other perceptual characteristics, such as its timbre. This is at odds with the musical developments of the last century. In this paper, weintroduce a method for replacing the interface of musicalinstruments (both conventional and unconventional) witha more flexible interface which can present the intrument'savailable sounds according to variety of different perceptualcharacteristics, such as their brightness or roughness. Weapply this method to an instrument of our own design whichcomprises an electro-mechanically controlled electric guitarand amplifier configured to produce feedback tones.},
address = {Sydney, Australia},
author = {Martin, Aengus and Ferguson, Sam and Beilharz, Kirsty},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Concatenative Synthesis,Feedback,Guitar},
pages = {364--367},
title = {{Mechanisms for Controlling Complex Sound Sources : Applications to Guitar Feedback Control}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}364.pdf},
year = {2010}
}
@inproceedings{Kocaballi2010,
abstract = {Human agency, our capacity for action, has been at the hub of discussions centring upon philosophical enquiry for a long period of time. Sensory supplementation devices can provide us with unique opportunities to investigate the different aspects of our agency by enabling new modes of perception and facilitating the emergence of novel interactions, all of which is impossible without the aforesaid devices. Our preliminary study investigates the non-verbal strategies employed for negotiation of our capacity for action with other bodies and the surrounding space through body-to-body and body-to-space couplings enabled by sensory supplementation devices. We employed a lowfi rapid prototyping approach to build this device, enabling distal perception by sonic and haptic feedback. Further, we conducted a workshop in which participants equipped with this device engaged in game-like activities.},
address = {Sydney, Australia},
author = {Kocaballi, A Baki and Gemeinboeck, Petra and Saunders, Rob},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Human agency,distal perception,enactive interfaces,sensory supplementation,sonic feedback,tactile feedback},
pages = {47--50},
title = {{Investigating the Potential for Shared Agency using Enactive Interfaces}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}047.pdf},
year = {2010}
}
@inproceedings{Heinz2010,
abstract = {This paper proposes a design concept for a tangible interface forcollaborative performances that incorporates two social factorspresent during performance, the individual creation andadaptation of technology and the sharing of it within acommunity. These factors are identified using the example of alaptop ensemble and then applied to three existing collaborativeperformance paradigms. Finally relevant technology, challengesand the current state of our implementation are discussed.},
address = {Sydney, Australia},
author = {Heinz, Sebastian and O'Modhrain, Sile},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Tangible User Interfaces,collaborative performances,social factors},
pages = {339--342},
title = {{Designing a Shareable Musical TUI}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}339.pdf},
year = {2010}
}
@inproceedings{Headlee2010,
abstract = {In this paper, we present an interactive system that uses the body as a generative tool for creating music. We explore innovative ways to make music, create self-awareness, and provide the opportunity for unique, interactive social experiences. The system uses a multi-player game paradigm, where players work together to add layers to a soundscape of three distinct environments. Various sensors and hardware are attached to the body and transmit signals to a workstation, where they are processed using Max/MSP. The game is divided into three levels, each of a different soundscape. The underlying purpose of our system is to move the player's focus away from complexities of the modern urban world toward a more internalized meditative state. The system is currently viewed as an interactive installation piece, but future iterations have potential applications in music therapy, bio games, extended performance art, and as a prototype for new interfaces for musical expression.},
address = {Sydney, Australia},
author = {Headlee, Kimberlee and Koziupa, Tatyana and Siwiak, Diana},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {biomusic,collaborative,expressive,hci,interactive,interactivity design,interface for musical expression,multimodal,musical mapping strategies,nime10,performance,sonification},
pages = {423--426},
title = {{Sonic Virtual Reality Game : How Does Your Body Sound ?}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}423.pdf},
year = {2010}
}
@inproceedings{Miller2010,
abstract = {The console gaming industry is experiencing a revolution in terms of user control, and a large part to Nintendo's introduction of the Wii remote. The online open source development community has embraced the Wii remote, integrating the inexpensive technology into numerous applications. Some of the more interesting applications demonstrate how the remote hardware can be leveraged for nonstandard uses. In this paper we describe a new way of interacting with the Wii remote and sensor bar to produce music. The Wiiolin is a virtual instrument which can mimic a violin or cello. Sensor bar motion relative to the Wii remote and button presses are analyzed in real-time to generate notes. Our design is novel in that it involves the remote's infrared camera and sensor bar as an integral part of music production, allowing users to change notes by simply altering the angle of their wrist, and henceforth, bow. The Wiiolin introduces a more realistic way of instrument interaction than other attempts that rely on button presses and accelerometer data alone.},
address = {Sydney, Australia},
author = {Miller, Jace and Hammond, Tracy},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Wii remote,cello,gesture recognition.,human computer interaction,motion recognition,violin,virtual instrument},
pages = {497--500},
title = {{Wiiolin : a Virtual Instrument Using the Wii Remote}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}497.pdf},
year = {2010}
}
@inproceedings{Yamaguchi2010,
abstract = {In this paper, we introduce a wireless musical interface driven by grasping forces and human motion. The sounds generated by the traditional digital musical instruments are dependent on the physical shape of the musical instruments. The freedom of the musical performance is restricted by its structure. Therefore, the sounds cannot be generated with the body expression like the dance. We developed a ball-shaped interface, TwinkleBall, to achieve the free-style performance. A photo sensor is embedded in the translucent rubber ball to detect the grasping force of the performer. The grasping force is translated into the luminance intensity for processing. Moreover, an accelerometer is also embedded in the interface for motion sensing. By using these sensors, a performer can control the note and volume by varying grasping force and motion respectively. The features of the proposed interface are ball-shaped, wireless, and handheld size. As a result, the proposed interface is able to generate the sound from the body expression such as dance.},
address = {Sydney, Australia},
author = {Yamaguchi, Tomoyuki and Kobayashi, Tsukasa and Ariga, Anna and Hashimoto, Shuji},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Dance Performance.,Embodied Sound Media,Musical Interface},
pages = {116--119},
title = {{TwinkleBall : A Wireless Musical Interface for Embodied Sound Media}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}116.pdf},
year = {2010}
}
@inproceedings{Reboursiere2010,
abstract = {This project aims at studying how recent interactive and interactions technologies would help extend how we play theguitar, thus defining the "multimodal guitar". Our contributions target three main axes: audio analysis, gestural control and audio synthesis. For this purpose, we designed anddeveloped a freely-available toolbox for augmented guitarperformances, compliant with the PureData and Max/MSPenvironments, gathering tools for: polyphonic pitch estimation, fretboard visualization and grouping, pressure sensing,modal synthesis, infinite sustain, rearranging looping and"smart" harmonizing.},
address = {Sydney, Australia},
author = {Reboursi{\`{e}}re, Lo{\"{i}}c and Frisson, Christian and L{\"{a}}hdeoja, Otso and Mills, John A and Picard-Limpens, C{\'{e}}cile and Todoroff, Todor},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Augmented guitar,audio synthesis,digital audio effects,gestural sensing,hexaphonic guitar,multimodal interaction,polyphonic tran- scription},
pages = {415--418},
title = {{Multimodal Guitar : A Toolbox For Augmented Guitar Performances}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}415.pdf},
year = {2010}
}
@inproceedings{Savage2010,
abstract = {This paper presents Mmmmm; a Multimodal Mobile MusicMixer that provides DJs a new interface for mixing musicon the Nokia N900 phones. Mmmmm presents a novel wayfor DJ to become more interactive with their audience andvise versa. The software developed for the N900 mobilephone utilizes the phones built-in accelerometer sensor andBluetooth audio streaming capabilities to mix and apply effects to music using hand gestures and have the mixed audiostream to Bluetooth speakers, which allows the DJ to moveabout the environment and get familiarized with their audience, turning the experience of DJing into an interactiveand audience engaging process.Mmmmm is designed so that the DJ can utilize handgestures and haptic feedback to help them perform the various tasks involved in DJing (mixing, applying effects, andetc). This allows the DJ to focus on the crowd, thus providing the DJ a better intuition of what kind of music ormusical mixing style the audience is more likely to enjoyand engage with. Additionally, Mmmmm has an "Ambient Tempo Detection mode in which the phones camera isutilized to detect the amount of movement in the environment and suggest to the DJ the tempo of music that shouldbe played. This mode utilizes frame differencing and pixelchange overtime to get a sense of how fast the environmentis changing, loosely correlating to how fast the audience isdancing or the lights are flashing in the scene. By determining the ambient tempo of the environment the DJ canget a better sense for the type of music that would fit bestfor their venue.Mmmmm helps novice DJs achieve a better music repertoire by allowing them to interact with their audience andreceive direct feedback on their performance. The DJ canchoose to utilize these modes of interaction and performance or utilize traditional DJ controls using MmmmmsN900 touch screen based graphics user interface.},
address = {Sydney, Australia},
author = {Savage, Norma Saiph and Ali, Syed Reza and Chavez, Norma E},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {DJ,Multi-modal,Nokia,accelerome- ter,audience,interaction,interactive,mixer,mobile,music,n900,phone,smart phones,touch screen},
pages = {395--398},
title = {{Mmmmm : A Multi-modal Mobile Music Mixer}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}395.pdf},
year = {2010}
}
@inproceedings{Liebman2010,
address = {Sydney, Australia},
author = {Liebman, Noah and Nagara, Michael and Spiewla, Jacek and Zolkosky, Erin},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {audio,control surfaces,mixing board,multitouch,nime10,screen,sound,theatre,touch-,user-centered design},
pages = {51--56},
title = {{Cuebert : A New Mixing Board Concept for Musical Theatre}},
url = {http://www.nime.org/proceedings/2010/nime2010{\_}051.pdf},
year = {2010}
}
@inproceedings{Berdahl2011a,
abstract = {This paper describes a new Beagle Board-based platform forteaching and practicing interaction design for musical applications. The migration from desktop and laptop computerbased sound synthesis to a compact and integrated control, computation and sound generation platform has enormous potential to widen the range of computer music instruments and installations that can be designed, and improvesthe portability, autonomy, extensibility and longevity of designed systems. We describe the technical features of theSatellite CCRMA platform and contrast it with personalcomputer-based systems used in the past as well as emergingsmart phone-based platforms. The advantages and tradeoffs of the new platform are considered, and some projectwork is described.},
address = {Oslo, Norway},
author = {Berdahl, Edgar and Ju, Wendy},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {arduino,beagle board,instruments omap,linux,microcontrollers,music controllers,nime,pd,pedagogy,texas},
pages = {173--178},
title = {{Satellite CCRMA: A Musical Interaction and Sound Synthesis Platform}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}173.pdf},
year = {2011}
}
@inproceedings{Schacher2011,
abstract = {In this paper the relationship between body, motion and sound is addressed. The comparison with traditional instruments and dance is shown with regards to basic types of motion. The difference between gesture and movement is outlined and some of the models used in dance for structuring motion sequences are described. In order to identify expressive aspects of motion sequences a test scenario is devised. After the description of the methods and tools used in a series of measurements, two types of data-display are shown and the applied in the interpretation. One salient feature is recognized and put into perspective with regards to movement and gestalt perception. Finally the merits of the technical means that were applied are compared and a model-based approach to motion-sound mapping is proposed.},
address = {Oslo, Norway},
author = {Schacher, Jan C and Stoecklin, Angela},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Interactive Dance,Mapping,Motion Perception,Motion and Gesture,Sonification},
pages = {292--295},
title = {{Traces -- Body, Motion and Sound}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}292.pdf},
year = {2011}
}
@inproceedings{Hayes2011,
address = {Oslo, Norway},
author = {Hayes, Lauren},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Vibrotactile feedback,augmented instruments.,digital composition,human-computer interfaces,real-time performance},
pages = {72--75},
title = {{Vibrotactile Feedback-Assisted Performance}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}072.pdf},
year = {2011}
}
@inproceedings{Ustarroz2011,
abstract = {TresnaNet explores the potential of Telematics as a generator ofmusical expressions. I pretend to sound the silent flow ofinformation from the network.This is realized through the fabrication of a prototypefollowing the intention of giving substance to the intangibleparameters of our communication. The result may haveeducational, commercial and artistic applications because it is aphysical and perceptible representation of the transfer ofinformation over the network. This paper describes the design,implementation and conclusions about TresnaNet.},
address = {Oslo, Norway},
author = {Ustarroz, Paula},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Interface,musical generation,musical instrument,network,network sniffer.,telematics},
pages = {425--428},
title = {{TresnaNet Musical Generation based on Network Protocols}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}425.pdf},
year = {2011}
}
@inproceedings{Gillian2011,
abstract = {This paper presents a novel algorithm that has been specifically designed for the recognition of multivariate temporal musical gestures. The algorithm is based on DynamicTime Warping and has been extended to classify any N dimensional signal, automatically compute a classificationthreshold to reject any data that is not a valid gesture andbe quickly trained with a low number of training examples.The algorithm is evaluated using a database of 10 temporalgestures performed by 10 participants achieving an averagecross-validation result of 99{\%}.},
address = {Oslo, Norway},
author = {Gillian, Nicholas and Knapp, R Benjamin and O'Modhrain, Sile},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Dynamic Time Warping,Gesture Recognition,Multivariate Temporal Gestures,Musician-Computer Interaction},
pages = {337--342},
title = {{Recognition Of Multivariate Temporal Musical Gestures Using N-Dimensional Dynamic Time Warping}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}337.pdf},
year = {2011}
}
@inproceedings{Fyfe2011,
abstract = {JunctionBox is a new software toolkit for creating multitouch interfaces for controlling sound and music. Morespecifically, the toolkit has special features which make iteasy to create TUIO-based touch interfaces for controllingsound engines via Open Sound Control. Programmers using the toolkit have a great deal of freedom to create highlycustomized interfaces that work on a variety of hardware.},
address = {Oslo, Norway},
author = {Fyfe, Lawrence and Tindale, Adam R and Carpendale, Sheelagh},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Multi-touch,Open Sound Control,TUIO,Toolkit},
pages = {276--279},
title = {{JunctionBox : A Toolkit for Creating Multi-touch Sound Control Interfaces}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}276.pdf},
year = {2011}
}
@inproceedings{Martin2011,
address = {Oslo, Norway},
author = {Martin, Charles and Lai, Chi-Hsia},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {computer vision,media performance,percussion},
pages = {142--143},
title = {{Strike on Stage : a Percussion and Media Performance}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}142.pdf},
year = {2011}
}
@inproceedings{Franinovic2011,
address = {Oslo, Norway},
author = {Franinovic, Karmen},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {exploration,gesture,habit,sonic interaction design},
pages = {448--452},
title = {{The Flo)(ps : Negotiating Between Habitual and Explorative Gestures}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}448.pdf},
year = {2011}
}
@inproceedings{Wang2011,
abstract = {This paper describes the origin, design, and implementation of Smule's Magic Fiddle, an expressive musical instrument for the iPad. Magic Fiddle takes advantage of the physical aspects of the device to integrate game-like and pedagogical elements. We describe the origin of Magic Fiddle, chronicle its design process, discuss its integrated music education system, and evaluate the overall experience.},
address = {Oslo, Norway},
author = {Wang, Ge and Oh, Jieun and Lieber, Tom},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Magic Fiddle,experiential design,iPad,music education.,physical interaction design},
pages = {197--202},
title = {{Designing for the iPad : Magic Fiddle}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}197.pdf},
year = {2011}
}
@inproceedings{Crevoisier2011,
abstract = {The Surface Editor is a software tool for creating control interfaces and mapping input actions to OSC or MIDI actions very easily and intuitively. Originally conceived to be used with a tactile interface, the Surface Editor has been extended to support the creation of graspable interfaces as well. This paper presents a new framework for the generic mapping of user actions with graspable objects on a surface. We also present a system for detecting touch on thin objects, allowing for extended interactive possibilities. The Surface Editor is not limited to a particular tracking system though, and the generic mapping approach for objects can have a broader use with various input interfaces supporting touch and/or objects.},
address = {Oslo, Norway},
author = {Crevoisier, Alain and Picard-Limpens, C{\'{e}}cile},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {NIME,graspable interfaces.,interaction,mapping,tangibles,user-defined interfaces},
pages = {236--239},
title = {{Mapping Objects with the Surface Editor}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}236.pdf},
year = {2011}
}
@inproceedings{Schedel2011,
abstract = {In this paper we discuss how the band 000000Swan uses machine learning to parse complex sensor data and create intricate artistic systems for live performance. Using the Wekinator software for interactive machine learning, we have created discrete and continuous models for controlling audio and visual environments using human gestures sensed by a commercially-available sensor bow and the Microsoft Kinect. In particular, we have employed machine learning to quickly and easily prototype complex relationships between performer gesture and performative outcome.},
address = {Oslo, Norway},
author = {Schedel, Margaret and Perry, Phoenix and Fiebrink, Rebecca},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Animation,Bow Articulation,Interactive,K-Bow,Kinect,Machine Learning,Motion-Tracking,Multimedia,Wekinator},
pages = {453--456},
title = {{Wekinating 000000{\{}S{\}}wan : Using Machine Learning to Create and Control Complex Artistic Systems}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}453.pdf},
year = {2011}
}
@inproceedings{Jessop2011,
abstract = {In composer Tod Machover's new opera Death and the Powers, the main character uploads his consciousness into anelaborate computer system to preserve his essence and agencyafter his corporeal death. Consequently, for much of theopera, the stage and the environment itself come alive asthe main character. This creative need brings with it a hostof technical challenges and opportunities. In order to satisfythe needs of this storyline, Machover's Opera of the Futuregroup at the MIT Media Lab has developed a suite of newperformance technologies, including robot characters, interactive performance capture systems, mapping systems for, , authoring interactive multimedia performances, new musical instruments, unique spatialized sound controls, anda unified control system for all these technological components. While developed for a particular theatrical production, many of the concepts and design procedures remain relevant to broader contexts including performance,robotics, and interaction design.},
address = {Oslo, Norway},
author = {Jessop, Elena and Torpey, Peter A and Bloomberg, Benjamin},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Death and the Powers,Disembodied Performance,Tod Machover,ambisonics,gestural interfaces,opera},
pages = {349--354},
title = {{Music and Technology in Death and the Powers}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}349.pdf},
year = {2011}
}
@inproceedings{Mitchell2011,
abstract = {This paper documents the first developmental phase of aninterface that enables the performance of live music usinggestures and body movements. The work included focuseson the first step of this project: the composition and performance of live music using hand gestures captured using asingle data glove. The paper provides a background to thefield, the aim of the project and a technical description ofthe work completed so far. This includes the developmentof a robust posture vocabulary, an artificial neural networkbased posture identification process and a state-based system to map identified postures onto a set of performanceprocesses. The paper is closed with qualitative usage observations and a projection of future plans.},
address = {Oslo, Norway},
author = {Mitchell, Tom and Heap, Imogen},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Data Glove,Gestural Music,Imogen Heap,Live Music Composition,Looping,Music Controller,Neural Network},
pages = {465--468},
title = {{SoundGrasp : A Gestural Interface for the Performance of Live Music}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}465.pdf},
year = {2011}
}
@inproceedings{Luhtala2011,
address = {Oslo, Norway},
author = {Luhtala, Matti and Kym{\"{a}}l{\"{a}}inen, Tiina and Plomp, Johan},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Human-Technology Interaction (HTI),Music interfaces,User-Centred Design (UCD),design for all (DfA),design tools,modifiable interfaces,music therapy,performance.,prototyping},
pages = {429--432},
title = {{Designing a Music Performance Space for Persons with Intellectual Learning Disabilities}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}429.pdf},
year = {2011}
}
@inproceedings{Gillian2011a,
abstract = {This paper presents the SARC EyesWeb Catalog, (SEC),a machine learning toolbox that has been specifically developed for musician-computer interaction. The SEC features a large number of machine learning algorithms that can be used in real-time to recognise static postures, perform regression and classify multivariate temporal gestures. The algorithms within the toolbox have been designed to work with any N -dimensional signal and can be quickly trained with a small number of training examples. We also provide the motivation for the algorithms used for the recognition of musical gestures to achieve a low intra-personal generalisation error, as opposed to the inter-personal generalisation error that is more common in other areas of human-computer interaction.},
address = {Oslo, Norway},
author = {Gillian, Nicholas and Knapp, R Benjamin and O'Modhrain, Sile},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Machine learning,SEC,gesture recognition,musician-computer interaction},
pages = {343--348},
title = {{A Machine Learning Toolbox For Musician Computer Interaction}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}343.pdf},
year = {2011}
}
@inproceedings{Cappelen2011,
abstract = {The traditional role of the musical instrument is to be the working tool of the professional musician. On the instrument the musician performs music for the audience to listen to. In this paper we present an interactive installation, where we expand the role of the instrument to motivate musicking and cocreation between diverse users. We have made an open installation, where users can perform a variety of actions in several situations. By using the abilities of the computer, we have made an installation, which can be interpreted to have many roles. It can both be an instrument, a co-musician, a communication partner, a toy, a meeting place and an ambient musical landscape. The users can dynamically shift between roles, based on their abilities, knowledge and motivation.},
address = {Oslo, Norway},
author = {Cappelen, Birgitta and Andersson, Anders-Petter},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {design,genre,interaction,interactive installation,music instrument,musicking,narrative,open,role,sound art},
pages = {511--514},
title = {{Expanding the Role of the Instrument}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}511.pdf},
year = {2011}
}
@inproceedings{Hansen2011,
abstract = {In this paper a collaborative music game for two pen tablets is studied in order to see how two people with no professional music background negotiated musical improvisation. In an initial study of what it is that constitutes play fluency in improvisation, a music game has been designed and evaluated through video analysis: A qualitative view of mutual action describes the social context of music improvisation: how two people with speech, laughter, gestures, postures and pauses negotiate individual and joint action. The objective behind the design of the game application was to support players in some aspects of their mutual play. Results show that even though players activated additional sound feedback as a result of their mutual play, players also engaged in forms of mutual play that the game engine did not account for. These ways of mutual play are descibed further along with some suggestions for how to direct future designs of collaborative music improvisation games towards ways of mutual play.},
address = {Oslo, Norway},
author = {Hansen, Anne-Marie Skriver and Andersen, Hans J{\o}rgen and Raudaskoski, Pirkko},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Collaborative interfaces,improvisation,interactive music games,novice.,play,social interaction},
pages = {220--223},
title = {{Play Fluency in Music Improvisation Games for Novices}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}220.pdf},
year = {2011}
}
@inproceedings{Janssen2011,
abstract = {The present article describes a reverberation instrumentwhich is based on cognitive categorization of reverberating spaces. Different techniques for artificial reverberationwill be covered. A multidimensional scaling experimentwas conducted on impulse responses in order to determinehow humans acoustically perceive spatiality. This researchseems to indicate that the perceptual dimensions are related to early energy decay and timbral qualities. Theseresults are applied to a reverberation instrument based ondelay lines. It can be contended that such an instrumentcan be controlled more intuitively than other delay line reverberation tools which often provide a confusing range ofparameters which have a physical rather than perceptualmeaning.},
address = {Oslo, Norway},
author = {Janssen, Berit},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Reverberation,mapping,multidimensional scaling,perception},
pages = {68--71},
title = {{A Reverberation Instrument Based on Perceptual Mapping}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}068.pdf},
year = {2011}
}
@inproceedings{Garcia2011a,
abstract = {We conducted three studies with contemporary music composers at IRCAM. We found that even highly computer-literate composers use an iterative process that begins with expressing musical ideas on paper, followed by active parallel exploration on paper and in software, prior to final execution of their ideas as an original score. We conducted a participatory design study that focused on the creative exploration phase, to design tools that help composers better integrate their paper-based and electronic activities. We then developed InkSplorer as a technology probe that connects users' hand-written gestures on paper to Max/MSP and OpenMusic. Composers appropriated InkSplorer according to their preferred composition styles, emphasizing its ability to help them quickly explore musical ideas on paper as they interact with the computer. We conclude with recommendations for designing interactive paper tools that support the creative process, letting users explore musical ideas both on paper and electronically.},
address = {Oslo, Norway},
author = {Garcia, J{\'{e}}r{\'{e}}mie and Tsandilas, Theophanis and Agon, Carlos and Mackay, Wendy E},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Composer,Creativity,Design Exploration,InkSplorer,Interactive Paper,OpenMusic,Technology Probes.},
pages = {361--366},
title = {{InkSplorer : Exploring Musical Ideas on Paper and Computer}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}361.pdf},
year = {2011}
}
@inproceedings{Friberg2011,
abstract = {This is an overview of the three installations Hoppsa Universum, CLOSE and Flying Carpet. They were all designed as choreographed sound and music installations controlled by the visitors movements. The perspective is from an artistic goal/vision intention in combination with the technical challenges and possibilities. All three installations were realized with video cameras in the ceiling registering the users' position or movement. The video analysis was then controlling different types of interactive software audio players. Different aspects like narrativity, user control, and technical limitations are discussed.},
address = {Oslo, Norway},
author = {Friberg, Anders and K{\"{a}}llblad, Anna},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Gestures,choreography,dance,interactive music.,music installation},
pages = {128--131},
title = {{Experiences from Video-Controlled Sound Installations}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}128.pdf},
year = {2011}
}
@inproceedings{Angel2011,
abstract = {This paper deals with the usage of bio-data from performers to create interactive multimedia performances or installations. It presents this type of research in some art works produced in the last fifty years (such as Lucier's Music for a Solo Performance, from 1965), including two interactive performances of my , , authorship, which use two different types of bio-interfaces: on the one hand, an EMG (Electromyography) and on the other hand, an EEG (electroencephalography). The paper explores the interaction between the human body and real-time media (audio and visual) by the usage of bio-interfaces. This research is based on biofeedback investigations pursued by the psychologist Neal E. Miller in the 1960s, mainly based on finding new methods to reduce stress. However, this article explains and shows examples in which biofeedback research is used for artistic purposes only.},
address = {Oslo, Norway},
author = {Angel, Claudia R},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Butoh,Live electronics,biofeedback,interactive sound and video.,performance},
pages = {421--424},
title = {{Creating Interactive Multimedia Works with Bio-data}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}421.pdf},
year = {2011}
}
@inproceedings{Gallin2011,
address = {Oslo, Norway},
author = {Gallin, Emmanuelle and Sirguy, Marc},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {A/D Converter,CV,Computer Music,Controller,DMX,Interface.,MIDI,OSC,Sensor,USB},
pages = {437--440},
title = {{Eobody3: a Ready-to-use Pre-mapped {\&} Multi-protocol Sensor Interface}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}437.pdf},
year = {2011}
}
@inproceedings{Kim2011,
abstract = {In this paper, we discuss the use of the clothesline as ametaphor for designing a musical interface called Airer Choir. This interactive installation is based on the function ofan ordinary object that is not a traditional instrument, andhanging articles of clothing is literally the gesture to use theinterface. Based on this metaphor, a musical interface withhigh transparency was designed. Using the metaphor, weexplored the possibilities for recognizing of input gesturesand creating sonic events by mapping data to sound. Thus,four different types of Airer Choir were developed. By classifying the interfaces, we concluded that various musicalexpressions are possible by using the same metaphor.},
address = {Oslo, Norway},
author = {Kim, Seunghun and Kim, Luke K and Jeong, Songhee and Yeo, Woon Seung},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {clothesline installation,metaphor,musical interface},
pages = {60--63},
title = {{Clothesline as a Metaphor for a Musical Interface}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}060.pdf},
year = {2011}
}
@inproceedings{Donald2011,
abstract = {This paper outlines the formation of the Expanded Performance (EP) trio, a chamber ensemble comprised of electriccello with sensor bow, augmented digital percussion, anddigital turntable with mixer. Decisions relating to physical set-ups and control capabilities, sonic identities, andmappings of each instrument, as well as their roles withinthe ensemble, are explored. The contributions of these factors to the design of a coherent, expressive ensemble andits emerging performance practice are considered. The trioproposes solutions to creation, rehearsal and performanceissues in ensemble live electronics.},
address = {Oslo, Norway},
author = {Donald, Erika and Duinker, Ben and Britton, Eliot},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Live electronics,chamber music,digital performance,ensemble,instrument identity,mapping},
pages = {491--494},
title = {{Designing the EP Trio: Instrument Identities, Control and Performance Practice in an Electronic Chamber Music Ensemble}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}491.pdf},
year = {2011}
}
@inproceedings{Erkut2011,
abstract = {We present a generic, structured model for design and evaluation of musical interfaces. This model is developmentoriented, and it is based on the fundamental function of themusical interfaces, i.e., to coordinate the human action andperception for musical expression, subject to human capabilities and skills. To illustrate the particulars of this modeland present it in operation, we consider the previous designand evaluation phase of iPalmas, our testbed for exploringrhythmic interaction. Our findings inform the current design phase of iPalmas visual and auditory displays, wherewe build on what has resonated with the test users, and explore further possibilities based on the evaluation results.},
address = {Oslo, Norway},
author = {Erkut, Cumhur and Jylh{\"{a}}, Antti and Discioglu, Reha},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {multimodal displays,rhythmic interaction,sonification,uml},
pages = {477--480},
title = {{A Structured Design and Evaluation Model with Application to Rhythmic Interaction Displays}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}477.pdf},
year = {2011}
}
@inproceedings{Pardue2011,
abstract = {This paper describes the motivation and construction ofGamelan Elektrika, a new electronic gamelan modeled aftera Balinese Gong Kebyar. The first of its kind, Elektrika consists of seven instruments acting as MIDI controllers accompanied by traditional percussion and played by 11 or moreperformers following Balinese performance practice. Threemain percussive instrument designs were executed using acombination of force sensitive resistors, piezos, and capacitive sensing. While the instrument interfaces are designedto play interchangeably with the original, the sound andtravel possiblilities they enable are tremendous. MIDI enables a massive new sound palette with new scales beyondthe quirky traditional tuning and non-traditional sounds.It also allows simplified transcription for an aurally taughttradition. Significantly, it reduces the transportation challenges of a previously large and heavy ensemble, creatingopportunities for wider audiences to experience Gong Kebyar's enchanting sound. True to the spirit of oneness inBalinese music, as one of the first large all-MIDI ensembles,Elek Trika challenges performers to trust silent instrumentsand develop an understanding of highly intricate and interlocking music not through the sound of the individual, butthrough the sound of the whole.},
address = {Oslo, Norway},
author = {Pardue, Laurel S and Boch, Andrew and Boch, Matt and Southworth, Christine and Rigopulos, Alex},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {MIDI ensemble,bali,gamelan,musical instrument design},
pages = {18--23},
title = {{Gamelan Elektrika: An Electronic Balinese Gamelan}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}018.pdf},
year = {2011}
}
@inproceedings{dAlessandro2011,
address = {Oslo, Norway},
author = {D'Alessandro, Nicolas and Calderon, Roberto and M{\"{u}}ller, Stefanie},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {agent,architecture,collaboration,figure 1,installation,instrument,interactive fabric,light,mo-,movements in the installation,space and,tion,voice synthesis},
pages = {132--135},
title = {{ROOM {\#}81---Agent-Based Instrument for Experiencing Architectural and Vocal Cues}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}132.pdf},
year = {2011}
}
@inproceedings{Murray-Browne2011,
abstract = {Many performers of novel musical instruments find it difficult to engage audiences beyond those in the field. Previousresearch points to a failure to balance complexity with usability, and a loss of transparency due to the detachmentof the controller and sound generator. The issue is oftenexacerbated by an audience's lack of prior exposure to theinstrument and its workings.However, we argue that there is a conflict underlyingmany novel musical instruments in that they are intendedto be both a tool for creative expression and a creative workof art in themselves, resulting in incompatible requirements.By considering the instrument, the composition and theperformance together as a whole with careful considerationof the rate of learning demanded of the audience, we propose that a lack of transparency can become an asset ratherthan a hindrance. Our approach calls for not only controllerand sound generator to be designed in sympathy with eachother, but composition, performance and physical form too.Identifying three design principles, we illustrate this approach with the Serendiptichord, a wearable instrument fordancers created by the , , authors.},
address = {Oslo, Norway},
author = {Murray-Browne, Tim and Mainstone, Di and Bryan-Kinns, Nick and Plumbley, Mark D},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Performance,composed instrument,constraint.,transparency},
pages = {56--59},
title = {{The Medium is the Message: Composing Instruments and Performing Mappings}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}056.pdf},
year = {2011}
}
@inproceedings{Milne2011,
address = {Oslo, Norway},
author = {Milne, Andrew J and Xamb{\'{o}}, Anna and Laney, Robin and Sharp, David B and Prechtl, Anthony and Holland, Simon},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {generalized keyboard,iPad,isomorphic layout,microtonality,multi-touch surface,musical interface design,tablet},
pages = {244--247},
title = {{Hex Player --- A Virtual Musical Controller}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}244.pdf},
year = {2011}
}
@inproceedings{Dimitrov2011,
abstract = {A contemporary PC user, typically expects a sound cardto be a piece of hardware, that: can be manipulated by'audio' software (most typically exemplified by 'media players'); and allows interfacing of the PC to audio reproduction and/or recording equipment. As such, a 'sound card'can be considered to be a system, that encompasses designdecisions on both hardware and software levels -- that also demand a certain understanding of the architecture of thetarget PC operating system.This project outlines how an Arduino Duemillanoveboard (containing a USB interface chip, manufactured byFuture Technology Devices International Ltd [FTDI]company) can be demonstrated to behave as a full-duplex,mono, 8-bit 44.1 kHz soundcard, through an implementation of: a PC audio driver for ALSA (Advanced LinuxSound Architecture); a matching program for theArduino'sATmega microcontroller -- and nothing more than headphones (and a couple of capacitors). The main contributionof this paper is to bring a holistic aspect to the discussionon the topic of implementation of soundcards -- also by referring to open-source driver, microcontroller code and testmethods; and outline a complete implementation of an open -- yet functional -- soundcard system.},
address = {Oslo, Norway},
author = {Dimitrov, Smilen and Serafin, Stefania},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {alsa,arduino,audio,driver,linux,sound card},
pages = {211--216},
title = {{Audio Arduino -- an ALSA (Advanced Linux Sound Architecture) Audio Driver for FTDI-based Arduinos}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}211.pdf},
year = {2011}
}
@inproceedings{Leslie2011,
abstract = {MoodMixer is an interactive installation in which participants collaboratively navigate a two-dimensional music spaceby manipulating their cognitive state and conveying thisstate via wearable Electroencephalography (EEG) technology. The participants can choose to actively manipulateor passively convey their cognitive state depending on theirdesired approach and experience level. A four-channel electronic music mixture continuously conveys the participants'expressed cognitive states while a colored visualization oftheir locations on a two-dimensional projection of cognitive state attributes aids their navigation through the space.MoodMixer is a collaborative experience that incorporatesaspects of both passive and active EEG sonification andperformance art. We discuss the technical design of the installation and place its collaborative sonification aestheticdesign within the context of existing EEG-based music andart.},
address = {Oslo, Norway},
author = {Leslie, Grace and Mullen, Tim},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {BCMI,EEG,collaboration,sonification,visualization},
pages = {296--299},
title = {{MoodMixer : EEG-based Collaborative Sonification}},
url = {http://www.nime.org/proceedings/2011/nime2011{\_}296.pdf},
year = {2011}
}
@inproceedings{El-Shimy:2012,
abstract = {In this paper, we discuss the design and testing of a reactive envi-ronment for musical performance. Driven by the interpersonal in-teractions amongst musicians, our system gives users, i.e., several musicians playing together in a band, real-time control over certain aspects of their performance, enabling them to change volume lev-els dynamically simply by moving around. It differs most notably from the majority of ventures into the design of novel musical in-terfaces and installations in its multidisciplinary approach, draw-ing on techniques from Human-Computer Interaction, social sci-ences and ludology. Our User-Centered Design methodology was central to producing an interactive environment that enhances tra-ditional performance with novel functionalities. During a formal experiment, musicians reported finding our system exciting and enjoyable. We also introduce some additional interactions that can further enhance the interactivity of our reactive environment. In describing the particular challenges of working with such a unique and creative user as the musician, we hope that our approach can be of guidance to interface developers working on applications of a creative nature.},
address = {Ann Arbor, Michigan},
author = {El-Shimy, Dalia and Hermann, Thomas and Cooperstock, Jeremy R},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
publisher = {University of Michigan},
title = {{A Reactive Environment for Dynamic Volume Control}},
url = {http://www.nime.org/proceedings/2012/nime2012{\_}88.pdf},
year = {2012}
}
@inproceedings{Trail:2012a,
abstract = {Hyper-instruments extend traditional acoustic instruments with sensing technologies that capture digitally subtle and sophisticated aspects of human performance. They leverage the long training and skills of performers while simultaneously providing rich possibilities for digital control. Many existing hyper-instruments suffer from being one of a kind instruments that require invasive modifications to the underlying acoustic instrument. In this paper we focus on the pitched percussion family and describe a non-invasive sensing approach for extending them to hyper-instruments. Our primary concern is to retain the technical integrity of the acoustic instrument and sound production methods while being able to intuitively interface the computer. This is accomplished by utilizing the Kinect sensor to track the position of the mallets without any modification to the instrument which enables easy and cheap replication of the pro-posed hyper-instrument extensions. In addition we describe two approaches to higher-level gesture control that remove the need for additional control devices such as foot pedals and fader boxes that are frequently used in electro-acoustic performance. This gesture control integrates more organically with the natural flow of playing the instrument providing user selectable control over filter parameters, synthesis, sampling, sequencing, and improvisation using a commer-cially available low-cost sensing apparatus.},
address = {Ann Arbor, Michigan},
author = {Trail, Shawn and Dean, Michael and Odowichuk, Gabrielle and Tavares, Tiago Fernandes and Driessen, Peter and Schloss, Andrew and Tzanetakis, George},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
publisher = {University of Michigan},
title = {{Non-invasive sensing and gesture control for pitched percussion hyper-instruments using the Kinect}},
url = {http://www.nime.org/proceedings/2012/nime2012{\_}297.pdf},
year = {2012}
}
@inproceedings{Lai:2012,
abstract = {This paper addresses the issue of engaging the audience with new musical instruments in live performance context. We introduce design concerns that we consider influential to enhance the communication flow between the audience and the performer. We also propose and put in practice a design approach that considers the use of performance space as a way to engage with the audience. A collaborative project, Sound Gloves, presented here exemplifies such a concept by dissolving the space between performers and audience. Our approach resulted in a continuous interaction between audience and performers, in which the social dynamics was changed in a positive way in a live performance context of NIMEs. Such an approach, we argue, may be considered as one way to further engage and interact with the audience.},
address = {Ann Arbor, Michigan},
author = {Lai, Chi-Hsia and Tahirolu, Koray},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {NIME,design approach,performance,wearable electronics},
publisher = {University of Michigan},
title = {{A Design Approach to Engage with Audience with Wearable Musical Instruments: Sound Gloves}},
url = {http://www.nime.org/proceedings/2012/nime2012{\_}197.pdf},
year = {2012}
}
@inproceedings{Henson:2012,
abstract = {This paper introduces the concept of Kugelschwung, a digital musical instrument centrally based around the use of pendulums and lasers to create unique and highly interactive electronic ambient soundscapes. Here, we explore the underlying design and physical construction of the instrument, as well as its implementation and feasibility as an instrument in the real world. To conclude, we outline potential expansions to the instrument, describing how its range of applications can be extended to accommodate a variety of musical styles.},
address = {Ann Arbor, Michigan},
author = {Henson, Jamie and Collins, Benjamin and Giles, Alexander and Webb, Kathryn and Livingston, Matthew and Mortensson, Thomas},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {electronic,expressive performance,instrument design,laser,pendulums,sampler,soundscape},
publisher = {University of Michigan},
title = {{Kugelschwung -a Pendulum-based Musical Instrument}},
url = {http://www.nime.org/proceedings/2012/nime2012{\_}131.pdf},
year = {2012}
}
@inproceedings{Beilharz:2012,
abstract = {In site-specific installation or situated media, a significant part of the "I" in NIME is the environment, the site and the implicit features of site such as humans, weather, materials, natural acoustics, etc. These could be viewed as design constraints, or features, even agency determining the outcome of responsive sound installation works. This paper discusses the notion of interface in public (especially outdoor) installation, starting with the authors' Sculpture by the Sea Windtraces work using this recent experience as the launch-pad, with reference to ways in which others have approached it (focusing on sensor, weather-activated outdoor installations in a brief traverse of related cases, e.g. works by Garth Paine, James Bulley and Daniel Jones, and David Bowen). This is a dialogical paper on the topic of interface and `site' as the aetiology of interaction/interface/instrument and its type of response (e.g. to environment and audience). While the focus here is on outdoor factors (particularly the climatic environment), indoor site-specific installation also experiences the effects of ambient noise, acoustic context, and audience as integral agents in the interface and perception of the work, its musical expression. The way in which features of the situation are integrated has relevance for others in the NIME community in the design of responsive spaces, art installation, and large-scale or installed instruments in which users, participants, acoustics play a significant role.},
address = {Ann Arbor, Michigan},
author = {Beilharz, Kirsty and Martin, Aengus},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {NIME,outdoor sound installation,site-specific installation},
publisher = {University of Michigan},
title = {{The `Interface' in Site-Specific Sound Installation}},
url = {http://www.nime.org/proceedings/2012/nime2012{\_}175.pdf},
year = {2012}
}
@inproceedings{t-Klooster:2012,
abstract = {This paper describes the development of the Emotion Light, an interactive biofeedback artwork where the user listens to a piece of electronic music whilst holding a semi-transparent sculpture that tracks his/her bodily responses and translates these into changing light patterns that emerge from the sculpture. The context of this work is briefly described and the questions it poses are derived from interviews held with audience members.},
address = {Ann Arbor, Michigan},
author = {{van 't Klooster}, Adinda},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Interactive biofeedback artwork,aesthetic interaction,affective computing,biology inspired system,biosignals,bodily response,heart rate,mediating body,music and emotion,novel interfaces,practice based research},
publisher = {University of Michigan},
title = {{The body as mediator of music in the Emotion Light}},
url = {http://www.nime.org/proceedings/2012/nime2012{\_}167.pdf},
year = {2012}
}
@inproceedings{Endo:2012,
abstract = {Tweet Harp is a musical instrument using Twitter and a laser harp. This instrument features the use of the human voice speaking tweets in Twitter as sounds for music. It is played by touching the six harp strings of laser beams. Tweet Harp gets the latest tweets from Twitter in real-time, and it creates music like a song with unexpected words. It also creates animation displaying the texts at the same time. The audience can visually enjoy this performance by sounds synchronized with animation. If the audience has a Twitter account, they can participate in the performance by tweeting.},
address = {Ann Arbor, Michigan},
author = {Endo, Ayaka and Moriyama, Takuma and Kuhara, Yasuo},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {AppleScript,Arduino,Max/MSP,Quartz Composer,TTS,Twitter,laser harp,speech,text,voice},
publisher = {University of Michigan},
title = {{Tweet Harp: Laser Harp Generating Voice and Text of Real-time Tweets in Twitter}},
url = {http://www.nime.org/proceedings/2012/nime2012{\_}66.pdf},
year = {2012}
}
@inproceedings{Lu:2012,
abstract = {WIS platform is a wireless interactive sensor platform de-signed to support dynamic and interactive applications. The platform consists of a capture system which includes multi-ple on-body Zigbee compatible motion sensors, a processing unit and an audio-visual display control unit. It has a com-plete open architecture and provides interfaces to interact with other user-designed applications. Therefore, WIS plat-form is highly extensible. Through gesture recognitions by on-body sensor nodes and data processing, WIS platform can offer real-time audio and visual experiences to the users. Based on this platform, we set up a multimedia installation that presents a new interaction model between the partic-ipants and the audio-visual environment. Furthermore, we are also trying to apply WIS platform to other installations and performances.},
address = {Ann Arbor, Michigan},
author = {Lu, Jia-Liang and Fang, Da-Lei and Qin, Yi and Tang, Jiu-Qiang},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Audio-visual experience,Interactive},
publisher = {University of Michigan},
title = {{Wireless Interactive Sensor Platform for Real-Time Audio-Visual Experience}},
url = {http://www.nime.org/proceedings/2012/nime2012{\_}98.pdf},
year = {2012}
}
@inproceedings{Ouzounian:2012,
abstract = {Music for Sleeping {\&} Waking Minds (2011-2012) is a new, overnight work in which four performers fall asleep while wearing custom designed EEG sensors which monitor their brainwave activity. The data gathered from the EEG sensors is applied in real time to different audio and image signal processing functions, resulting in continuously evolving multi-channel sound environment and visual projection. This material serves as an audiovisual description of the individual and collective neurophysiological state of the ensemble. Audiences are invited to experience the work in different states of attention: while alert and asleep, resting and awakening.},
address = {Ann Arbor, Michigan},
author = {Ouzounian, Gascia and Knapp, R Benjamin and Lyon, Eric and DuBois, Luke},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {BCI,EEG,bio art,biosignals,consciousness,dream,sleep},
publisher = {University of Michigan},
title = {{Music for Sleeping {\&} Waking Minds}},
url = {http://www.nime.org/proceedings/2012/nime2012{\_}229.pdf},
year = {2012}
}
@inproceedings{Bukvic:2012,
abstract = {In the following paper we propose a new tiered granularity approach to developing modules or abstractions in the Pd-L2Ork visual multimedia programming environment with the specific goal of devising creative environments that scale their educational scope and difficulty to encompass several stages within the context of primary and secondary (K-12) education. As part of a preliminary study, the team designed modules targeting 4th and 5th grade students, the primary focus being exploration of creativity and collaborative learning. The resulting environment infrastructure -coupled with the Boys {\&} Girls Club of Southwest Virginia Satellite Linux Laptop Orchestra -offers opportunities for students to design and build original instruments, master them through a series of rehearsals, and ultimately utilize them as part of an ensemble in a performance of a predetermined piece whose parameters are coordinated by instructor through an embedded networked module. The ensuing model will serve for the assessment and development of a stronger connection with content-area standards and the development of creative thinking and collaboration skills.},
address = {Ann Arbor, Michigan},
author = {Bukvic, Ivica and Baum, Liesl and Layman, Bennett and Woodard, Kendall},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Education,Granular,K-12,L2Ork,Learning Objects,PdL2Ork},
publisher = {University of Michigan},
title = {{Granular Learning Objects for Instrument Design and Collaborative Performance in K-12 Education}},
url = {http://www.nime.org/proceedings/2012/nime2012{\_}315.pdf},
year = {2012}
}
@inproceedings{dAlessandro:2012,
abstract = {We present the integration of two musical interfaces into a new music-making system that seeks to capture the expe-rience of a choir and bring it into the mobile space. This system relies on three pervasive technologies that each support a different part of the musical experience. First, the mobile device application for performing with an artificial voice, called ChoirMob. Then, a central composing and conducting application running on a local interactive display, called Vuzik. Finally, a network protocol to synchronize the two. ChoirMob musicians can perform music together at any location where they can connect to a Vuzik central conducting device displaying a composed piece of music. We explored this system by creating a chamber choir of ChoirMob performers, consisting of both experienced musicians and novices, that performed in rehearsals and live concert scenarios with music composed using the Vuzik interface.},
address = {Ann Arbor, Michigan},
author = {D'Alessandro, Nicolas and Pon, Aura and Wang, Johnty and Eagle, David and Sharlin, Ehud and Fels, Sidney},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {ChoirMob,OSC,Vuzik,choir,interactive display,interface design,mobile music,singing synthesis,social music},
publisher = {University of Michigan},
title = {{A Digital Mobile Choir: Joining Two Interfaces towards Composing and Performing Collaborative Mobile Music}},
url = {http://www.nime.org/proceedings/2012/nime2012{\_}310.pdf},
year = {2012}
}
@inproceedings{Reboursiere:2012,
abstract = {In this paper we present a series of algorithms developed to detect the following guitar playing techniques : bend, hammer-on, pull-off, slide, palm muting and harmonic. Detection of playing techniques can be used to control exter-nal content (i.e audio loops and effects, videos, light events, etc.), as well as to write real-time score or to assist guitar novices in their learning process. The guitar used is a Godin Multiac with an under-saddle RMC hexaphonic piezo pickup (one pickup per string, i.e six mono signals).},
address = {Ann Arbor, Michigan},
author = {Reboursi{\`{e}}re, Lo{\"{i}}c and L{\"{a}}hdeoja, Otso and Drugman, Thomas and Dupont, St{\'{e}}phane and Picard-Limpens, C{\'{e}}cile and Riche, Nicolas},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Guitar audio analysis,augmented guitar,controller,hexaphonic pickup,playing techniques},
publisher = {University of Michigan},
title = {{Left and right-hand guitar playing techniques detection}},
url = {http://www.nime.org/proceedings/2012/nime2012{\_}213.pdf},
year = {2012}
}
@inproceedings{Fyfe:2012,
abstract = {Message mapping between control interfaces and sound engines is an important task that could benefit from tools that streamline development. A new Open Sound Control (OSC) namespace called Nexus Data Exchange Format (NDEF) streamlines message mapping by offering developers the ability to manage sound engines as network nodes and to query those nodes for the messages in their OSC address spaces. By using NDEF, developers will have an eas-ier time managing nodes and their messages, especially for scenarios in which a single application or interface controls multiple sound engines. NDEF is currently implemented in the JunctionBox interaction toolkit but could easily be implemented in other toolkits.},
address = {Ann Arbor, Michigan},
author = {Fyfe, Lawrence and Tindale, Adam R and Carpendale, Sheelagh},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {OSC,interaction,namespace,node},
publisher = {University of Michigan},
title = {{Node and Message Management with the JunctionBox Interaction Toolkit}},
url = {http://www.nime.org/proceedings/2012/nime2012{\_}299.pdf},
year = {2012}
}
@inproceedings{Wyse:2012,
abstract = {The upper limit of frequency sensitivity for vibrotactile stimulation of the fingers and hand is commonly accepted as 1 kHz. However, during the course of our research to develop a full-hand vibrotactile musical communication device for the hearing-impaired, we repeatedly found evidence suggesting sensitivity to higher frequencies. Most of the studies on which vibrotactile sensitivity are based have been conducted using sine tones delivered by point-contact actuators. The current study was designed to investigate vibrotactile sensitivity using complex signals and full, open-hand contact with a flat vibrating surface representing more natural environmental conditions. Sensitivity to frequencies considerably higher than previously reported was demonstrated for all the signal types tested. Furthermore, complex signals seem to be more easily detected than sine tones, especially at low frequencies. Our findings are applicable to a general understanding of sensory physiology, and to the development of new vibrotactile display devices for music and other applications.},
address = {Ann Arbor, Michigan},
author = {Wyse, Lonce and Nanayakkara, Suranga and Seekings, Paul and Ong, Sim Heng and Taylor, Elizabeth},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Haptic Sensitivity,Hearing-impaired,Vibrotactile Threshold},
publisher = {University of Michigan},
title = {{Palm-area sensitivity to vibrotactile stimuli above 1{\~{}}{\{}kHz{\}}}},
url = {http://www.nime.org/proceedings/2012/nime2012{\_}105.pdf},
year = {2012}
}
@inproceedings{Yuksel:2012,
abstract = {In this work, a comprehensive study is performed on the relationship between audio, visual and emotion by applying the principles of cognitive emotion theory into digital creation. The study is driven by an audiovisual emotion library project that is named AVIEM, which provides an interactive interface for experimentation and evaluation of the perception and creation processes of audiovisuals. AVIEM primarily consists of separate audio and visual libraries and grows with user contribution as users explore different combinations between them. The library provides a wide range of experimentation possibilities by allowing users to create audiovisual relations and logging their emotional responses through its interface. Besides being a resourceful tool of experimentation, AVIEM aims to become a source of inspiration, where digitally created abstract virtual environments and soundscapes can elicit target emotions at a preconscious level, by building genuine audiovisual relations that would engage the viewer on a strong emotional stage. Lastly, various schemes are proposed to visualize information extracted through AVIEM, to improve the navigation and designate the trends and dependencies among audiovisual relations.},
address = {Ann Arbor, Michigan},
author = {Yuksel, Kamer Ali and Buyukbas, Sinan and Ayiter, Elif},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Designing emotive audiovisuals,audiovisual perception and interaction,cognitive emotion theory,synaesthesia},
publisher = {University of Michigan},
title = {{An Interface for Emotional Expression in Audio-Visuals}},
url = {http://www.nime.org/proceedings/2012/nime2012{\_}60.pdf},
year = {2012}
}
@inproceedings{Nort:2012,
abstract = {In this paper we discuss aspects of our work in develop-ing performance systems that are geared towards human-machine co-performance with a particular emphasis on improvisation. We present one particular system, FILTER, which was created in the context of a larger project related to artificial intelligence and performance, and has been tested in the context of our electro-acoustic performance trio. We discuss how this timbrally rich and highly non-idiomatic musical context has challenged the design of the system, with particular emphasis on the mapping of machine listening parameters to higher-level behaviors of the system in such a way that spontaneity and creativity are encouraged while maintaining a sense of novel dialogue.},
address = {Ann Arbor, Michigan},
author = {Nort, Doug Van and Braasch, Jonas and Oliveros, Pauline},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Electroacoustic Improvisation,Machine Learning,Mapping,Sonic Gestures,Spatialization},
publisher = {University of Michigan},
title = {{Mapping to musical actions in the FILTER system}},
url = {http://www.nime.org/proceedings/2012/nime2012{\_}235.pdf},
year = {2012}
}
@inproceedings{Mitchell:2012,
abstract = {This paper presents a toolbox of gestural control mechanisms which are available when the input sensing apparatus is a pair of data gloves fitted with orientation sensors. The toolbox was developed in advance of a live music performance in which the mapping from gestural input to audio output was to be developed rapidly in collaboration with the performer. The paper begins with an introduction to the associated literature before introducing a range of continuous, discrete and combined control mechanisms, enabling a flexible range of mappings to be explored and modified easily. An application of the toolbox within a live music performance is then described with an evaluation of the system with ideas for future developments.},
address = {Ann Arbor, Michigan},
author = {Mitchell, Tom and Madgwick, Sebastian and Heap, Imogen},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Computer Music,Data Gloves,Gestural Control},
publisher = {University of Michigan},
title = {{Musical Interaction with Hand Posture and Orientation: A Toolbox of Gestural Control Mechanisms}},
url = {http://www.nime.org/proceedings/2012/nime2012{\_}272.pdf},
year = {2012}
}
@inproceedings{Oliver:2012,
abstract = {There is some evidence that structured training can benefit cochlear implant (CI) users' appraisal of music as well as their music perception abilities. There are currently very limited music training resources available for CI users to explore. This demonstration will introduce delegates to the `Interactive Music Awareness Program' (IMAP) for cochlear implant users, which was developed in response to the need for a client-centered, structured, interactive, creative, open-ended, educational and challenging music (re)habilitation resource.},
address = {Ann Arbor, Michigan},
author = {Oliver, Benjamin and van Besouw, Rachel M and Nicholls, David R},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {auditory training,client-centred software,cochlear implants,interactive learning,music,perception,rehabilitation},
publisher = {University of Michigan},
title = {{The `Interactive Music Awareness Program' (IMAP) for Cochlear Implant Users}},
url = {http://www.nime.org/proceedings/2012/nime2012{\_}109.pdf},
year = {2012}
}
@inproceedings{Kimura:2012,
abstract = {As a 2010 Artist in Residence in Musical Research at IRCAM, Mari Kimura used the Augmented Violin to develop new compositional approaches, and new ways of creating interactive performances [1]. She contributed her empirical and historical knowledge of violin bowing technique, working with the Real Time Musical Interactions Team at IRCAM. Thanks to this residency, her ongoing long-distance collaboration with the team since 2007 dramatically accelerated, and led to solving several compositional and calibration issues of the Gesture Follower (GF) [2]. Kimura was also the first artist to develop projects between the two teams at IRCAM, using OMAX (Musical Representation Team) with GF. In the past year, the performance with Augmented Violin has been expanded in larger scale interactive audio/visual projects as well. In this paper, we report on the various techniques developed for the Augmented Violin and compositions by Kimura using them, offering specific examples and scores.},
address = {Ann Arbor, Michigan},
author = {Kimura, Mari and Rasamimanana, Nicolas and Bevilacqua, Fr{\'{e}}d{\'{e}}ric and Schnell, Norbert and Zamborlin, Bruno and Fl{\'{e}}ty, Emmanuel},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Augmented Violin,Gesture Follower,Interactive Performance},
publisher = {University of Michigan},
title = {{Extracting Human Expression For Interactive Composition with the Augmented Violin}},
url = {http://www.nime.org/proceedings/2012/nime2012{\_}279.pdf},
year = {2012}
}
@inproceedings{Han:2012,
abstract = {Virtual Pottery is an interactive audiovisual piece that uses hand gesture to create 3D pottery objects and sound shape. Using the OptiTrack motion capture (Rigid Body) system at TransLab in UCSB, performers can take a glove with attached trackers, move the hand in x, y, and z axis and create their own sound pieces. Performers can also manipulate their pottery pieces in real time and change arrangement on the musical score interface in order to create a continuous musical composition. In this paper we address the relationship between body, sound and 3D shapes. We also describe the origin of Virtual Pottery, its design process, discuss its aesthetic value and musical sound synthesis system, and evaluate the overall experience.},
address = {Ann Arbor, Michigan},
author = {Han, Yoon Chung and Han, Byeong-jun},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Virtual Pottery,interactive sound installation.,motion and gesture,motion perception,pottery,sound synthesis,virtual musical instrument},
publisher = {University of Michigan},
title = {{Virtual Pottery: An Interactive Audio-Visual Installation}},
url = {http://www.nime.org/proceedings/2012/nime2012{\_}216.pdf},
year = {2012}
}
@inproceedings{Hansen:2012,
abstract = {This paper presents the results of user interaction with two explorative music environments (sound system A and B) that were inspired from the Banda Linda music tradition in two different ways. The sound systems adapted to how a team of two players improvised and made a melody together in an interleaved fashion: Systems A and B used a fuzzy logic algorithm and pattern recognition to respond with modifications of a background rhythms. In an experiment with a pen tablet interface as the music instrument, users aged 10-13 were to tap tones and continue each other's melody. The sound systems rewarded users sonically, if they managed to add tones to their mutual melody in a rapid turn taking manner with rhythmical patterns. Videos of experiment sessions show that user teams contributed to a melody in ways that resemble conversation. Interaction data show that each sound system made player teams play in different ways, but players in general had a hard time adjusting to a non-Western music tradition. The paper concludes with a comparison and evaluation of the two sound systems. Finally it proposes a new approach to the design of collaborative and shared music environments that is based on ''listening applications''.},
address = {Ann Arbor, Michigan},
author = {Hansen, Anne-Marie Skriver and Andersen, Hans J{\o}rgen and Raudaskoski, Pirkko},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Music improvisation,interaction design.,interaction studies,novices,social learning},
publisher = {University of Michigan},
title = {{Two Shared Rapid Turn Taking Sound Interfaces for Novices}},
url = {http://www.nime.org/proceedings/2012/nime2012{\_}123.pdf},
year = {2012}
}
@inproceedings{Astrinaki:2012,
abstract = {In this paper, we describe our pioneering work in developing speech synthesis beyond the Text-To-Speech paradigm. We introduce tangible speech synthesis as an alternate way of envisioning how artificial speech content can be produced. Tangible speech synthesis refers to the ability, for a given system, to provide some physicality and interactivity to important speech production parameters. We present MAGE, our new software platform for high-quality reactive speech synthesis, based on statistical parametric modeling and more particularly hidden Markov models. We also introduce a new HandSketch-based musical instrument. This instrument brings pen and posture based interaction on the top of MAGE, and demonstrates a first proof of concept.},
address = {Ann Arbor, Michigan},
author = {Astrinaki, Maria and D'Alessandro, Nicolas and Dutoit, Thierry},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {HTS,Hidden Markov Models,MAGE,performative,software library,speech synthesis,tangible interaction},
publisher = {University of Michigan},
title = {{MAGE --A Platform for Tangible Speech Synthesis}},
url = {http://www.nime.org/proceedings/2012/nime2012{\_}164.pdf},
year = {2012}
}
@inproceedings{Barbosa:2012,
abstract = {The authors propose the development of a more complete Digital Music Instrument (DMI) evaluation methodology, which provides structured tools for the incremental development of prototypes based on user feedback. This paper emphasizes an important but often ignored stakeholder present in the context of musical performance: the audience. We demonstrate the practical application of an audience focused methodology through a case study (`Illusio'), discuss the obtained results and possible improvements for future works.},
address = {Ann Arbor, Michigan},
author = {Barbosa, Jer{\^{o}}nimo and Calegario, Filipe and Teichrieb, Ver{\^{o}}nica and Ramalho, Geber and McGlynn, Patrick},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Empirical methods,Illusio,digital musical instruments,evaluation methodology,quantitative,usability testing and evaluation},
publisher = {University of Michigan},
title = {{Considering Audience's View Towards an Evaluation Methodology for Digital Musical Instruments}},
url = {http://www.nime.org/proceedings/2012/nime2012{\_}174.pdf},
year = {2012}
}
@inproceedings{Clay:2012,
abstract = {The augmented ballet project aims at gathering research from several fields and directing them towards a same application case: adding virtual elements (visual and acoustic) to a dance live performance, and allowing the dancer to interact with them. In this paper, we describe a novel interaction that we used in the frame of this project: using the dancer's movements to recognize the emotions he expresses, and use these emotions to generate musical audio flows evolving in real-time. The originality of this interaction is threefold. First, it covers the whole interaction cycle from the input (the dancer's movements) to the output (the generated music). Second, this interaction isn't direct but goes through a high level of abstraction: dancer's emotional expression is recognized and is the source of music generation. Third, this interaction has been designed and validated through constant collaboration with a choreographer, culminating in an augmented ballet performance in front of a live audience.},
address = {Ann Arbor, Michigan},
author = {Clay, Alexis and Couture, Nadine and Desainte-Catherine, Myriam and Vulliard, Pierre-Henri and Larralde, Joseph and Decarsin, Elodie},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Interactive sonification,gesture and music,interaction,live performance,motion,musical human-computer interaction},
publisher = {University of Michigan},
title = {{Movement to emotions to music: using whole body emotional expression as an interaction for electronic music generation}},
url = {http://www.nime.org/proceedings/2012/nime2012{\_}180.pdf},
year = {2012}
}
@inproceedings{Kikukawa:2012,
abstract = {We developed original solenoid actuator units with several built-in sensors, and produced a box-shaped musical inter-face ``PocoPoco'' using 16 units of them as a universal input/output device. We applied up-and-down movement of the solenoid-units and user's intuitive input to musical interface. Using transformation of the physical interface, we can apply movement of the units to new interaction design. At the same time we intend to suggest a new interface whose movement itself can attract the user.},
address = {Ann Arbor, Michigan},
author = {Kikukawa, Yuya and Kanai, Takaharu and Suzuki, Tatsuhiko and Yoshiike, Toshiki and Baba, Tetsuaki and Kushiyama, Kumiko},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {interaction design,kinetic,moving,musical interface,tactile},
publisher = {University of Michigan},
title = {{PocoPoco: A Kinetic Musical Interface With Electro-Magnetic Levitation Units}},
url = {http://www.nime.org/proceedings/2012/nime2012{\_}232.pdf},
year = {2012}
}
@inproceedings{Gong:2012,
abstract = {This paper describes a novel music control sensate surface, which enables integration between any musical instruments with a v ersatile, customizable, and essentially cost-effective user interface. This sensate surface is based on c onductive inkjet printing technology which allows capacitive sensor electrodes and connections between electronics components to be printed onto a large roll of flexible substrate that is unrestricted in length. The high dynamic range capacitive sensing electrodes can not only infer touch, but near-range, non-contact gestural nuance in a music performance. With this sensate surface, users can ``cut'' out their desired shapes, ``paste'' the number of inputs, and customize their controller interface, which can then send signals wirelessly to effects or software synthesizers. We seek to find a solution for integrating the form factor of traditional music controllers seamlessly on top of one's music instrument and meanwhile adding expressiveness to the music performance by sensing and incorporating movements and gestures to manipulate the musical output. We present an example of implementation on an electric ukulele and provide several design examples to demonstrate the versatile capabilities of this system.},
address = {Ann Arbor, Michigan},
author = {Gong, Nan-Wei and Zhao, Nan and Paradiso, Joseph A},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Sensate surface,customizable controller surface,flexible electronics,music controller skin},
publisher = {University of Michigan},
title = {{A Customizable Sensate Surface for Music Control}},
url = {http://www.nime.org/proceedings/2012/nime2012{\_}201.pdf},
year = {2012}
}
@inproceedings{Bergsland:2012,
abstract = {As a part of the research project Voice Meetings, a solo live-electronic vocal performance was presented for 63 students. Through a mixed method approach applying both written and oral response, feedback from one blindfolded and one seeing audience group was collected and analyzed. There were marked differences between the groups regarding focus, in that the participants in blindfolded group tended to focus on fewer aspects, have a heightened focus and be less distracted than the seeing group. The seeing group, on its part, focused more on the technological instruments applied in the performance, the performer herself and her actions. This study also shows that there were only minor differences between the groups regarding the experience of skill and control, and argues that this observation can be explained by earlier research on skill in NIMEs.},
address = {Ann Arbor, Michigan},
author = {Bergsland, Andreas and {\AA}se, Tone},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Performance,acousmatic listening,audience reception,live-electronics,qualitative research,voice},
publisher = {University of Michigan},
title = {{Using a seeing/blindfolded paradigm to study audience experiences of live-electronic performances with voice}},
url = {http://www.nime.org/proceedings/2012/nime2012{\_}168.pdf},
year = {2012}
}
@inproceedings{Chacin:2012,
abstract = {This paper is an in depth exploration of the fashion object and device, the Play-A-Grill. It details inspirations, socio-cultural implications, technical function and operation, and potential applications for the Play-A-Grill system.},
address = {Ann Arbor, Michigan},
author = {Chacin, Aisen Caro},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Digital Music Players,Grills,Hip Hop,Mouth Controllers,Mouth Jewelry,Music Fashion,Rap,and Bone Conduction Hearing.},
publisher = {University of Michigan},
title = {{Play-A-Grill: Music To Your Teeth}},
url = {http://www.nime.org/proceedings/2012/nime2012{\_}48.pdf},
year = {2012}
}
@inproceedings{Wolf:2013,
abstract = {As any computer user employs the Internet to accomplish everyday activities, a flow of data packets moves across the network, forming their own patterns in response to his or her actions. Artists and sound designers who are interested in accessing that data to make music must currently possess low-level knowledge of Internet protocols and spend signifi-cant effort working with low-level networking code. We have created SonNet, a new software tool that lowers these practical barriers to experimenting and composing with network data. SonNet executes packet-sniffng and network connection state analysis automatically, and it includes an easy-touse ChucK object that can be instantiated, customized, and queried from a user's own code. In this paper, we present the design and implementation of the SonNet system, and we discuss a pilot evaluation of the system with computer music composers. We also discuss compositional applications of SonNet and illustrate the use of the system in an example composition.},
address = {Daejeon, Republic of Korea},
author = {Wolf, KatieAnna E and Fiebrink, Rebecca},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Sonification,compositional tools,network data},
month = {may},
pages = {503--506},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{SonNet: A Code Interface for Sonifying Computer Network Data}},
url = {http://nime.org/proceedings/2013/nime2013{\_}94.pdf},
year = {2013}
}
@inproceedings{Bortz:2013,
abstract = {Mountains and Valleys (an anonymous name for confidentiality) is a communal,site-specific installation that takes shape as a spatially-responsiveaudio-visual field. The public participates in the creation of theinstallation, resulting in shared ownership of the work between both theartists and participants. Furthermore, the installation takes new shape in eachrealization, both to incorporate the constraints and affordances of eachspecific site, as well as to address the lessons learned from the previousiteration. This paper describes the development and execution of Mountains andValleys over its most recent version, with an eye toward the next iteration ata prestigious art museum during a national festival in Washington, D.C.},
address = {Daejeon, Republic of Korea},
author = {Bortz, Brennon and Ishida, Aki and Bukvic, Ivica and Knapp, R Benjamin},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Japanese lanterns,Participatory creation,communal interaction,fields,interactive installation},
month = {may},
pages = {73--78},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Lantern Field: Exploring Participatory Design of a Communal, Spatially Responsive Installation}},
url = {http://nime.org/proceedings/2013/nime2013{\_}192.pdf},
year = {2013}
}
@inproceedings{Lai:2013,
abstract = {This paper presents observations from investigating audience experience of apractice-based research in live sound performance with electronics. In seekingto understand the communication flow and the engagement between performer andaudience in this particular performance context, we designed an experiment thatinvolved the following steps: (a) performing WOSAWIP at a new media festival,(b) conducting a qualitative research study with audience members and (c)analyzing the data for new insights.},
address = {Daejeon, Republic of Korea},
author = {Lai, Chi-Hsia and Bovermann, Till},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Audience Experience Study,Evaluation,Live Performance,Research Methods},
month = {may},
pages = {170--173},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Audience Experience in Sound Performance}},
url = {http://nime.org/proceedings/2013/nime2013{\_}197.pdf},
year = {2013}
}
@inproceedings{Wang:2013,
abstract = {By building a wired passive stylus we have added pressure sensitivity toexisting capacitive touch screen devices for less than},
address = {Daejeon, Republic of Korea},
author = {Wang, Johnty and D'Alessandro, Nicolas and Pon, Aura and Fels, Sidney},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {10 in materials,about1/10th the cost of existing solutions. The s,and increased latency equalto the period of at le,theoccupation of one audio input and output chann},
month = {may},
pages = {input interfaces, touch screens, tablets, pressure},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{PENny: An Extremely Low-Cost Pressure-Sensitive Stylus for Existing Capacitive Touchscreens}},
url = {http://nime.org/proceedings/2013/nime2013{\_}150.pdf},
year = {2013}
}
@inproceedings{McLean:2013,
abstract = {The Human vocal tract is considered for its sonorous qualities incarrying prosodic information, which implicates vision in theperceptual processes of speech. These considerations are put in thecontext of previous work in NIME, forming background for theintroduction of two sound installations; ``Microphone'', which uses acamera and computer vision to translate mouth shapes to sounds, and``Microphone II'', a work-in-progress, which adds physical modellingsynthesis as a sound source, and visualisation of mouth movements.},
address = {Daejeon, Republic of Korea},
author = {McLean, Alex and Shin, EunJoo and Ng, Kia},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {computer vision,face tracking,installation,microphone},
month = {may},
pages = {381--384},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Paralinguistic Microphone}},
url = {http://nime.org/proceedings/2013/nime2013{\_}122.pdf},
year = {2013}
}
@inproceedings{Kleinberger:2013,
abstract = {PAMDI is an electromechanical music controller based on an expansion of thecommon metal music boxes. Our system enables an augmentation of the musicalproperties by adding different musical channels triggered and parameterized bynatural gestures during the ``performance''. All the channels are generatedform the original melody recorded once at the start.To capture and treat the different expressive parameters both natural andintentional, our platform is composed of a metallic structure supportingsensors. The measured values are processed by an arduino system that finallysends the results by serial communication to a Max/MSP patch for signaltreatment and modification. We will explain how our embedded instrument aims to bring a certain awarenessto the player of the mapping and the potential musical freedom of the veryspecific -- and not that much automatic - instrument that is a music box. Wewill also address how our design tackles the different questions of mapping,ergonomics and expressiveness while choosing the controller modalities and theparameters to be sensed.},
address = {Daejeon, Republic of Korea},
author = {Kleinberger, R{\'{e}}becca},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Tangible interface,mapping.,mechanical and electronic coupling,music box,musical controller},
month = {may},
pages = {19--20},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{PAMDI Music Box: Primarily Analogico-Mechanical, Digitally Iterated Music Box}},
url = {http://nime.org/proceedings/2013/nime2013{\_}24.pdf},
year = {2013}
}
@inproceedings{Sarwate:2013,
abstract = {The Variator is a compositional assistance tool that allows users to quicklyproduce and experiment with variations on musical objects, such as chords,melodies, and chord progressions. The transformations performed by the Variatorcan range from standard counterpoint transformations (inversion, retrograde,transposition) to more complicated custom transformations, and the system isbuilt to encourage the writing of custom transformations.This paper explores the design decisions involved in creating a compositionalassistance tool, describes the Variator interface and a preliminary set ofimplemented transformation functions, analyzes the results of the evaluationsof a prototype system, and lays out future plans for expanding upon thatsystem, both as a stand-alone application and as the basis for an opensource/collaborative community where users can implement and share their owntransformation functions.},
address = {Daejeon, Republic of Korea},
author = {Sarwate, Avneesh and Fiebrink, Rebecca},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Composition assistance tool,computer-aided composition,social composition},
month = {may},
pages = {279--282},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Variator: A Creativity Support Tool for Music Composition}},
url = {http://nime.org/proceedings/2013/nime2013{\_}224.pdf},
year = {2013}
}
@inproceedings{Han:2013a,
abstract = {This paper presents a framework that transforms fingerprint patterns intoaudio. We describe Digiti Sonus, an interactive installation performingfingerprint sonification and visualization, including novel techniques forrepresenting user-intended fingerprint expression as audio parameters. In orderto enable personalized sonification and broaden timbre of sound, theinstallation employs sound synthesis based on various visual feature analysissuch as minutiae extraction, area, angle, and push pressure of fingerprints.The sonification results are discussed and the diverse timbres of soundretrieved from different fingerprints are compared.},
address = {Daejeon, Republic of Korea},
author = {Han, Yoon Chung and Han, Byeong-jun and Wright, Matthew},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Fingerprint,Fingerprint sonification,biometric data,interactive sonification,sound synthesis},
month = {may},
pages = {136--141},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Digiti Sonus: Advanced Interactive Fingerprint Sonification Using Visual Feature Analysis}},
url = {http://nime.org/proceedings/2013/nime2013{\_}170.pdf},
year = {2013}
}
@inproceedings{Oda:2013,
abstract = {The Internet allows musicians and other artists to collaborate remotely.However, network latency presents a fundamental challenge for remotecollaborators who need to coordinate and respond to each other's performancein real time. In this paper, we investigate the viability of predictingpercussion hits before they have occurred, so that information about thepredicted drum hit can be sent over a network, and the sound can be synthesizedat a receiver's location at approximately the same moment the hit occurs atthe sender's location. Such a system would allow two percussionists to playin perfect synchrony despite the delays caused by computer networks. Toinvestigate the feasibility of such an approach, we record vibraphone malletstrikes with a high-speed camera and track the mallet head position. We showthat 30 ms before the strike occurs, it is possible to predict strike time andvelocity with acceptable accuracy. Our method fits a second-order polynomial tothe data to produce a strike time prediction that is within the bounds ofperceptual synchrony, and a velocity estimate that will enable the soundpressure level of the synthesized strike to be accurate within 3 dB.},
address = {Daejeon, Republic of Korea},
author = {Oda, Reid and Finkelstein, Adam and Fiebrink, Rebecca},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Networked performance,computer vision,prediction},
month = {may},
pages = {94--97},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Towards Note-Level Prediction for Networked Music Performance}},
url = {http://nime.org/proceedings/2013/nime2013{\_}258.pdf},
year = {2013}
}
@inproceedings{Bragg:2013,
abstract = {This paper presents a graph-theoretic model that supports the design andanalysis of data flow within digital musical instruments (DMIs). The state ofthe art in DMI design fails to provide any standards for the scheduling ofcomputations within a DMI's data flow. It does not provide a theoreticalframework within which we can analyze different scheduling protocols and theirimpact on the DMI's performance. Indeed, the mapping between the DMI's sensoryinputs and sonic outputs is classically treated as a black box. DMI designersand builders are forced to design and schedule the flow of data through thisblack box on their own. Improper design of the data flow can produceundesirable results, ranging from overflowing buffers that cause system crashesto misaligned sensory data that result in strange or disordered sonic events.In this paper, we attempt to remedy this problem by providing a framework forthe design and analysis of the DMI data flow. We also provide a schedulingalgorithm built upon that framework that guarantees desirable properties forthe resulting DMI.},
address = {Daejeon, Republic of Korea},
author = {Bragg, Danielle},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {DMI design,data flow,mapping function},
month = {may},
pages = {237--242},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Synchronous Data Flow Modeling for DMIs}},
url = {http://nime.org/proceedings/2013/nime2013{\_}139.pdf},
year = {2013}
}
@inproceedings{Kapur:2013,
abstract = {This paper describes the creation of new interfaces that extend traditionalKorean music and dance. Specifically, this research resulted in the design ofthe eHaegum (Korean bowed instrument), eJanggu (Korean drum), and ZiOm wearableinterfaces. The paper describes the process of making these new interfaces aswell as how they have been used to create new music and forms of digital artmaking that blend traditional practice with modern techniques.},
address = {Daejeon, Republic of Korea},
author = {Kapur, Ajay and Kim, Dae Hong and Kapur, Raakhi and Eom, Kisoon},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Hyperinstrument,Korean interface design,bowed controllers,dance controllers,drum controllers,wearable sensors},
month = {may},
pages = {45--48},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{New Interfaces for Traditional Korean Music and Dance}},
url = {http://nime.org/proceedings/2013/nime2013{\_}113.pdf},
year = {2013}
}
@inproceedings{Liu:2013,
abstract = {Cloud Bridge is an immersive interactive audiovisual software interface forboth data exploration and artistic creation. It explores how information can besonified and visualized to facilitate findings, and eventually becomeinteractive musical compositions. Cloud Bridge functions as a multi-user,multimodal instrument. The data represents the history of items checked out bypatrons of the Seattle Public Library. A single user or agroup of users functioning as a performance ensemble participate in the pieceby interactively querying the database using iOS devices. Each device isassociated with aunique timbre and color for contributing to the piece, whichappears on large shared screens and a surround-sound system for allparticipants and observers. Cloud Bridge leads to a new media interactiveinterface utilizing audio synthesis, visualization and real-time interaction.},
address = {Daejeon, Republic of Korea},
author = {Liu, Qian and Han, Yoon Chung and Kuchera-Morin, JoAnn and Wright, Matthew},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Data Sonification,Data Visualization,Open Sound Control,Sonic Interaction Design,Sonification,User Interface},
month = {may},
pages = {431--436},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Cloud Bridge: a Data-driven Immersive Audio-Visual Software Interface}},
url = {http://nime.org/proceedings/2013/nime2013{\_}250.pdf},
year = {2013}
}
@inproceedings{Tang:2013,
abstract = {CalliMusic, is a system developed for users to generate traditional Chinesemusic by writing Chinese ink brush calligraphy, turning the long-believedstrong linkage between the two art forms with rich histories into reality. Inaddition to traditional calligraphy writing instruments (brush, ink and paper),a camera is the only addition needed to convert the motion of the ink brushinto musical notes through a variety of mappings such as human-inspired,statistical and a hybrid. The design of the system, including details of eachmapping and research issues encountered are discussed. A user study of systemperformance suggests that the result is quite encouraging. The technique is,obviously, applicable to other related art forms with a wide range ofapplications.},
address = {Daejeon, Republic of Korea},
author = {Tang, Wai Wa and Chan, Stephen and Ngai, Grace and Leong, Hong-va},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Assisted Music Generation,Chinese Calligraphy,Chinese Music},
month = {may},
pages = {84--89},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Computer Assisted Melo-rhythmic Generation of Traditional Chinese Music from Ink Brush Calligraphy}},
url = {http://nime.org/proceedings/2013/nime2013{\_}208.pdf},
year = {2013}
}
@inproceedings{Hamano:2013,
abstract = {Electroencephalography (EEG) has been used to generate music for over 40 years,but the most recent developments in brain--computer interfaces (BCI) allowgreater control and more flexible expression for using new musical instrumentswith EEG. We developed a real-time musical performance system using BCItechnology and sonification techniques to generate imagined musical chords withorganically fluctuating timbre. We aim to emulate the expressivity oftraditional acoustic instruments. The BCI part of the system extracts patternsfrom the neural activity while a performer imagines a score of music. Thesonification part of the system captures non-stationary changes in the brainwaves and reflects them in the timbre by additive synthesis. In this paper, wediscuss the conceptual design, system development, and the performance of thisinstrument.},
address = {Daejeon, Republic of Korea},
author = {Hamano, Takayuki and Rutkowski, Tomasz and Terasawa, Hiroko and Okanoya, Kazuo and Furukawa, Kiyoshi},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Brain-computer interface (BCI),classification,qualitative and quantitative information,sonification},
month = {may},
pages = {49--54},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Generating an Integrated Musical Expression with a Brain--Computer Interface}},
url = {http://nime.org/proceedings/2013/nime2013{\_}120.pdf},
year = {2013}
}
@inproceedings{Pardue:2013,
abstract = {The Hand Controller is a new interface designed to enable a performer toachieve detailed control of audio and visual parameters through a tangibleinterface combined with motion tracking of the hands to capture large scalephysical movement. Such movement empowers an expressive dynamic for bothperformer and audience. However tracking movements in free space isnotoriously difficult for virtuosic performance. The lack of tactile feedbackleads to difficulty learning the repeated muscle movements required for precisecontrol. In comparison, the hands have shown an impressive ability to mastercomplex motor tasks through feel. The hand controller uses both modes ofinteraction. Electro-magnetic field tracking enables 6D hand motion trackingwhile two options provide tactile interaction- a set of tracks that providelinear positioning and applied finger pressure, or a set of trumpet like sliderkeys that provide continuous data describing key depth. Thumbs actuateadditional pressure sensitive buttons. The two haptic interfaces are mountedto a comfortable hand grip that allows a significant range of reach, andpressure to be applied without restricting hand movement highly desirable inexpressive motion.},
address = {Daejeon, Republic of Korea},
author = {Pardue, Laurel S and Sebastian, William},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {force sensing resistor,free gesture,hand,interface,new musical instrument,position tracking,tactile feedback},
month = {may},
pages = {90--93},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Hand-Controller for Combined Tactile Control and Motion Tracking}},
url = {http://nime.org/proceedings/2013/nime2013{\_}245.pdf},
year = {2013}
}
@inproceedings{Lo:2013,
abstract = {Mobile DJ is a music-listening system that allows multiple users to interactand collaboratively contribute to a single song over a social network. Activelistening through a tangible interface facilitates users to manipulate musicaleffects, such as incorporating chords or ``scratching'' the record. Acommunication and interaction server further enables multiple users to connectover the Internet and collaborate and interact through their music. User testsindicate that the device is successful at facilitating user immersion into theactive listening experience, and that users enjoy the added sensory input aswell as the novel way of interacting with the music and each other.},
address = {Daejeon, Republic of Korea},
author = {Lo, Kenneth W K and Lau, Chi Kin and Huang, Michael Xuelin and Tang, Wai Wa and Ngai, Grace and Chan, Stephen},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Mobile,interaction design,music,tangible user interface},
month = {may},
pages = {217--222},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Mobile DJ: a Tangible, Mobile Platform for Active and Collaborative Music Listening}},
url = {http://nime.org/proceedings/2013/nime2013{\_}81.pdf},
year = {2013}
}
@inproceedings{Pardue:2013a,
abstract = {This paper explores the potential of near-field optical reflective sensing formusical instrument gesture capture. Near-field optical sensors are inexpensive,portable and non-intrusive, and their high spatial and temporal resolutionmakes them ideal for tracking the finer motions of instrumental performance.The paper discusses general optical sensor performance with detailedinvestigations of three sensor models. An application is presented to violinbow position tracking using reflective sensors mounted on the stick. Bowtracking remains a difficult task, and many existing solutions are expensive,bulky, or offer limited temporal resolution. Initial results indicate that bowposition and pressure can be derived from optical measurements of thehair-string distance, and that similar techniques may be used to measure bowtilt.},
address = {Daejeon, Republic of Korea},
author = {Pardue, Laurel S and McPherson, Andrew},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {LED,bow tracking,gesture,near-field sensing,optical sensor,photodiode,phototransistor,reflectance,violin},
month = {may},
pages = {363--368},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Near-Field Optical Reflective Sensing for Bow Tracking}},
url = {http://nime.org/proceedings/2013/nime2013{\_}247.pdf},
year = {2013}
}
@inproceedings{Xiao:2013,
abstract = {The body channels rich layers of information when playing music, from intricatemanipulations of the instrument to vivid personifications of expression. Butwhen music is captured and replayed across distance and time, the performer'sbody is too often trapped behind a small screen or absent entirely.This paper introduces an interface to conjure the recorded performer bycombining the moving keys of a player piano with life-sized projection of thepianist's hands and upper body. Inspired by reflections on a lacquered grandpiano, our interface evokes the sense that the virtual pianist is playing thephysically moving keys.Through our interface, we explore the question of how to viscerally simulate aperformer's presence to create immersive experiences. We discuss designchoices, outline a space of usage scenarios and report reactions from users.},
address = {Daejeon, Republic of Korea},
author = {Xiao, Xiao and Pereira, Anna and Ishii, Hiroshi},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {augmented reality,body language,embodiment,musical expressivity,piano performance,player piano,recorded music},
month = {may},
pages = {7--12},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Conjuring the Recorded Pianist: A New Medium to Experience Musical Performance}},
url = {http://nime.org/proceedings/2013/nime2013{\_}28.pdf},
year = {2013}
}
@inproceedings{Astrinaki:2013,
abstract = {This paper describes the recent progress in our approach to generateperformative and controllable speech. The goal of the performative HMM-basedspeech and singing syn- thesis library, called Mage, is to have the ability togenerate natural sounding speech with arbitrary speaker's voicecharacteristics, speaking styles and expressions and at the same time to haveaccurate reactive user control over all the available production levels. Mageallows to arbitrarily change between voices, control speaking style or vocalidentity, manipulate voice characteristics or alter the targeted contexton-the-fly and also maintain the naturalness and intelligibility of the output.To achieve these controls, it was essential to redesign and improve the initiallibrary. This paper focuses on the improvements of the architectural design,the additional user controls and provides an overview of a prototype, where aguitar is used to reactively control the generation of a synthetic voice invarious levels.},
address = {Daejeon, Republic of Korea},
author = {Astrinaki, Maria and D'Alessandro, Nicolas and Reboursi{\`{e}}re, Lo{\"{i}}c and Moinet, Alexis and Dutoit, Thierry},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {augmented guitar,hexaphonic guitar,speech synthesis},
month = {may},
pages = {547--550},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{MAGE 2.0: New Features and its Application in the Development of a Talking Guitar}},
url = {http://nime.org/proceedings/2013/nime2013{\_}214.pdf},
year = {2013}
}
@inproceedings{Andersson:2013,
abstract = {Our voice and body are important parts of our self-experience, andcommunication and relational possibilities. They gradually become moreimportant for Interaction Design, due to increased development of tangibleinteraction and mobile communication. In this paper we present and discuss ourwork with voice and tangible interaction in our ongoing research project XXXXX.The goal is to improve health for families, adults and children withdisabilities through use of collaborative, musical, tangible media. We build onuse of voice in Music Therapy and on a humanistic health approach. Ourchallenge is to design vocal and tangible interactive media that through usereduce isolation and passivity and increase empowerment for the users. We usesound recognition, generative sound synthesis, vibrations and cross-mediatechniques, to create rhythms, melodies and harmonic chords to stimulatebody-voice connections, positive emotions and structures for actions.},
address = {Daejeon, Republic of Korea},
author = {Andersson, Anders-Petter and Cappelen, Birgitta},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Empowerment,Music {\&} Health,Music Therapy,Resource-Oriented,Tangible Interaction,Vocal Interaction,Voice},
month = {may},
pages = {406--412},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Designing Empowering Vocal and Tangible Interaction}},
url = {http://nime.org/proceedings/2013/nime2013{\_}210.pdf},
year = {2013}
}
@inproceedings{Baldan:2013,
abstract = {This paper presents an audio-based tennis simulation game for mobile devices, which uses motion input and non-verbal audio feedback as exclusive means of interaction. Players have to listen carefully to the provided auditory clues, like racquet hits and ball bounces, rhythmically synchronizing their movements in order to keep the ball into play. The device can be swung freely and act as a full-fledged motionbased controller, as the game does not rely at all on visual feedback and the device display can thus be ignored. The game aims to be entertaining but also effective for educational purposes, such as ear training or improvement of the sense of timing, and enjoyable both by visually-impaired and sighted users.},
address = {Daejeon, Republic of Korea},
author = {Baldan, Stefano and de G{\"{o}}tzen, Amalia and Serafin, Stefania},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Audio game,mobile devices,motion-based,rhythmic interaction,sonic interaction design},
month = {may},
pages = {200--201},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Sonic Tennis: a rhythmic interaction game for mobile devices}},
url = {http://nime.org/proceedings/2013/nime2013{\_}288.pdf},
year = {2013}
}
@inproceedings{Christopher:2013,
abstract = {This paper describes Kontrol, a new hand interface that extends the intuitivecontrol of electronic music to traditional instrumentalist and dancers. Thegoal of the authors has been to provide users with a device that is capable ofdetecting the highly intricate and expressive gestures of the master performer,in order for that information to be interpreted and used for control ofelectronic music. This paper discusses related devices, the architecture ofKontrol, it's potential as a gesture recognition device, and severalperformance applications.},
address = {Daejeon, Republic of Korea},
author = {Christopher, Kameron and He, Jingyin and Kapur, Raakhi and Kapur, Ajay},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Hand controller,Wekinator,computational ethnomusicology,conducting interface,dance interface,wearable sensors},
month = {may},
pages = {267--270},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Kontrol: Hand Gesture Recognition for Music and Dance Interaction}},
url = {http://nime.org/proceedings/2013/nime2013{\_}164.pdf},
year = {2013}
}
@inproceedings{Trento:2013,
abstract = {This paper describes the development of a prototype of a sonic toy forpre-scholar kids. The device, which is a mod- ified version of a footballratchet, is based on the spinning gesture and it allows to experience fourdifferent types of auditory feedback. These algorithms let a kid play withmusic rhythm, generate a continuous sound feedback and control the pitch of apiece of music. An evaluation test of the device has been performed withfourteen kids in a kindergarten. Results and observations showed that kidspreferred the algorithms based on the exploration of the music rhythm and onpitch shifting.},
address = {Daejeon, Republic of Korea},
author = {Trento, Stefano and Serafin, Stefania},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Sonic toy,auditory feedback.,children},
month = {may},
pages = {456--459},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Flag beat: a novel interface for rhythmic musical expression for kids}},
url = {http://nime.org/proceedings/2013/nime2013{\_}295.pdf},
year = {2013}
}
@inproceedings{Fried:2013,
abstract = {We present a method for automatic feature extraction and cross-modal mappingusing deep learning. Our system uses stacked autoencoders to learn a layeredfeature representation of the data. Feature vectors from two (or more)different domains are mapped to each other, effectively creating a cross-modalmapping. Our system can either run fully unsupervised, or it can use high-levellabeling to fine-tune the mapping according a user's needs. We show severalapplications for our method, mapping sound to or from images or gestures. Weevaluate system performance both in standalone inference tasks and incross-modal mappings.},
address = {Daejeon, Republic of Korea},
author = {Fried, Ohad and Fiebrink, Rebecca},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Deep learning,feature learning,gestural control,mapping},
month = {may},
pages = {531--534},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Cross-modal Sound Mapping Using Deep Learning}},
url = {http://nime.org/proceedings/2013/nime2013{\_}111.pdf},
year = {2013}
}
@inproceedings{Barbosa:2013,
abstract = {This paper presents an innovative digital musical instrument, the Illusio, based on an augmented multi-touch interface that combines a traditional multi-touch surface and a device similar to a guitar pedal. Illusio allows users to perform by drawing and by associating the sketches with live loops. These loops are manipulated based on a concept called hierarchical live looping, which extends traditional live looping through the use of a musical tree, in which any music operation applied to a given node affects all its children nodes. Finally, we evaluate the instrument considering the performer and the audience, which are two of the most important stakeholders involved in the use, conception, and perception of a musical device. The results achieved are encouraging and led to useful insights about how to improve instrument features, performance and usability.},
address = {Daejeon, Republic of Korea},
author = {Barbosa, Jer{\^{o}}nimo and Calegario, Filipe and Teichrieb, Ver{\^{o}}nica and Ramalho, Geber and Cabral, Giordano},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Digital musical instruments,augmented multi-touch,evaluation methodology,hierarchical live looping,interaction techniques},
month = {may},
pages = {499--502},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{A Drawing-Based Digital Music Instrument}},
url = {http://nime.org/proceedings/2013/nime2013{\_}220.pdf},
year = {2013}
}
@inproceedings{Batula:2013,
abstract = {We present a system which allows an adult-sized humanoid to determine whetheror not it is correctly playing a pitched percussive instrument to produce adesired sound. As hu- man musicians utilize sensory feedback to determine ifthey are successfully using their instruments to generate certain pitches,robot performers should be capable of the same feat. We present a noteclassification algorithm that uses auditory and haptic feedback to decide if anote was well- or poorly-struck. This system is demonstrated using Hubo, anadult-sized humanoid, which has been enabled to actu- ate pitched pipes usingmallets. We show that, with this system, Hubo is able to determine whether ornot a note was played correctly.},
address = {Daejeon, Republic of Korea},
author = {Batula, Alyssa and Colacot, Manu and Grunberg, David and Kim, Youngmoo E},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Musical robots,auditory feedback,haptic feedback,humanoids},
month = {may},
pages = {295--300},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Using Audio and Haptic Feedback to Improve Pitched Percussive Instrument Performance in Humanoids}},
url = {http://nime.org/proceedings/2013/nime2013{\_}235.pdf},
year = {2013}
}
@inproceedings{Park:2013b,
abstract = {SSN (Sound Surfing Network) is a performance system that provides a new musicalexperience by incorporating mobile phone-based spatial sound control tocollaborative music performance. SSN enables both the performer and theaudience to manipulate the spatial distribution of sound using the smartphonesof the audience as distributed speaker system. Proposing a new perspective tothe social aspect music appreciation, SSN will provide a new possibility tomobile music performances in the context of interactive audience collaborationas well as sound spatialization.},
address = {Daejeon, Republic of Korea},
author = {Park, Sae Byul and Ban, Seonghoon and Hong, Dae Ryong and Yeo, Woon Seung},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Mobile music,audience participation,digital performance,smartphone,spatial sound control},
month = {may},
pages = {111--114},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Sound Surfing Network (SSN): Mobile Phone-based Sound Spatialization with Audience Collaboration}},
url = {http://nime.org/proceedings/2013/nime2013{\_}305.pdf},
year = {2013}
}
@inproceedings{Oh:2013,
abstract = {"Significant progress in the domains of speech- and singing-synthesis hasenhanced communicative potential of machines. To make computers more vocallyexpressive, however, we need a deeper understanding of how nonlinguistic socialsignals are patterned and perceived. In this paper, we focus on laughterexpressions: how a phrase of vocalized notes that we call ""laughter"" may bemodeled and performed to implicate nuanced meaning imbued in the acousticsignal. In designing our model, we emphasize (1) using high-level descriptorsas control parameters, (2) enabling real-time performable laughter, and (3)prioritizing expressiveness over realism. We present an interactive systemimplemented in ChucK that allows users to systematically play with the musicalingredients of laughter. A crowdsourced study on the perception of synthesizedlaughter showed that our model is capable of generating a range of laughtertypes, suggesting an exciting potential for expressive laughter synthesis."},
address = {Daejeon, Republic of Korea},
author = {Oh, Jieun and Wang, Ge},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {interface for musical expression,laughter,real-time controller,synthesis model,vocalization},
month = {may},
pages = {190--195},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{LOLOL: Laugh Out Loud On Laptop}},
url = {http://nime.org/proceedings/2013/nime2013{\_}86.pdf},
year = {2013}
}
@inproceedings{Johnson:2013,
abstract = {This paper presents a new technique for interface-driven diffusion performance.Details outlining the development of a new tabletop surface-based performanceinterface, named tactile.space, are discussed. User interface and amplitudepanning processes employed in the creation of tactile.space are focused upon,and are followed by a user study-based evaluation of the interface. It is hopedthat the techniques described in this paper afford performers and composers anenhanced level of creative expression in the diffusion performance practice.This paper introduces and evaluates tactile.space, a multi-touch performanceinterface for diffusion built on the BrickTable. It describes how tactile.spaceimplements Vector Base Amplitude Panning to achieve real- time sourcepositioning. The final section of this paper presents the findings of a userstudy that was conducted by those who performed with the interface, evaluatingthe interface as a performance tool with a focus on the increased creativeexpression the interface affords, and directly comparing it to the traditionaldiffusion user interface.},
address = {Daejeon, Republic of Korea},
author = {Johnson, Bridget and Kapur, Ajay},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {Multi touch,VBAP,diffusion,tabletop surface},
month = {may},
pages = {213--216},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{MULTI-TOUCH INTERFACES FOR PHANTOM SOURCE POSITIONING IN LIVE SOUND DIFFUSION}},
url = {http://nime.org/proceedings/2013/nime2013{\_}75.pdf},
year = {2013}
}
@inproceedings{El-Shimy:2013,
abstract = {For a number of years, musicians in different locations have been able toperform with one another over a network as though present on the same stage.However, rather than attempt to re-create an environment for Network MusicPerformance (NMP) that mimics co-present performance as closely as possible, wepropose focusing on providing musicians with additional controls that can helpincrease the level of interaction between them. To this end, we have developeda reactive environment for distributed performance that provides participantsdynamic, real-time control over several aspects of their performance, enablingthem to change volume levels and experience exaggerated stereo panning. Inaddition, our reactive environment reinforces a feeling of a ``shared space''between musicians. It differs most notably from standard ventures into thedesign of novel musical interfaces and installations in its reliance onuser-centric methodologies borrowed from the field of Human-ComputerInteraction (HCI). Not only does this research enable us to closely examine thecommunicative aspects of performance, it also allows us to explore newinterpretations of the network as a performance space. This paper describes themotivation and background behind our project, the work that has been undertakentowards its realization and the future steps that have yet to be explored.},
address = {Daejeon, Republic of Korea},
author = {El-Shimy, Dalia and Cooperstock, Jeremy R},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
month = {may},
pages = {158--163},
publisher = {Graduate School of Culture Technology, KAIST},
title = {{Reactive Environment for Network Music Performance}},
url = {http://nime.org/proceedings/2013/nime2013{\_}66.pdf},
year = {2013}
}
@inproceedings{lpardue:2014,
abstract = {This paper presents a multi-modal approach to musical instrument pitch tracking combining audio and position sensor data. Finger location on a violin fingerboard is measured using resistive sensors, allowing rapid detection of approximate pitch. The initial pitch estimate is then used to restrict the search space of an audio pitch tracking algorithm. Most audio-only pitch tracking algorithms face a fundamental tradeoff between accuracy and latency, with longer analysis windows producing better pitch estimates at the cost of noticeable lag in a live performance environment. Conversely, sensor-only strategies struggle to achieve the fine pitch accuracy a human listener would expect. By combining the two approaches, high accuracy and low latency can be simultaneously achieved.},
address = {London, United Kingdom},
author = {Pardue, Laurel S and Nian, Dongjuan and Harte, Christopher and McPherson, Andrew},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
pages = {54--59},
publisher = {Goldsmiths, University of London},
title = {{Low-Latency Audio Pitch Tracking: A Multi-Modal Sensor-Assisted Approach}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}336.pdf},
year = {2014}
}
@inproceedings{arenaud:2014,
abstract = {This paper provides an overview of a proposed demonstration of 3DinMotion, a system using real time motion capture of one or several subjects, which can be used in interactive audiovisual pieces and network performances. The skeleton of a subject is analyzed in real time and displayed as an abstract avatar as well as sonified based on mappings and rules to make the interplay experience lively and rewarding. A series of musical pieces have been composed for the interface following cueing strategies. In addition a second display, ``the prompter" guides the users through the piece. 3DinMotion has been developed from scratch and natively, leading to a system with a very low latency, making it suitable for real time music interactions. In addition, 3DinMotion is fully compatible with the OpenSoundControl (OSC) protocol, allowing expansion to commonly used musical and sound design applications.},
address = {London, United Kingdom},
author = {Renaud, Alain and Charbonnier, Caecilia and Chagu{\'{e}}, Sylvain},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
pages = {495--496},
publisher = {Goldsmiths, University of London},
title = {{3DinMotion A Mocap Based Interface for Real Time Visualisation and Sonification of Multi-User Interactions}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}423.pdf},
year = {2014}
}
@inproceedings{jbowers:2014,
abstract = {This paper outlines a concept of hybrid resonant assemblages, combinations of varied materials excited by sound transducers, feeding back to themselves via digital signal processing. We ground our concept as an extension of work by David Tudor, Nicolas Collins and Bowers and Archer [NIME 2005] and draw on a variety of critical perspectives in the social sciences and philosophy to explore such assemblages as an alternative to more familiar ideas of instruments and interfaces. We lay out a conceptual framework for the exploration of hybrid resonant assemblages and describe how we have approached implementing them. Our performance experience is presented and implications for work are discussed. In the light of our work, we urge a reconsideration of the implicit norms of performance which underlie much research in NIME. In particular, drawing on the philosophical work of Jean-Luc Nancy, we commend a wider notion of touch that also recognises the performative value of withholding contact.},
address = {London, United Kingdom},
author = {Bowers, John and Haas, Annika},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
pages = {7--12},
publisher = {Goldsmiths, University of London},
title = {{Hybrid Resonant Assemblages: Rethinking Instruments, Touch and Performance in New Interfaces for Musical Expression}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}438.pdf},
year = {2014}
}
@inproceedings{slee1:2014,
abstract = {This work introduces a way to distribute mobile applications using mobile ad-hoc network in the context of audience participation. The goal is to minimize user configuration so that the process is highly accessible for casual smartphone users. The prototype mobile applications utilize WiFiDirect and Service Discovery Protocol to distribute code. With the aid of these two technologies, the prototype system requires no infrastructure and minimum user configuration.},
address = {London, United Kingdom},
author = {Lee, Sang Won and Essl, Georg and Mao, Z Morley},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
pages = {533--536},
publisher = {Goldsmiths, University of London},
title = {{Distributing Mobile Music Applications for Audience Participation Using Mobile Ad-hoc Network ({\{}MANET{\}})}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}546.pdf},
year = {2014}
}
@inproceedings{fmorreale:2014,
abstract = {This paper presents MINUET, a framework for musical interface design grounded in the experience of the player. MINUET aims to provide new perspectives on the design of musical interfaces, referred to as a general term that comprises digital musical instruments and interactive installations. The ultimate purpose is to reduce the complexity of the design space emphasizing the experience of the player. MINUET is structured as a design process consisting of two stages: goal and specifications. The reliability of MINUET is tested through a systematic comparison with the related work and through a case study. To this end, we present the design and prototyping of Hexagon, a new musical interface with learning purposes.},
address = {London, United Kingdom},
author = {Morreale, Fabio and Angeli, Antonella De and O'Modhrain, Sile},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
pages = {467--472},
publisher = {Goldsmiths, University of London},
title = {{Musical Interface Design: An Experience-oriented Framework}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}437.pdf},
year = {2014}
}
@inproceedings{mrodrigues:2014,
abstract = {Digital Musical Instruments (DMIs) have difficulties establishing themselves after their creation. A huge number of DMIs is presented every year and few of them actually remain in use. Several causes could explain this reality, among them the lack of a proper instrumental technique, inadequacy of the traditional musical notation and the non-existence of a repertoire dedicated to the instrument. In this paper we present Entoa, the first written music for Intonaspacio, a DMI we designed in our research project. We propose some strategies for mapping data from sensors to sound processing, in order to accomplish an expressive performance. Entoa is divided in five different sections that corresponds to five movements. For each, a different mapping is designed, introducing subtle alterations that progressively explore the ensemble of features of the instrument. The performer is then required to adapt his repertoire of gestures along the piece. Indications are expressed through a gestural notation, where freedom is give to performer to control certain parameters at specific moments in the music.},
address = {London, United Kingdom},
author = {Mamedes, Clayton and Rodrigues, Mailis and Wanderley, Marcelo M and Manzolli, J{\^{o}}natas and Garcia, Denise H L and Ferreira-Lopes, Paulo},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
pages = {509--512},
publisher = {Goldsmiths, University of London},
title = {{Composing for DMIs Entoa, a Dedicate Piece for Intonaspacio}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}411.pdf},
year = {2014}
}
@inproceedings{mmainsbridge:2014,
abstract = {This paper explores the challenge of achieving nuanced control and physical engagement with gestural interfaces in performance. Performances with a prototype gestural performance system, Gestate, provide the basis for insights into the application of gestural systems in live contexts. These reflections stem from a performer's perspective, outlining the experience of prototyping and performing with augmented instruments that extend vocal or instrumental technique through ancillary gestures. Successful implementation of rapidly evolving gestural technologies in real-time performance calls for new approaches to performing and musicianship, centred around a growing understanding of the body's physical and creative potential. For musicians hoping to incorporate gestural control seamlessly into their performance practice a balance of technical mastery and kinaesthetic awareness is needed to adapt existing systems to their own purposes. Within non-tactile systems, visual feedback mechanisms can support this process by providing explicit visual cues that compensate for the absence of haptic or tangible feedback. Experience gained through prototyping and performance can yield a deeper understanding of the broader nature of gestural control and the way in which performers inhabit their own bodies.},
address = {London, United Kingdom},
author = {Mainsbridge, Mary and Beilharz, Kirsty},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
pages = {110--113},
publisher = {Goldsmiths, University of London},
title = {{Body As Instrument: Performing with Gestural Interfaces}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}393.pdf},
year = {2014}
}
@inproceedings{lpereira:2014,
abstract = {The Well--Sequenced Synthesizer is a series of sequencers that create music in dialog with the user. Through the sequencers' physical interfaces, users can control music theory-based generative algorithms. This series --a work-in-progress-is composed by three sequencers at this time. The first one, called The Counterpointer, takes a melody input from the user and responds by generating voices based on the rules of eighteenth--century counterpoint. The second one is based on a recent treatise on harmony and counterpoint by music theorist Dmitri Tymoczco: El Ordenador lets users explore a set of features of tonality by constraining randomly generated music according to one or more of them. El Ordenador gives the user less control than The Counterpointer, but more than La Mec{\'{a}}nica, the third sequencer in the series. La Mec{\'{a}}nica plays back the sequences generated by El Ordenador using a punch-card reading music box mechanism. It makes the digital patterns visible and tactile, and links them back to the physical world.},
address = {London, United Kingdom},
author = {Hors, Luisa Pereira},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
pages = {88--89},
publisher = {Goldsmiths, University of London},
title = {{The Well-Sequenced Synthesizer}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}2.pdf},
year = {2014}
}
@inproceedings{aandersson:2014,
abstract = {In this paper we explore how we compose sound for an interactive tangible and mobile interface; where the goal is to improve health and well-being for families with children with disabilities. We describe the composition process from how we decompose a linear beat-based and vocal sound material; recompose it with real-time audio synthesis and composition rules into interactive Scenes. Scenes that make it possible for the user to select, explore and recreate different ``sound worlds" with the tangible interface as an instrument; create and play with it as a friend; improvise and create; or relax with it as an ambient sounding furniture. We continue discussing a user story, how the Scenes are recreated by amateur users, persons with severe disabilities and family members; improvising with the mobile tangibles. We discuss composition techniques for mixing sound, tangible-physical and lighting elements in the Scenes. Based on observations we explore how a diverse audience in the family and at school can recreate and improvise their own sound experience and play together with others. We conclude by discussing the possible impact of our findings for the NIME-community; how the techniques of decomposing, recomposing and recreating sound, based on a relational perspective, could contribute to the design of new instruments for musical expression.},
address = {London, United Kingdom},
author = {Andersson, Anders-Petter and Cappelen, Birgitta and Olofsson, Fredrik},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
pages = {529--532},
publisher = {Goldsmiths, University of London},
title = {{Designing Sound for Recreation and Well-Being}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}572.pdf},
year = {2014}
}
@inproceedings{bjohnson:2014,
abstract = {This paper introduces recent developments in the Chronus series, a family of custom controllers that afford a performer gestural interaction with surround sound systems that can be easily integrated into their personal performance systems. The controllers are built with the goal of encouraging more electronic musicians to include the creation of dynamic pantophonic fields in performance. The paper focuses on technical advances of the Chronus 2.0 prototype that extend the interface to control both radial and angular positional data, and the controllers' ease of integration into electronic performance configurations, both for diffusion and for performance from the wider electronic music community.},
address = {London, United Kingdom},
author = {Johnson, Bridget and Norris, Michael and Kapur, Ajay},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
pages = {335--338},
publisher = {Goldsmiths, University of London},
title = {{The Development Of Physical Spatial Controllers}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}349.pdf},
year = {2014}
}
@inproceedings{sfavilla:2014,
abstract = {This paper presents new touch-screen collaborative music interaction for people with dementia. The authors argue that dementia technology has yet to focus on collaborative multi-user group musical interactions. The project aims to contribute to dementia care while addressing a significant gap in current literature. Two trials explore contrasting musical scenarios: the performance of abstract electronic music and the distributed performance of J.S. Bach's Goldberg Variations. Findings presented in this paper; demonstrate that people with dementia can successfully perform and engage in collaborative music performance activities with little or no scaffolded instruction. Further findings suggest that people with dementia can develop and retain musical performance skill over time. This paper proposes a number of guidelines and design solutions.},
address = {London, United Kingdom},
author = {Favilla, Stuart and Pedell, Sonja},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
pages = {35--39},
publisher = {Goldsmiths, University of London},
title = {{Touch Screen Collaborative Music: Designing NIME for Older People with Dementia}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}417.pdf},
year = {2014}
}
@inproceedings{sserafin:2014,
abstract = {In this paper we propose an empirical method to develop mapping strategies between a gestural based interface (the Gloves) and physically based sound synthesis models. An experiment was performed in order to investigate which kind of gestures listeners associate to synthesised sounds produced using physical models, corresponding to three categories of sound: sustained, iterative and impulsive. The results of the experiment show that listeners perform similar gestures when controlling sounds from the different categories. We used such gestures in order to create the mapping strategy between the Gloves and the physically based synthesis engine.},
address = {London, United Kingdom},
author = {Serafin, Stefania and Trento, Stefano and Grani, Francesco and Perner-Wilson, Hannah and Madgwick, Sebastian and Mitchell, Tom},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
pages = {521--524},
publisher = {Goldsmiths, University of London},
title = {{Controlling Physically Based Virtual Musical Instruments Using The Gloves}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}307.pdf},
year = {2014}
}
@inproceedings{pdahlstedt1:2014,
abstract = {The idea behind the YouHero was two-fold. First, to make an expressive instrument out of the computer game toy guitar controller from the famous game GuitarHero. With its limited amount of control parameters, this was a challenge. Second, through this instrument we wanted to provide an alternative to the view that you become a hero by perfect imitation of your idols. Instead, play yourself. You are the hero. In this paper, we describe the design of the instrument, including its novel mapping approach based on switched timbre vectors scaled by accellerometer data, unconventional sound engines and the sound and mapping editing features, including manual editing of individual vectors. The instrument is evaluated through its practical applications during the whole project, with workshops with teenagers, a set of state-funded commissions from professional composers, and the development of considerable skill by the key performers. We have also submitted a performance proposal for this project.},
address = {London, United Kingdom},
author = {Dahlstedt, Palle and Karlsson, Patrik and Widell, Katarina and Blomdahl, Tony},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
pages = {403--406},
publisher = {Goldsmiths, University of London},
title = {{YouHero Making an Expressive Concert Instrument from the GuitarHero Controller}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}513.pdf},
year = {2014}
}
@inproceedings{xxiao:2014,
abstract = {We present Andante, a representation of music as animated characters walking along the piano keyboard that appear to play the physical keys with each step. Based on a view of music pedagogy that emphasizes expressive, full-body communication early in the learning process, Andante promotes an understanding of the music rooted in the body, taking advantage of walking as one of the most fundamental human rhythms. We describe three example visualizations on a preliminary prototype as well as applications extending our examples for practice feedback, improvisation and composition. Through our project, we reflect on some high level considerations for the NIME community.},
address = {London, United Kingdom},
author = {Xiao, Xiao and Tome, Basheer and Ishii, Hiroshi},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
pages = {629--632},
publisher = {Goldsmiths, University of London},
title = {{Andante: Walking Figures on the Piano Keyboard to Visualize Musical Motion}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}467.pdf},
year = {2014}
}
@inproceedings{tkelkar:2014,
abstract = {We propose a new musical interface, TrAP (TRace-A-Phrase) for generating phrases of Hindustani Classical Music (HCM). In this system the user traces melodic phrases on a tablet interface to create phrases in a raga. We begin by analyzing tracings drawn by 28 participants, and train a classifier to categorize them into one of four melodic categories from the theory of Hindustani Music. Then we create a model based on note transitions from the raga grammar for the notes used in the singable octaves in HCM. Upon being given a new tracing, the system segments the tracing and computes a final phrase that best approximates the tracing.},
address = {London, United Kingdom},
author = {Roy, Udit and Kelkar, Tejaswinee and Indurkhya, Bipin},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
pages = {243--246},
publisher = {Goldsmiths, University of London},
title = {{TrAP: An Interactive System to Generate Valid Raga Phrases from Sound-Tracings}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}424.pdf},
year = {2014}
}
@inproceedings{avantklooster:2014,
abstract = {Emotion is a complex topic much studied in music and arguably equally central to the visual arts where this is usually referred to with the overarching label of aesthetics. This paper explores how music and the arts have incorporated the study of emotion. We then introduce the development of a live audio visual interface entitled In A State that detects emotion from live audio (in this case a piano performance) and generates visuals and electro acoustic music in response.},
address = {London, United Kingdom},
author = {{van 't Klooster}, Adinda and Collins, Nick},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
pages = {545--548},
publisher = {Goldsmiths, University of London},
title = {{In A State: Live Emotion Detection and Visualisation for Music Performance}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}469.pdf},
year = {2014}
}
@inproceedings{aberndt:2014,
abstract = {We present the digital musical instrument TouchNoise that is based on multitouch interaction with a particle system. It implements a novel interface concept for modulating noise spectra. Each particle represents a sine oscillator that moves through the two-dimensional frequency and stereo panning domain via Brownian motion. Its behavior can be affected by multitouch gestures allowing the shaping of the resulting sound in many different ways. Particles can be dragged, attracted, repelled, accentuated, and their autonomous behavior can be manipulated. In this paper we introduce the concepts behind this instrument, describe its implementation and discuss the sonic design space emerging from it.},
address = {London, United Kingdom},
author = {Berndt, Axel and Al-Kassab, Nadia and Dachselt, Raimund},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
pages = {323--326},
publisher = {Goldsmiths, University of London},
title = {{TouchNoise: A Particle-based Multitouch Noise Modulation Interface}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}325.pdf},
year = {2014}
}
@inproceedings{mkrzyzaniak:2014,
abstract = {This paper describes the implementation of a digital audio / visual feedback system for an extemporaneous dance performance. The system was designed to automatically synchronize aesthetically with the dancers. The performance was premiered at the Slingshot festival in Athens Georgia on March 9, 2013.},
address = {London, United Kingdom},
author = {Krzyzaniak, Michael and Akerly, Julie and Mosher, Matthew and Yildirim, Muharrem and Paine, Garth},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
pages = {303--306},
publisher = {Goldsmiths, University of London},
title = {{Separation: Short Range Repulsion. Implementation of an Automated Aesthetic Synchronization System for a Dance Performance.}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}279.pdf},
year = {2014}
}
@inproceedings{emorgan:2014,
abstract = {New technologies have led to the design of exciting interfaces for collaborative music making. However we still have very little understanding of the underlying affective and communicative processes which occur during such interactions. To address this issue, we carried out a pilot study where we collected continuous behavioural, physiological, and performance related measures from pairs of improvising drummers. This paper presents preliminary findings, which could be useful for the evaluation and design of user-centred collaborative interfaces for musical creativity and expression.},
address = {London, United Kingdom},
author = {Morgan, Evan and Gunes, Hatice and Bryan-Kinns, Nick},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
pages = {23--28},
publisher = {Goldsmiths, University of London},
title = {{Instrumenting the Interaction: Affective and Psychophysiological Features of Live Collaborative Musical Improvisation}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}353.pdf},
year = {2014}
}
@inproceedings{lfyfe:2014,
abstract = {The Nexus Data Exchange Format (NDEF) is an Open Sound Control (OSC) namespace specification designed to make connection and message management tasks easier for OSC-based networked performance systems. New extensions to the NDEF namespace improve both connection and message management between OSC client and server nodes. Connection management between nodes now features human-readable labels for connections and a new message exchange for pinging connections to determine their status. Message management now has improved namespace synchronization via a message count exchange and by the ability to add, remove, and replace messages on connected nodes.},
address = {London, United Kingdom},
author = {Fyfe, Lawrence and Tindale, Adam R and Carpendale, Sheelagh},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
pages = {343--346},
publisher = {Goldsmiths, University of London},
title = {{Extending the Nexus Data Exchange Format (NDEF) Specification}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}368.pdf},
year = {2014}
}
@inproceedings{asa:2014,
abstract = {The text exposes a perceptual approach to instrument design and composition, and it introduces an instrument that outputs acoustic sound, digital sound, and digital image. We explore disparities between human perception and digital analysis as creative material. Because the instrument repurposes software intended to create video games, we establish a distinction between the notion of ``flow" in music and gaming, questioning how it may substantiate in interaction design. Furthermore, we extrapolate from cognition/attention research to describe how the projected image creates a reactive stage scene without deviating attention from the music.},
address = {London, United Kingdom},
author = {S{\'{a}}, Adriana},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
pages = {331--334},
publisher = {Goldsmiths, University of London},
title = {{Repurposing Video Game Software for Musical Expression: A Perceptual Approach}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}343.pdf},
year = {2014}
}
@inproceedings{hdiao:2014,
abstract = {Sketching is a natural way for one person to convey their thoughts and intentions to another. With the recent rise of tablet-based computing, the use of sketching as a control and interaction paradigm is one that deserves exploration. In this paper we present an interactive sketch-based music composition and performance system called Drawchestra. The aim of the system is to give users an intuitive way to convey their musical ideas to a computer system with the minimum of technical training thus enabling them to focus on the creative tasks of composition and performance. The system provides the user with a canvas upon which they may create their own instruments by sketching shapes on the tablet screen. The system recognises a certain set of shapes which it treats as virtual instruments or effects. Once recognised, these virtual instruments can then be played by the user in real time. The size of a sketched instrument shape is used to control certain parameters of the sound so the user can build complex orchestras containing many different shapes of different sizes. The sketched shapes may also be moved and resized as desired making it possible to customise and edit the virtual orchestra as the user goes along. The system has been implemented in Python and user tests conducted using an iPad as the control surface. We report the results of the user study at the end of the paper before briefly discussing the outcome and outlining the next steps for the system design.},
address = {London, United Kingdom},
author = {Diao, Haojing and Zhou, Yanchao and Harte, Christopher and Bryan-Kinns, Nick},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
pages = {569--572},
publisher = {Goldsmiths, University of London},
title = {{Sketch-Based Musical Composition and Performance}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}517.pdf},
year = {2014}
}
@inproceedings{axambo:2014,
abstract = {Co-located tabletop tangible user interfaces (TUIs) for music performance are known for promoting multi-player collaboration with a shared interface, yet it is still unclear how to best support the awareness of the workspace in terms of understanding individual actions and the other group members actions, in parallel. In this paper, we investigate the effects of providing auditory feedback using ambisonics spatialisation, aimed at informing users about the location of the tangibles on the tabletop surface, with groups of mixed musical backgrounds. Participants were asked to improvise music on "SoundXY4: The Art of Noise", a tabletop system that includes sound samples inspired by Russolo's taxonomy of noises. We compared spatialisation vs. no-spatialisation conditions, and findings suggest that, when using spatialisation, there was a clearer workspace awareness, and a greater engagement in the musical activity as an immersive experience.},
address = {London, United Kingdom},
author = {Xamb{\'{o}}, Anna and Roma, Gerard and Laney, Robin and Dobbyn, Chris and Jord{\`{a}}, Sergi},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
pages = {40--45},
publisher = {Goldsmiths, University of London},
title = {{SoundXY4: Supporting Tabletop Collaboration and Awareness with Ambisonics Spatialisation}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}471.pdf},
year = {2014}
}
@inproceedings{croberts:2014,
abstract = {We describe research enabling the rapid creation of digital musical instruments and their publication to the Internet. This research comprises both high-level abstractions for making continuous mappings between audio, interactive, and graphical elements, as well as a centralized database for storing and accessing instruments. Published instruments run in most devices capable of running a modern web browser. Notation of instrument design is optimized for readability and expressivity.},
address = {London, United Kingdom},
author = {Roberts, Charles and Wright, Matthew and Kuchera-Morin, JoAnn and H{\"{o}}llerer, Tobias},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
pages = {239--242},
publisher = {Goldsmiths, University of London},
title = {{Rapid Creation and Publication of Digital Musical Instruments}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}373.pdf},
year = {2014}
}
@inproceedings{rcollecchia:2014,
abstract = {Sirens evoke images of alarm, public service, war, and forthcoming air raid. Outside of the music of Edgard Varese, sirens have rarely been framed as musical instruments. By connecting air hoses to spinning disks with evenly-spaced perforations, the siren timbre is translated musically. Polyphony gives our instrument an organ-like personality: keys are mapped to different frequencies and the pressure applied to them determines volume. The siren organ can produce a large range of sounds both timbrally and dynamically. In addition to a siren timbre, the instrument produces similar sounds to a harmonica. Portability, robustness, and electronic stability are all areas for improvement.},
address = {London, United Kingdom},
author = {Collecchia, Regina and Somen, Dan and McElroy, Kevin},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
pages = {391--394},
publisher = {Goldsmiths, University of London},
title = {{The Siren Organ}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}558.pdf},
year = {2014}
}
@inproceedings{ebertelli:2014,
abstract = {Through examining the decisions and sequences of presenting a multi-media instrument fabrication program to students, this paper seeks to uncover practical elements of best practice and possible improvements in science and music education. The Conductive Music program incorporates public engagement principles, open-source hardware, DIY ethos, contemporary composition techniques, and educational activities for creative and analytical thinking. These activities impart positive skills through multi-media content delivery for all learning types. The program is designed to test practices for engaging at-risk young people from urban areas in the construction and performance of new electronic instruments. The goal is to open up the world of electronic music performance to a new generation of young digital artists and to replace negative social behaviours with creative outlets for expression through technology and performance. This paper highlights the key elements designed to deliver the program's agenda and examines the ways in which these aims were realised or tested in the classroom.},
address = {London, United Kingdom},
author = {Robertson, Emily and Bertelli, Enrico},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
pages = {517--520},
publisher = {Goldsmiths, University of London},
title = {{Conductive Music: Teaching Innovative Interface Design and Composition Techniques with Open-Source Hardware}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}301.pdf},
year = {2014}
}
@inproceedings{jeaton:2014,
abstract = {The Space Between Us is a live performance piece for vocals, piano and live electronics using a Brain-Computer Music Interface system for emotional control of the score. The system not only aims to reflect emotional states but to direct and induce emotional states through the real-time generation of the score, highlighting the potential of direct neural-emotional manipulation in live performance. The EEG of the vocalist and one audience member is measured throughout the performance and the system generates a real-time score based on mapping the emotional features within the EEG. We measure the two emotional descriptors, valence and arousal, within EEG and map the two-dimensional correlate of averaged windows to musical phrases. These pre-composed phrases contain associated emotional content based on the KTH Performance Rules System (Director Musices). The piece is in three movements, the first two are led by the emotions of each subject respectively, whilst the third movement interpolates the combined response of the performer and audience member. The system not only aims to reflect the individuals' emotional states but also attempts to induce a shared emotional experience by drawing the two responses together. This work highlights the potential available in affecting neural-emotional manipulation within live performance and demonstrates a new approach to real-time, affectively-driven composition.},
address = {London, United Kingdom},
author = {Eaton, Joel and Jin, Weiwei and Miranda, Eduardo},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
pages = {593--596},
publisher = {Goldsmiths, University of London},
title = {{The Space Between Us. A Live Performance with Musical Score Generated via Emotional Levels Measured in {\{}EEG{\}} of One Performer and an Audience Member}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}418.pdf},
year = {2014}
}
@inproceedings{chutchins:2014,
abstract = {Government spying on internet traffic has seemingly become ubiquitous. Not to be left out, the private sector tracks our online footprint via our ISP or with a little help from facebook. Web services, such as advertisement servers and Google track our progress as we surf the net and click on links. The Mozilla plugin, Lightbeam (formerly Collusion), shows the user a visual map of every site a surfer sends data to. A interconnected web of advertisers and other (otherwise) invisible data-gatherers quickly builds during normal usage. We propose modifying this plugin so that as the graph builds, its state is broadcast visa OSC. Members of BiLE will receive and interpret those OSC messages in SuperCollider and PD. We will act as a translational object in a process of live-sonification. The collected data is the material with which we will develop a set of music tracks based on patterns we may discover. The findings of our data collection and the developed music will be presented in the form of an audiovisual live performance. Snippets of collected text and URLs will both form the basis of our audio interpretation, but also be projected on to a screen, so an audience can voyeuristically experience the actions taken on their behalf by governments and advertisers. After the concert, all of the scripts and documentation related to the data collection and sharing in the piece will be posted to github under a GPL license.},
address = {London, United Kingdom},
author = {Hutchins, Charles and Ballweg, Holger and Knotts, Shelly and Hummel, Jonas and Roberts, Antonio},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
pages = {497--498},
publisher = {Goldsmiths, University of London},
title = {{Soundbeam: A Platform for Sonyfing Web Tracking}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}447.pdf},
year = {2014}
}
@inproceedings{sknotts:2014,
abstract = {This paper reports the results of an online survey of 160 laptop ensembles and the relative democracy of their organisational and social structures. For the purposes of this research a laptop ensemble is defined as a performing group of three or more musicians for whom the laptop is the main sound generating source and who typically perform together in the same room. The concept of democracy (i.e. governance by members of the group) has been used as a starting point to assess firstly what types of organisational structures are currently used in laptop ensembles and secondarily to what extent laptop ensembles consider the implications of organisational and social structure on their musical output. To assess this I recorded a number of data points including ensemble size, whether the group has a director or conductor, use of homogenous vs. heterogenous hardware and software, whether they perform composed pieces or mainly improvise, the level of network interaction and whether or not the ensemble has an academic affiliation. The survey allowed me to define a scale of democracy in laptop ensembles and typical features of the most and least democratic groups. Some examples are given of democratic and autocratic activity in existing laptop ensembles. This work is part of a larger scale project investigating the effect of social structures on the musical output of laptop ensembles.},
address = {London, United Kingdom},
author = {Knotts, Shelly and Collins, Nick},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
pages = {191--194},
publisher = {Goldsmiths, University of London},
title = {{The Politics of Laptop Ensembles: A Survey of 160 Laptop Ensembles and their Organisational Structures}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}521.pdf},
year = {2014}
}
@inproceedings{rkleinberger:2014,
abstract = {Vocal Vibrations is a new project by the Opera of the Future group at the MIT Media Lab that seeks to engage the public in thoughtful singing and vocalizing, while exploring the relationship between human physiology and the resonant vibrations of the voice. This paper describes the motivations, the technical implementation, and the experience design of the Vocal Vibrations public installation. This installation consists of a space for reflective listening to a vocal composition (the Chapel) and an interactive space for personal vocal exploration (the Cocoon). In the interactive experience, the participant also experiences a tangible exteriorization of his voice by holding the ORB, a handheld device that translates his voice and singing into tactile vibrations. This installation encourages visitors to explore the physicality and expressivity of their voices in a rich musical context.},
address = {London, United Kingdom},
author = {Holbrow, Charles and Jessop, Elena and Kleinberger, R{\'{e}}becca},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
pages = {431--434},
publisher = {Goldsmiths, University of London},
title = {{Vocal Vibrations: A Multisensory Experience of the Voice}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}378.pdf},
year = {2014}
}
@inproceedings{tmurray-browne1:2014,
abstract = {The Cave of Sounds is an interactive sound installation made up of new musical instruments. Exploring what it means to create instruments together within the context of NIME and the maker scene, each instrument was created by an individual but with the aim of forming a part of this new ensemble over ten months, with the final installation debuting at the Barbican in London in August 2013. In this paper, we describe how ideas of prehistoric collective music making inspired and guided this participatory musical work, both in terms of how it was created and the audience experience of musical collaboration we aimed to create in the final installation. Following a detailed description of the installation itself, we reflect on the successes, lessons and future challenges of encouraging creative musical collaboration among members of an audience.},
address = {London, United Kingdom},
author = {Murray-Browne, Tim and Aversano, Dom and Garcia, Susanna and Hobbes, Wallace and Lopez, Daniel and Sendon, Tadeo and Tigas, Panagiotis and Ziemianin, Kacper and Chapman, Duncan},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
pages = {307--310},
publisher = {Goldsmiths, University of London},
title = {{The Cave of Sounds: An Interactive Installation Exploring How We Create Music Together}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}288.pdf},
year = {2014}
}
@inproceedings{kkeatch:2014,
abstract = {Sounds of Solitaire is a novel interface for musical expression based on an extended peg solitaire board as a generator of live musical composition. The classic puzzle game, for one person, is extended by mapping the moves of the game through a self contained system using Arduino and Raspberry Pi, triggering both analogue and digital sound. The solitaire board, as instrument, is presented as a wood and Perspex box with the hardware inside. Ball bearings function as both solitaire pegs and switches, while a purpose built solenoid controlled monochord and ball bearing run provide the analogue sound source, which is digitally manipulated in real-time, according to the sequences of game moves. The creative intention of Sounds of Solitaire is that the playful approach to participation in a musical experience, provided by the material for music making in real-time, demonstrates an integrated approach to concepts of composing, performing and listening.},
address = {London, United Kingdom},
author = {Keatch, Kirsty},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
pages = {102--105},
publisher = {Goldsmiths, University of London},
title = {{An Exploration of Peg Solitaire as a Compositional Tool}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}466.pdf},
year = {2014}
}
@inproceedings{gdublon:2015,
abstract = {The Electronic Fox Ears helmet is a listening device that changes its wearer's experience of hearing. A pair of head-mounted, independently articulated parabolic microphones and built-in bone conduction transducers allow the wearer to sharply direct their attention to faraway sound sources. Joysticks in each hand control the orientations of the microphones, which are mounted on servo gimbals for precise targeting. Paired with a mobile device, the helmet can function as a specialized, wearable field recording platform. Field recording and ambient sound have long been a part of electronic music; our device extends these practices by drawing on a tradition of wearable technologies and prosthetic art that blur the boundaries of human perception.},
address = {Baton Rouge, Louisiana, USA},
author = {Kleinberger, R{\'{e}}becca and Dublon, Gershon and Paradiso, Joseph A and Machover, Tod},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Berdahl, Edgar and Allison, Jesse},
month = {may},
pages = {30--31},
publisher = {Louisiana State University},
title = {{PHOX Ears: A Parabolic, Head-mounted, Orientable, eXtrasensory Listening Device}},
url = {http://www.nime.org/proceedings/2015/nime2015{\_}165.pdf},
year = {2015}
}
@inproceedings{hpurwins:2015,
abstract = {An interactive music instrument museum experience for children of 10-12 years is presented. Equipped with tablet devices, the children are sent on a treasure hunt. Based on given sound samples, the participants have to identify the right musical instrument (harpsichord, double bass, viola) out of an instrument collection. As the right instrument is located, a challenge of playing an application on the tablet is initiated. This application is an interactive digital representation of the found instrument, mimicking some of its key playing techniques, using a simplified scrolling on screen musical notation. The musical performance of the participant is graded on a point scale. After completion of the challenge, the participants' performances of the three instruments are played back simultaneously, constituting a composition. A qualitative evaluation of the application in a focus group interview with school children revealed that the children were more engaged when playing with the interactive application than when only watching a music video.},
address = {Baton Rouge, Louisiana, USA},
author = {J{\"{o}}rgensen, Mikkel and Knudsen, Aske and Wilmot, Thomas and Lund, Kasper and Serafin, Stefania and Purwins, Hendrik},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Berdahl, Edgar and Allison, Jesse},
month = {may},
pages = {36--37},
publisher = {Louisiana State University},
title = {{A Mobile Music Museum Experience for Children}},
url = {http://www.nime.org/proceedings/2015/nime2015{\_}267.pdf},
year = {2015}
}
@inproceedings{esheffield:2015,
abstract = {The Pneumatic Practice Pad is a commercially available 10'' practice pad that has been modified to allow for tension changes in a matter of seconds using a small electric air pump. In this paper, we examine the rebound characteristics of the Pneumatic Practice Pad at various pressure presets and compare them to a sample of acoustic drums. We also review subjective feedback from participants in a playing test.},
address = {Baton Rouge, Louisiana, USA},
author = {Sheffield, Eric and O'Modhrain, Sile and Gould, Michael and Gillespie, Brent},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Berdahl, Edgar and Allison, Jesse},
month = {may},
pages = {231--234},
publisher = {Louisiana State University},
title = {{The Pneumatic Practice Pad}},
url = {http://www.nime.org/proceedings/2015/nime2015{\_}286.pdf},
year = {2015}
}
@inproceedings{dverdonk:2015,
abstract = {In electronic music performance, a good relationship between what is visible and what is audible can contribute to a more succesful way of conveying thought or feeling. This connection can be enhanced by putting visible energy into an electronic interface or instrument. This paper discusses the advantages and implementations of visible excitation methods, and how these could reinforce the bridge between the performance of acoustic and electronic instruments concerning expressiveness.},
address = {Baton Rouge, Louisiana, USA},
author = {Verdonk, Dianne},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Berdahl, Edgar and Allison, Jesse},
month = {may},
pages = {42--43},
publisher = {Louisiana State University},
title = {{Visible Excitation Methods: Energy and Expressiveness in Electronic Music Performance}},
url = {http://www.nime.org/proceedings/2015/nime2015{\_}273.pdf},
year = {2015}
}
@inproceedings{cbrown:2015,
abstract = {Lambeosaurine hadrosaurs are duck-billed dinosaurs known for their large head crests, which researchers hypothesize were resonators for vocal calls. This paper describes the motivation and process of iteratively designing a musical instrument and interactive sound installation based on imagining the sounds of this extinct dinosaur. We used scientific research as a starting point to create a means of sound production and resonator, using a 3D model obtained from Computed Topology (CT) scans of a Corythosaurus skull and an endocast of its crest and nasal passages. Users give voice to the dinosaur by blowing into a mouthpiece, exciting a larynx mechanism and resonating the sound through the hadrosaur's full-scale nasal cavities and skull. This action allows an embodied glimpse into an ancient past. Users know the dinosaur through the controlled exhalation of their breath, how the compression of the lungs leads to a whisper or a roar.},
address = {Baton Rouge, Louisiana, USA},
author = {Brown, Courtney and Razzaque, Sharif and Paine, Garth},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Berdahl, Edgar and Allison, Jesse},
month = {may},
pages = {5--10},
publisher = {Louisiana State University},
title = {{Rawr! A Study in Sonic Skulls: Embodied Natural History}},
url = {http://www.nime.org/proceedings/2015/nime2015{\_}325.pdf},
year = {2015}
}
@inproceedings{roda:2015,
abstract = {The Internet holds a lot of potential as a music listening, collaboration, and performance space. It has become commonplace to stream music and video of musical performance over the web. However, the goal of playing rhythmically synchronized music over long distances has remained elusive due to the latency inherent in networked communication. The farther apart two artists are from one another, the greater the delay. Furthermore, latency times can change abruptly with no warning. In this paper, we demonstrate that it is possible to create a distributed, synchronized musical instrument that allows performers to play together over long distances, despite latency. We describe one such instrument, MalLo, which combats latency by predicting a musician's action before it is completed. MalLo sends information about a predicted musical note over the Internet before it is played, and synthesizes this note at a collaborator's location at nearly the same moment it is played by the performer. MalLo also protects against latency spikes by sending the prediction data across multiple network paths, with the intention of routing around latency.},
address = {Baton Rouge, Louisiana, USA},
author = {Jin, Zeyu and Oda, Reid and Finkelstein, Adam and Fiebrink, Rebecca},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Berdahl, Edgar and Allison, Jesse},
month = {may},
pages = {293--298},
publisher = {Louisiana State University},
title = {{MalLo: A Distributed Synchronized Musical Instrument Designed For Internet Performance}},
url = {http://www.nime.org/proceedings/2015/nime2015{\_}223.pdf},
year = {2015}
}
@inproceedings{lhayes:2015,
abstract = {Live music making can be understood as an enactive process, whereby musical experiences are created through human action. This suggests that musical worlds coevolve with their agents through repeated sensorimotor interactions with the environment (where the music is being created), and at the same time cannot be separated from their sociocultural contexts. This paper investigates this claim by exploring ways in which technology, physiology, and context are bound up within two different musical scenarios: live electronic musical performance; and person-centred arts applications of NIMEs. In this paper I outline an ethnographic and phenomenological enquiry into my experiences as both a performer of live electronic and electro-instrumental music, as well as my extensive background in working with new technologies in various therapeutic and person-centred artistic situations. This is in order to explore the sociocultural and technological contexts in which these activities take place. I propose that by understanding creative musical participation as a highly contextualised practice, we may discover that the greatest impact of rapidly developing technological resources is their ability to afford richly diverse, personalised, and embodied forms of music making. I argue that this is applicable over a wide range of musical communities.},
address = {Baton Rouge, Louisiana, USA},
author = {Hayes, Lauren},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Berdahl, Edgar and Allison, Jesse},
month = {may},
pages = {299--302},
publisher = {Louisiana State University},
title = {{Enacting Musical Worlds: Common Approaches to using NIMEs within both Performance and Person-Centred Arts Practices}},
url = {http://www.nime.org/proceedings/2015/nime2015{\_}227.pdf},
year = {2015}
}
@inproceedings{ndalessandro:2015,
abstract = {In this paper, we describe the prototyping of two musical interfaces that use the LeapMotion camera in conjunction with two different touch surfaces: a Wacom tablet and a transparent PVC sheet. In the Wacom use case, the camera is between the hand and the surface. In the PVC use case, the camera is under the transparent sheet and tracks the hand through it. The aim of this research is to explore hovering motion surrounding the touch interaction on the surface and include properties of such motion in the musical interaction. We present our unifying software, called AirPiano, that discretises the 3D space into 'keys' and proposes several mapping strategies with the available dimensions. These control dimensions are mapped onto a modified HandSketch sound engine that achieves multitimbral pitch-synchronous point cloud granulation.},
address = {Baton Rouge, Louisiana, USA},
author = {D'Alessandro, Nicolas and Tilmanne, Jo{\"{e}}lle and Moreau, Ambroise and Puleo, Antonin},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Berdahl, Edgar and Allison, Jesse},
month = {may},
pages = {255--258},
publisher = {Louisiana State University},
title = {{AirPiano: A Multi-Touch Keyboard with Hovering Control}},
url = {http://www.nime.org/proceedings/2015/nime2015{\_}261.pdf},
year = {2015}
}
@inproceedings{hlimerick:2015,
abstract = {Liveness is a well-known problem with Digital Musical Instruments (DMIs). When used in performances, DMIs provide less visual information than acoustic instruments, preventing the audience from understanding how the musicians influence the music. In this paper, we look at this issue through the lens of causality. More specifically, we investigate the attribution of causality by an external observer to a performer, relying on the theory of apparent mental causation. We suggest that the perceived causality between a performer's gestures and the musical result is central to liveness. We present a framework for assessing attributed causality and agency to a performer, based on a psychological theory which suggests three criteria for inferred causality. These criteria then provide the basis of an experimental study investigating the effect of visual augmentations on audience's inferred causality. The results provide insights on how the visual component of performances with DMIs impacts the audience's causal inferences about the performer. In particular we show that visual augmentations help highlight the influence of the musician when parts of the music are automated, and help clarify complex mappings between gestures and sounds. Finally we discuss the potential wider implications for assessing liveness in the design of new musical interfaces.},
address = {Baton Rouge, Louisiana, USA},
author = {Berthaut, Florent and Coyle, David and Moore, James and Limerick, Hannah},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Berdahl, Edgar and Allison, Jesse},
month = {may},
pages = {382--386},
publisher = {Louisiana State University},
title = {{Liveness Through the Lens of Agency and Causality}},
url = {http://www.nime.org/proceedings/2015/nime2015{\_}272.pdf},
year = {2015}
}
@inproceedings{ajenseniusb:2015,
abstract = {The MYO armband from Thalmic Labs is a complete and wireless motion and muscle sensing platform. This paper evaluates the armband's sensors and its potential for NIME applications. This is followed by a presentation of the prototype instrument MuMYO. We conclude that, despite some shortcomings, the armband has potential of becoming a new ``standard'' controller in the NIME community.},
address = {Baton Rouge, Louisiana, USA},
author = {Nymoen, Kristian and Haugen, Mari Romarheim and Jensenius, Alexander Refsum},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Berdahl, Edgar and Allison, Jesse},
month = {may},
pages = {215--218},
publisher = {Louisiana State University},
title = {{MuMYO - Evaluating and Exploring the MYO Armband for Musical Interaction}},
url = {http://www.nime.org/proceedings/2015/nime2015{\_}179.pdf},
year = {2015}
}
@inproceedings{kyerkes:2015,
abstract = {We document results from exploring ensemble feedback in loosely-structured electroacoustic improvisations. A conceptual justification for the explorations is provided, in addition to discussion of tools and methodologies. Physical configurations of intra-ensemble feedback networks are documented, along with qualitative analysis of their effectiveness.},
address = {Baton Rouge, Louisiana, USA},
author = {Rosli, Muhammad Hafiz Wan and Yerkes, Karl and Wright, Matthew and Wood, Timothy and Wolfe, Hannah and Roberts, Charles and Haron, Anis and Estrada, Fernando Rincon},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Berdahl, Edgar and Allison, Jesse},
month = {may},
pages = {144--149},
publisher = {Louisiana State University},
title = {{Ensemble Feedback Instruments}},
url = {http://www.nime.org/proceedings/2015/nime2015{\_}329.pdf},
year = {2015}
}
@inproceedings{pbennett:2015,
abstract = {Resonant Bits proposes giving digital information resonant dynamic properties, requiring skill and concerted effort for interaction. This paper applies resonant interaction to musical control, exploring musical instruments that are controlled through both purposeful and subconscious resonance. We detail three exploratory prototypes, the first two illustrating the use of resonant gestures and the third focusing on the detection and use of the ideomotor (subconscious micro-movement) effect.},
address = {Baton Rouge, Louisiana, USA},
author = {Bennett, Peter and Knibbe, Jarrod and Berthaut, Florent and Cater, Kirsten},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Berdahl, Edgar and Allison, Jesse},
month = {may},
pages = {176--177},
publisher = {Louisiana State University},
title = {{Resonant Bits: Controlling Digital Musical Instruments with Resonance and the Ideomotor Effect}},
url = {http://www.nime.org/proceedings/2015/nime2015{\_}235.pdf},
year = {2015}
}
@inproceedings{apon:2015,
abstract = {This paper describes the motivation and process of developing a musical instrument for an unborn child. Well established research shows a fetus in the womb can respond to and benefit from stimuli from the outside world. A musical instrument designed for this unique context can leverage the power of this interaction. Two prototypes were constructed and tested during separate pregnancies and the experiences are presented, and the limitation of the sensor technology identified. We discuss our discoveries about design considerations and challenges for such an instrument, and project thought-provoking questions that arise from its potential applications.},
address = {Baton Rouge, Louisiana, USA},
author = {Pon, Aura and Wang, Johnty and Radford, Laurie and Carpendale, Sheelagh},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Berdahl, Edgar and Allison, Jesse},
month = {may},
pages = {87--90},
publisher = {Louisiana State University},
title = {{Womba: A Musical Instrument for an Unborn Child}},
url = {http://www.nime.org/proceedings/2015/nime2015{\_}304.pdf},
year = {2015}
}
@inproceedings{swaite:2015,
abstract = {This paper discusses the use of typed text as a real-time input for interactive performance systems. A brief review of the literature discusses text-based generative systems, links between typing and playing percussion instruments and the use of typing gestures in contemporary performance practice. The paper then documents the author's audio-visual system that is driven by the typing of text/lyrics in real-time. It is argued that the system promotes the sensation of liveness through clear, perceptible links between the performer's gestures, the system's audio outputs and the its visual outputs. The system also provides a novel approach to the use of generative techniques in the composition and live performance of songs. Future developments would include the use of dynamic text effects linked to sound generation and greater interaction between human performer and the visuals.},
address = {Baton Rouge, Louisiana, USA},
author = {Waite, Si},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Berdahl, Edgar and Allison, Jesse},
month = {may},
pages = {168--169},
publisher = {Louisiana State University},
title = {{Reimagining the Computer Keyboard as a Musical Interface}},
url = {http://www.nime.org/proceedings/2015/nime2015{\_}193.pdf},
year = {2015}
}
@inproceedings{klin:2015,
abstract = {Many new melodic instruments use a touch sensitive surface with notes arranged in a two-dimensional grid. Most of these arrange notes in chromatic half-steps along the horizontal axis and in intervals of fourths along the vertical axis. Although many alternatives exist, this arrangement, which resembles that of a bass guitar, is quickly becoming the de facto standard. In this study we present experimental evidence that grid based instruments are significantly easier to play when we tune adjacent rows in Major thirds rather than fourths. We have developed a grid-based instrument as an iPad app that has sold 8,000 units since 2012. To test our proposed alternative tuning, we taught a group twenty new users to play basic chords on our app, using both the standard tuning and our proposed alternative. Our results show that the Major thirds tuning is much easier to learn, even for users that have previous experience playing guitar.},
address = {Baton Rouge, Louisiana, USA},
author = {Anderson, Hans and Lin, Kin Wah Edward and Agus, Natalie and Lui, Simon},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Berdahl, Edgar and Allison, Jesse},
month = {may},
pages = {365--368},
publisher = {Louisiana State University},
title = {{Major Thirds: A Better Way to Tune Your iPad}},
url = {http://www.nime.org/proceedings/2015/nime2015{\_}157.pdf},
year = {2015}
}
@inproceedings{arau:2015,
abstract = {We introduce the Peripipe, a tangible remote control for a music player that comes in the shape of a wooden tobacco pipe. The design is based on breath control, using sips and puffs as control commands. An atmospheric pressure sen- sor in the Peripipe senses changes in the air pressure. Based on these changes, the pipe determines when the user per- forms a puff, double-puff, sip, double-sip or a long puff or long sip action, and wirelessly sends commands to a smart- phone running the music player. Additionally, the Perip- ipe provides fumeovisual feedback, using color-illuminated smoke to display the system status. With the form fac- tor, the materials used, the interaction through breath, and the ephemeral feedback we aim to emphasize the emotional component of listening to music that, in our eyes, is not very well reflected in traditional remote controls.},
address = {Baton Rouge, Louisiana, USA},
author = {Feldt, Tommy and Freilich, Sarah and Mendonsa, Shaun and Molin, Daniel and Rau, Andreas},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Berdahl, Edgar and Allison, Jesse},
month = {may},
pages = {34--35},
publisher = {Louisiana State University},
title = {{Puff, Puff, Play: A Sip-And-Puff Remote Control for Music Playback}},
url = {http://www.nime.org/proceedings/2015/nime2015{\_}260.pdf},
year = {2015}
}
@inproceedings{croberts:2015,
abstract = {We describe research extending the interactive affordances of textual code fragments in creative coding environments. In particular we examine the potential of source code both to display the state of running processes and also to alter state using means other than traditional text editing. In contrast to previous research that has focused on the inclusion of additional interactive widgets inside or alongside text editors, our research adds a parsing stage to the runtime evaluation of code fragments and imparts additional interactive capabilities on the source code itself. After implementing various techniques in the creative coding environment Gibber, we evaluate our research through a survey on the various methods of visual feedback provided by our research. In addition to results quantifying preferences for certain techniques over others, we found near unanimous support among survey respondents for including similar techniques in other live coding environments.},
address = {Baton Rouge, Louisiana, USA},
author = {Roberts, Charles and Wright, Matthew and Kuchera-Morin, JoAnn},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Berdahl, Edgar and Allison, Jesse},
month = {may},
pages = {126--131},
publisher = {Louisiana State University},
title = {{Beyond Editing: Extended Interaction with Textual Code Fragments}},
url = {http://www.nime.org/proceedings/2015/nime2015{\_}310.pdf},
year = {2015}
}
@inproceedings{rbhandari:2015,
abstract = {Biofeedback tools generally use visualizations to display physiological information to the user. As such, these tools are incompatible with visually demanding tasks such as driving. While auditory or haptic biofeedback may be used in these cases, the additional sensory channels can increase workload or act as a nuisance to the user. A number of studies, however, have shown that music can improve mood and concentration, while also reduce aggression and boredom. Here, we propose an intervention that combines the benefits of biofeedback and music to help users regulate their stress response while performing a visual task (driving a car simulator). Our approach encourages slow breathing by adjusting the quality of the music in response to the user's breathing rate. We evaluate the intervention on a 2{\$}\backslashtimes{\$}2 design with music and auditory biofeedback as independent variables. Our results indicate that our music-biofeedback intervention leads to lower arousal (reduced electrodermal activity and increased heart rate variability) than music alone, auditory biofeedback alone and a control condition.},
address = {Baton Rouge, Louisiana, USA},
author = {Bhandari, Rhushabh and Parnandi, Avinash and Shipp, Eva and Ahmed, Beena and Gutierrez-Osuna, Ricardo},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Berdahl, Edgar and Allison, Jesse},
month = {may},
pages = {78--82},
publisher = {Louisiana State University},
title = {{Music-based respiratory biofeedback in visually-demanding tasks}},
url = {http://www.nime.org/proceedings/2015/nime2015{\_}149.pdf},
year = {2015}
}
@inproceedings{Leitman2016,
abstract = {Music Maker is a free online resource that provides files for 3D printing woodwind and brass mouthpieces and tutorials for using those mouthpieces to learn about acoustics and music. The mouthpieces are designed to fit into standard plumbing and automobile parts that can be easily purchased at home improvement and automotive stores. The result is a musical tool that can be used as simply as a set of building blocks to bridge the gap between our increasingly digital world of fabrication and the real-world materials that make up our daily lives. An increasing number of schools, libraries and community groups are purchasing 3D printers but many are still struggling to create engaging and relevant curriculum that ties into academic subjects. Making new musical instruments is a fantastic way to learn about acoustics, physics and mathematics.},
address = {Brisbane, Australia},
author = {Leitman, Sasha and Granzow, John},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
isbn = {978-1-925455-13-7},
pages = {118--121},
publisher = {Queensland Conservatorium Griffith University},
series = {2220-4806},
title = {{Music Maker: 3d Printing and Acoustics Curriculum}},
url = {http://www.nime.org/proceedings/2016/nime2016{\_}paper0024.pdf},
volume = {16},
year = {2016}
}
@inproceedings{Snicode248derberg2016,
abstract = {This paper explores the possibility of breaking the barrier between deaf and hearing people when it comes to the subject of making music. Suggestions on how deaf and hearing people can collaborate in creating music together, are presented. The conducted research will focus on deaf people with a general interest in music as well as hearing musicians as target groups. Through reviewing different related research areas, it is found that visualization of sound along with a haptic feedback can help deaf people interpret and interact with music. With this in mind, three variations of a collaborative user interface are presented, in which deaf and hearing people are meant to collaborate in creating short beats and melody sequences. Through evaluating the three prototypes, with two deaf people and two hearing musicians, it is found that the target groups can collaborate to some extent in creating beats. However, in order for the target groups to create melodic sequences together in a satisfactory manner, more detailed visualization and distributed haptic output is necessary, mostly due to the fact that the deaf test participants struggle in distinguishing between higher pitch and timbre.},
address = {Brisbane, Australia},
author = {S{\"{o}}derberg, Ene Alicia and Odgaard, Rasmus Emil and Bitsch, Sarah and H{\"{o}}eg-Jensen, Oliver and Christensen, Nikolaj Schildt and Poulsen, S{\"{o}}ren Dahl and Gelineck, Steven},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
isbn = {978-1-925455-13-7},
pages = {321--326},
publisher = {Queensland Conservatorium Griffith University},
series = {2220-4806},
title = {{Music Aid---Towards a Collaborative Experience for Deaf and Hearing People in Creating Music}},
url = {http://www.nime.org/proceedings/2016/nime2016{\_}paper0063.pdf},
volume = {16},
year = {2016}
}
@inproceedings{Olowe2016,
abstract = {We propose residUUm, an audiovisual performance tool that uses sonification to orchestrate a particle system of shapes, as an attempt to build an audiovisual user interface in which all the actions of a performer on a laptop are intended to be explicitly interpreted by the audience. We propose two approaches to performing with residUUm and discuss the methods utilized to fulfill the promise of audience-visible interaction: mapping and performance strategies applied to express audiovisual interactions with multilayered sound-image relationships. The system received positive feedback from 34 audience participants on aspects such as aesthetics and audiovisual integration, and we identified further design challenges around performance clarity and strategy. We discuss residUUm's development objectives, modes of interaction and the impact of an audience-visible interface on the performer and observer.},
address = {Brisbane, Australia},
author = {Olowe, Ireti and Moro, Giulio and Barthet, Mathieu},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
isbn = {978-1-925455-13-7},
pages = {271--276},
publisher = {Queensland Conservatorium Griffith University},
series = {2220-4806},
title = {{residUUm: user mapping and performance strategies for multilayered live audiovisual generation}},
url = {http://www.nime.org/proceedings/2016/nime2016{\_}paper0053.pdf},
volume = {16},
year = {2016}
}
@inproceedings{Hnicode233on-Morissette2016,
abstract = {The author's artistic practice as a composer and performer is transdisciplinary. The body as a vector associated with sound, gesture, video, physical space, and technological space, constitute the six founding elements. They give rise to works between music and dance, between musical theater and multimedia works leading to a new hybrid performative practice. These works are realized using a motion capture system by computer vision, SICMAP (Syst{\'{e}}me Interactif de Captation du Mouvement en Art Performatif --- Interactive Motion Capture System For The Performative Arts). In this paper, the author situates her artistic practice founded by the three pillars of transdisciplinary research methodology. The path taken by the performer-creator, leading to the conception of the SICMAP, is then explained through a reflection on the `dream instrument'. Followed by a technical description, the SICMAP is contextualized using theoretical models: the instrumental continuum and energy continuum, the `dream instrument' and the typology of the instrumental gesture. Initiated by the SICMAP, these are then applied to a new paradigm the gesture-sound space and subsequently put into practice through the creation of the work From Infinity To Within.},
address = {Brisbane, Australia},
author = {H{\'{e}}on-Morissette, Barah},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
isbn = {978-1-925455-13-7},
pages = {253--258},
publisher = {Queensland Conservatorium Griffith University},
series = {2220-4806},
title = {{Transdisciplinary Methodology: from Theory to the Stage, Creation for the {\{}SIC{\}}MAP}},
url = {http://www.nime.org/proceedings/2016/nime2016{\_}paper0050.pdf},
volume = {16},
year = {2016}
}
@inproceedings{Banas2016,
abstract = {An auditory game has been developed as a part of research in Wavefield Synthesis. In order to design and implement this game, a number of technologies have been incorporated in the development process. By pairing motion capture with a WiiMote new dimension of movement input was achieved. We present an evaluation study where the game was assessed.},
address = {Brisbane, Australia},
author = {Banas, Jan and Paisa, Razvan and Vogiatzoglou, Iakovos and Grani, Francesco and Serafin, Stefania},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
isbn = {978-1-925455-13-7},
pages = {188--193},
publisher = {Queensland Conservatorium Griffith University},
series = {2220-4806},
title = {{Design and evaluation of a gesture driven wave field synthesis auditory game}},
url = {http://www.nime.org/proceedings/2016/nime2016{\_}paper0039.pdf},
volume = {16},
year = {2016}
}
@inproceedings{Hope2016,
abstract = {This paper-demonstration provides an overview of an generative music score adapted for the iPad by the Decibel new music ensemble. The original score `Loaded (NSFW)' (2015) is by Western Australian composer Laura Jane Lowther, and is scored for ensemble and electronics, commissioned for a performance in April 2015 at the Perth Institute of Contemporary Arts. It engages and develops the Decibel Score Player application, a score reader and generator for the iPad as a tool for displaying an interactive score that requires performers to react to news headlines through musical means. The paper will introduce the concept for the player, how it was developed, and how it was used in the premiere performance. The associated demonstration shows how the score appears on the iPads.},
address = {Brisbane, Australia},
author = {Hope, Cat and James, Stuart and Wyatt, Aaron},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
isbn = {978-1-925455-13-7},
pages = {375--376},
publisher = {Queensland Conservatorium Griffith University},
series = {2220-4806},
title = {{Headline grabs for music: The development of the iPad score generator for `Loaded (NSFW)'}},
url = {http://www.nime.org/proceedings/2016/nime2016{\_}paper0074.pdf},
volume = {16},
year = {2016}
}
@inproceedings{Waite2016,
abstract = {This paper presents a brief review of current literature detailing some of the issues and trends in composition and performance with interactive music systems. Of particular interest is how musicians interact with a separate machine entity that exercises agency over the creative process. The use of real-world metaphors as a strategy for increasing audience engagement is also discussed. The composition and system Church Belles is presented, analyzed and evaluated in terms of its architecture, how it relates to existing studies of musician-machine creative interaction and how the use of a real-world metaphor can promote audience perceptions of liveness. This develops previous NIME work by offering a detailed case study of the development process of both a system and a piece for popular, non-improvisational vocal/guitar music.},
address = {Brisbane, Australia},
author = {Waite, Si},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
isbn = {978-1-925455-13-7},
pages = {265--270},
publisher = {Queensland Conservatorium Griffith University},
series = {2220-4806},
title = {{Church Belles: An Interactive System and Composition Using Real-World Metaphors}},
url = {http://www.nime.org/proceedings/2016/nime2016{\_}paper0052.pdf},
volume = {16},
year = {2016}
}
@inproceedings{Huberth2016,
abstract = {Notation systems are used in almost all fields, especially for the communication and expression of ideas. This paper proposes and discusses a notation system for Gametrak-based computer music instruments. The notation system's design is informed both by Western music notation and dance notation, as well as common mappings used in laptop orchestras. It is designed to be sound- agnostic, primarily instructing the performer in their motions. While the discussion of such a notation system may be particularly timely due to the growing commercially-available 3D motion tracking controllers, the notation system may prove especially useful in the context of Gametrak and laptop orchestra, for which score-based representation can help clarify performer interaction and serve as a teaching tool in documenting prior work.},
address = {Brisbane, Australia},
author = {Huberth, Madeline and Nanou, Chryssie},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
isbn = {978-1-925455-13-7},
pages = {96--105},
publisher = {Queensland Conservatorium Griffith University},
series = {2220-4806},
title = {{Notation for {\{}3D{\}} Motion Tracking Controllers: A Gametrak Case Study}},
url = {http://www.nime.org/proceedings/2016/nime2016{\_}paper0020.pdf},
volume = {16},
year = {2016}
}
@inproceedings{Xiao2016,
abstract = {This paper explores how an actuated pin-based shape display may serve as a platform on which to build musical instruments and controllers. We designed and prototyped three new instruments that use the shape display not only as an input device, but also as a source of acoustic sound. These cover a range of interaction paradigms to generate ambient textures, polyrhythms, and melodies. This paper first presents existing work from which we drew interactions and metaphors for our designs. We then introduce each of our instruments and the back-end software we used to prototype them. Finally, we offer reflections on some central themes of NIME, including the relationship between musician and machine.},
address = {Brisbane, Australia},
author = {Xiao, Xiao and Haddad, Donald Derek and Sanchez, Thomas and van Troyer, Akito and Kleinberger, R{\'{e}}becca and Webb, Penny and Paradiso, Joseph A and Machover, Tod and Ishii, Hiroshi},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
isbn = {978-1-925455-13-7},
pages = {259--264},
publisher = {Queensland Conservatorium Griffith University},
series = {2220-4806},
title = {{Kin{\'{e}}phone: Exploring the Musical Potential of an Actuated Pin-Based Shape Display}},
url = {http://www.nime.org/proceedings/2016/nime2016{\_}paper0051.pdf},
volume = {16},
year = {2016}
}
@inproceedings{Dabin2016,
abstract = {This project explores the potential for 3D modelling and printing to create customised flutes that can play music in a variety of microtonal scales. One of the challenges in the field of microtonality is that conventional musical instruments are inadequate for realising the abundance of theoretical tunings that musicians wish to investigate. This paper focuses on the development of two types of flutes, the recorder and transverse flute, with interchangeable mouthpieces. These flutes are designed to play subharmonic microtonal scales. The discussion provides an overview of the design and implementation process, including calculation methods for acoustic modelling and 3D printing technologies, as well as an evaluation of some of the difficulties encountered. Results from our 3D printed flutes suggest that whilst further refinements are necessary in our designs, 3D modelling and printing techniques offer new and valuable methods for the design and production of customised musical instruments. The long term goal of this project is to create a system in which users can specify the tuning of their instrument to generate a 3D model and have it printed on demand.},
address = {Brisbane, Australia},
author = {Dabin, Matthew and Narushima, Terumi and Beirne, Stephen and Ritz, Christian and Grady, Kraig},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
isbn = {978-1-925455-13-7},
pages = {286--290},
publisher = {Queensland Conservatorium Griffith University},
series = {2220-4806},
title = {{{\{}3D{\}} Modelling and Printing of Microtonal Flutes}},
url = {http://www.nime.org/proceedings/2016/nime2016{\_}paper0056.pdf},
volume = {16},
year = {2016}
}
@inproceedings{Zhang2016,
abstract = {This paper presents a web-based application enabling audiences to collaboratively contribute to the creative process during live music performances. The system aims at enhancing audience engagement and creating new forms of live music experiences. Interaction between audience and performers is made possible through a client/server architecture enabling bidirectional communication of creative data. Audience members can vote for pre-determined musical attributes using a smartphone-friendly and cross-platform web application. The system gathers audience members' votes and provide feedback through visualisations that can be tailored for specific needs. In order to support multiple performers and large audiences, automatic audience-to-performer groupings are handled by the application. The framework was applied to support live interactive musical improvisations where creative roles are shared amongst audience and performers (Open Symphony). Qualitative analyses of user surveys highlighted very positive feedback related to themes such as engagement and creativity and also identified further design challenges around audience sense of control and latency.},
address = {Brisbane, Australia},
author = {Zhang, Leshao and Wu, Yongmeng and Barthet, Mathieu},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
isbn = {978-1-925455-13-7},
pages = {170--175},
publisher = {Queensland Conservatorium Griffith University},
series = {2220-4806},
title = {{A Web Application for Audience Participation in Live Music Performance: The Open Symphony Use Case}},
url = {http://www.nime.org/proceedings/2016/nime2016{\_}paper0036.pdf},
volume = {16},
year = {2016}
}
@inproceedings{Bowers2016,
abstract = {This paper describes an instance of what we call `curated research', a concerted thinking, making and performance activity between two research teams with a dedicated interest in the creation of experimental musical instruments and the development of new performance practices. Our work builds theoretically upon critical work in philosophy, anthropology and aesthetics, and practically upon previous explorations of strategies for facilitating rapid, collaborative, publicly-oriented making in artistic settings. We explored an orientation to making which promoted the creation of a family of instruments and performance environments that were responses to the self-consciously provocative theme of `One Knob To Rule Them All'. A variety of design issues were explored including: mapping, physicality, the question of control in interface design, reductionist aesthetics and design strategies, and questions of gender and power in musical culture. We discuss not only the technologies which were made but also reflect on the value of such concerted, provocatively thematised, collective making activities for addressing foundational design issues. As such, our work is intended not just as a technical and practical contribution to NIME but also a reflective provocation into how we conduct research itself in a curated critical manner.},
address = {Brisbane, Australia},
author = {Bowers, John and Richards, John and Shaw, Tim and Frieze, Jim and Freeth, Ben and Topley, Sam and Spowage, Neal and Jones, Steve and Patel, Amit and Rui, Li},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
isbn = {978-1-925455-13-7},
pages = {433--438},
publisher = {Queensland Conservatorium Griffith University},
series = {2220-4806},
title = {{One Knob To Rule Them All: Reductionist Interfaces for Expansionist Research}},
url = {http://www.nime.org/proceedings/2016/nime2016{\_}paper0085.pdf},
volume = {16},
year = {2016}
}
@inproceedings{Wu2016,
abstract = {This paper presents an empirical evaluation of a digital music instrument (DMI) for electroacoustic vocal performance, the Tibetan Singing Prayer Wheel (TSPW). Specifically, we study audience preference for the way it maps horizontal spinning gestures to vocal processing parameters. We filmed six songs with the singer using the TSPW, and created two alternative soundtracks for each song: one desynchronized, and one with the mapping inverted. Participants viewed all six songs with either the original or desynchronized soundtrack (Experiment 1), or either the original or inverted-mapping soundtrack (Experiment 2). Participants were asked several questions via questionnaire after each song. Overall, they reported higher engagement and preference for the original versions, suggesting that audiences of the TSPW prefer more highly synchronized performances, as well as more intuitive mappings, though level of perceived expression of the performer only significantly differed in Experiment 1. Further, we believe that our experimental methods contribute to how DMIs can be evaluated from the audience's (a recently noted under- represented stakeholder) perspective.},
address = {Brisbane, Australia},
author = {Wu, Jiayue Cecilia and Huberth, Madeline and Yeh, Yoo Hsiu and Wright, Matthew},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
isbn = {978-1-925455-13-7},
pages = {206--211},
publisher = {Queensland Conservatorium Griffith University},
series = {2220-4806},
title = {{Evaluating the Audience's Perception of Real-time Gestural Control and Mapping Mechanisms in Electroacoustic Vocal Performance}},
url = {http://www.nime.org/proceedings/2016/nime2016{\_}paper0042.pdf},
volume = {16},
year = {2016}
}
@inproceedings{Bhumber2016,
abstract = {This paper describes the processes involved in developing Pendula, a performance environment and interactive installation using swings, interactive video, and audio. A presentation of the project is described using three swings. Gyroscopic and accelerometer data were used in each of the setups to control audio and visual parameters.The installation was presented as both an interactive environment and as a performance instrument, with multiple public performances. Construction of the physical devices used, circuits built, and software created is covered in this paper, along with a discussion of problems and their solutions encountered during the development of Pendula.},
address = {Brisbane, Australia},
author = {Bhumber, Kiran and Lee, Nancy and Topp, Brian},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
isbn = {978-1-925455-13-7},
pages = {277--285},
publisher = {Queensland Conservatorium Griffith University},
series = {2220-4806},
title = {{Pendula: An Interactive Swing Installation and Performance Environment}},
url = {http://www.nime.org/proceedings/2016/nime2016{\_}paper0054.pdf},
volume = {16},
year = {2016}
}
@inproceedings{Barrett2016,
abstract = {Despite increasingly accessible and user-friendly multi-channel compositional tools, many composers still choose stereo formats for their work, where the compositional process is allied to diffusion performance over a `classical' loudspeaker orchestra. Although such orchestras remain common within UK institutions as well as in France, they are in decline in the rest of the world. In contrast, permanent, high-density loudspeaker arrays are on the rise, as is the practical application of 3-D audio technologies. Looking to the future, we need to reconcile the performance of historical and new stereo works, side-by-side native 3-D compositions. In anticipation of this growing need, we have designed and tested a prototype `Virtualmonium'. The Virtualmonium is an instrument for classical diffusion performance over an acousmonium emulated in higher-order Ambisonics. It allows composers to custom-design loudspeaker orchestra emulations for the performance of their works, rehearse and refine performances off-site, and perform classical repertoire alongside native 3-D formats in the same concert. This paper describes the technical design of the Virtualmonium, assesses the success of the prototype in some preliminary listening tests and concerts, and speculates how the instrument can further composition and performance practice.},
address = {Brisbane, Australia},
author = {Barrett, Natasha and Jensenius, Alexander Refsum},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
isbn = {978-1-925455-13-7},
pages = {55--60},
publisher = {Queensland Conservatorium Griffith University},
series = {2220-4806},
title = {{The `Virtualmonium': an instrument for classical sound diffusion over a virtual loudspeaker orchestra}},
url = {http://www.nime.org/proceedings/2016/nime2016{\_}paper0012.pdf},
volume = {16},
year = {2016}
}
@inproceedings{Johnson2016,
abstract = {This paper provides an overview of a new mechatronic loudspeaker system: speaker.motion. The system affords automated positioning of a loudspeaker in real-time in order to manipulate the spatial qualities of electronic music. The paper gives a technical overview of how the system's hardware and software were developed and the design criteria and methodology. There is discussion of the unique features of the speaker.motion spatialisation system and the methods of user interaction, as well as a look at the creative possibilities that the loudspeakers afford. The creative affordances are explored through the case study of two new pieces written for the speaker.motion system. It is hoped that the speaker.motion system will afford composers and performers with a new range of spatial aesthetics to use in spatial performances, and encourage exploration of the acoustic properties of physical performance and installation spaces in electronic music.},
address = {Brisbane, Australia},
author = {Johnson, Bridget and Norris, Michael and Kapur, Ajay},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
isbn = {978-1-925455-13-7},
pages = {41--45},
publisher = {Queensland Conservatorium Griffith University},
series = {2220-4806},
title = {{speaker.motion: A Mechatronic Loudspeaker System for Live Spatialisation}},
url = {http://www.nime.org/proceedings/2016/nime2016{\_}paper0009.pdf},
volume = {16},
year = {2016}
}
@inproceedings{Jakobsen2016,
abstract = {This paper presents a novel platform for expressive music making called Hitmachine. Hitmachine lets you build and play your own musical instruments from Legos and sensors and is aimed towards empowering everyone to engage in rich music making despite of prior musical experience. The paper presents findings from a 4-day workshop where more that 150 children from ages 3-13 built and played their own musical instruments. The children used different sensors for playing and performed with their instruments on stage. The findings show how age influenced the children's musical understanding and expressivity, and gives insight into important aspects to consider when designing for expressive music for novices.},
address = {Brisbane, Australia},
author = {Jakobsen, Kasper Buhl and Winge, Jakob and Petersen, Marianne Graves and Stougaard, Jeppe and Groenbaek, Jens Emil and Rasmussen, Majken Kirkegaard},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
isbn = {978-1-925455-13-7},
pages = {241--246},
publisher = {Queensland Conservatorium Griffith University},
series = {2220-4806},
title = {{Hitmachine: Collective Musical Expressivity for Novices}},
url = {http://www.nime.org/proceedings/2016/nime2016{\_}paper0048.pdf},
volume = {16},
year = {2016}
}
@inproceedings{Lee2016,
abstract = {This paper suggests a novel form of audiovisual performance- live writing- that transforms creative writing into a real-time performing art. The process of typing a poem on the fly is captured and augmented to create an audiovisual performance that establishes natural links among the components of typing gestures, the poem being written on the fly, and audiovisual artifacts. Live writing draws upon ideas from the tradition of live coding in which the process of programming is revealed to the audience in real-time. This paper discusses the motivation behind the idea, interaction schemes and a performance interface for such a performance practice. Our live writing performance system is enabled by a custom text editor, writing-sound mapping strategies of our choice, a poem-sonification, and temporal typography. We describe two live writing performances that take different approaches as they vary the degree of composition and improvisation in writing.},
address = {Brisbane, Australia},
author = {Lee, Sang Won and Essl, Georg and Martinez, Mari},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
isbn = {978-1-925455-13-7},
pages = {212--217},
publisher = {Queensland Conservatorium Griffith University},
series = {2220-4806},
title = {{Live Writing : Writing as a Real-time Audiovisual Performance}},
url = {http://www.nime.org/proceedings/2016/nime2016{\_}paper0043.pdf},
volume = {16},
year = {2016}
}
@inproceedings{Baldwin2016,
abstract = {An interactive museum exhibit of a digitally augmented medieval musical instrument, the tromba marina, is presented. The tromba marina is a curious single stringed instrument with a rattling bridge, from which a trumpet-like timbre is produced. The physical instrument was constructed as a replica of one found in Musikmuseet in Frederiksberg. The replicated instrument was augmented with a pickup, speakers and digital signal processing to create a more reliable, approachable and appropriate instrument for interactive display in the museum. We report on the evaluation of the instrument performed at the Danish museum of musical instruments.},
address = {Brisbane, Australia},
author = {Baldwin, Alex and Hammer, Troels and Pechiulis, Edvinas and Williams, Peter and Overholt, Dan and Serafin, Stefania},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
isbn = {978-1-925455-13-7},
pages = {14--19},
publisher = {Queensland Conservatorium Griffith University},
series = {2220-4806},
title = {{Tromba Moderna: A Digitally Augmented Medieval Instrument}},
url = {http://www.nime.org/proceedings/2016/nime2016{\_}paper0004.pdf},
volume = {16},
year = {2016}
}
@inproceedings{Lindell2016,
abstract = {We organised an elven day intense course in materiality for musical expressions to explore underlying principles of New Interfaces for Musical Expression (NIME) in higher educa- tion. We grounded the course in different aspects of ma- teriality and gathered interdisciplinary student teams from three Nordic universities. Electronic music instrument mak- ers participated in providing the course. In eleven days the students designed and built interfaces for musical expres- sions, composed a piece, and performed at the Norberg elec- tronic music festival. The students explored the relationship between technology and possible musical expression with a strong connection to culture and place. The emphasis on performance provided closure and motivated teams to move forward in their design and artistic processes. On the basis of the course we discuss an interdisciplinary NIME course syllabus, and we infer that it benefits from grounding in materiality and in the place with a strong reference to culture.},
address = {Brisbane, Australia},
author = {Lindell, Rikard and Tahirolu, Koray and Riis, Morten and Schaeffer, Jennie},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
isbn = {978-1-925455-13-7},
pages = {344--349},
publisher = {Queensland Conservatorium Griffith University},
series = {2220-4806},
title = {{Materiality for Musical Expressions: an Approach to Interdisciplinary Syllabus Development for NIME}},
url = {http://www.nime.org/proceedings/2016/nime2016{\_}paper0067.pdf},
volume = {16},
year = {2016}
}
@inproceedings{Rieger2016,
abstract = {The Driftwood is a maneuverable sculptural instrument {\&} controller. Tactilely, it is a micro-terrain one can explore with the hands as with the ears. Closed circuit sensors, moving wooden parts and Piezo microphones are discussed in the design phase alongside background and musical implementation concepts. Electronics and nature converge in this instrument harmoniously referencing our changing world and environment. When engaging with the sonic sculpture silent objects become audible and rest-wood is venerated. It is revealed to the musician interacting with Driftwood that our actions intervene directly with issues relating to sustainability and the amount of value we place on the world we live in. Every scrap of wood was once a tree, Driftwood reminds us of this in a multi-sensory playing experience. The Driftwood proposes a reinterpretation of the process of music creation, awareness and expression.},
address = {Brisbane, Australia},
author = {Rieger, Alexandra and Topel, Spencer},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
isbn = {978-1-925455-13-7},
pages = {158--159},
publisher = {Queensland Conservatorium Griffith University},
series = {2220-4806},
title = {{Driftwood: Redefining Sound Sculpture Controllers}},
url = {http://www.nime.org/proceedings/2016/nime2016{\_}paper0032.pdf},
volume = {16},
year = {2016}
}
@inproceedings{Greenhill2016,
abstract = {We present Focal, an eye-tracking musical expression controller which allows hands-free control over audio effects and synthesis parameters during peformance. A see-through head-mounted display projects virtual dials and switches into the visual field. The performer controls these with a single expression pedal, switching context by glancing at the object they wish to control. This simple interface allows for minimal physical disturbance to the instrumental musician, whilst enabling the control of many parameters otherwise only achievable with multiple foot pedalboards. We describe the development of the system, including the construction of the eye-tracking display, and the design of the musical interface. We also present a comparison of a performance between Focal and conventional controllers.},
address = {Brisbane, Australia},
author = {Greenhill, Stewart and Travers, Cathie},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
isbn = {978-1-925455-13-7},
pages = {230--235},
publisher = {Queensland Conservatorium Griffith University},
series = {2220-4806},
title = {{Focal : An Eye-Tracking Musical Expression Controller}},
url = {http://www.nime.org/proceedings/2016/nime2016{\_}paper0046.pdf},
volume = {16},
year = {2016}
}
@inproceedings{Volioti2016,
abstract = {There is a growing interest in `unlocking' the motor skills of expert musicians. Motivated by this need, the main objective of this paper is to present a new way of modeling expressive gesture variations in musical performance. For this purpose, the 3D gesture recognition engine `x2Gesture' (eXpert eXpressive Gesture) has been developed, inspired by the Gesture Variation Follower, which is initially designed and developed at IRCAM in Paris and then extended at Goldsmiths College in London. x2Gesture supports both learning of musical gestures and live performing, through gesture sonification, as a unified user experience. The deeper understanding of the expressive gestural variations permits to define the confidence bounds of the expert's gestures, which are used during the decoding phase of the recognition. The first experiments show promising results in terms of recognition accuracy and temporal alignment between template and performed gesture, which leads to a better fluidity and immediacy and thus gesture sonification.},
address = {Brisbane, Australia},
author = {Volioti, Christina and Manitsaris, Sotiris and Katsouli, Eleni and Manitsaris, Athanasios},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
isbn = {978-1-925455-13-7},
pages = {310--315},
publisher = {Queensland Conservatorium Griffith University},
series = {2220-4806},
title = {{x2Gesture: how machines could learn expressive gesture variations of expert musicians}},
url = {http://www.nime.org/proceedings/2016/nime2016{\_}paper0061.pdf},
volume = {16},
year = {2016}
}
@inproceedings{Oda2016,
abstract = {At a time in the near future, many computers (including devices such as smart-phones) will have system clocks that are synchronized to a high degree (less than 1 ms of error). This will enable us to coordinate events across unconnected devices with a degree of accuracy that was previously impossible. In particular, high clock synchronization means that we can use these clocks to synchronize tempo between humans or sequencers with little-to-no communication between the devices. To facilitate this low-overhead tempo synchronization, we propose the Global Metronome, which is a simple, computationally cheap method to obtain absolute tempo synchronization. We present experimental results demonstrating the effectiveness of using the Global Metronome and compare the performance to MIDI clock sync, a common synchronization method. Finally, we present an open source implementation of a Global Metronome server using a GPS-connected Raspberry Pi that can be built for under {\$}100.},
address = {Brisbane, Australia},
author = {Oda, Reid and Fiebrink, Rebecca},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
isbn = {978-1-925455-13-7},
pages = {26--31},
publisher = {Queensland Conservatorium Griffith University},
series = {2220-4806},
title = {{The Global Metronome: Absolute Tempo Sync For Networked Musical Performance}},
url = {http://www.nime.org/proceedings/2016/nime2016{\_}paper0006.pdf},
volume = {16},
year = {2016}
}
@inproceedings{Cherston2016,
abstract = {We present a sonification platform for generating audio driven by real-time particle collision data from the ATLAS experiment at CERN. This paper provides a description of the data-to-audio mapping interfaces supported by the project's composition tool as well as a preliminary evaluation of the platform's evolution to meet the aesthetic needs of vastly distinct musical styles and presentation venues. Our work has been conducted in collaboration with the ATLAS Outreach team and is part of a broad vision to better harness real-time sensor data as a canvas for artistic expression. Data-driven streaming audio can be treated as a reimagined form of live radio for which composers craft the instruments but real-time particle collisions pluck the strings.},
address = {Brisbane, Australia},
author = {Cherston, Juliana and Hill, Ewan and Goldfarb, Steven and Paradiso, Joseph A},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
isbn = {978-1-925455-13-7},
pages = {78--83},
publisher = {Queensland Conservatorium Griffith University},
series = {2220-4806},
title = {{Musician and Mega-Machine: Compositions Driven by Real-Time Particle Collision Data from the ATLAS Detector}},
url = {http://www.nime.org/proceedings/2016/nime2016{\_}paper0017.pdf},
volume = {16},
year = {2016}
}
@inproceedings{Shapiro2016,
abstract = {NIME research realizes a vision of performance by means of computational expression, linking body and space to sound and imagery through eclectic forms of sensing and interaction. This vision could dramatically impact computer science education, simultaneously modernizing the field and drawing in diverse new participants. We describe our work creating a NIME-inspired computer music toolkit for kids called BlockyTalky; the toolkit enables users to create networks of sensing devices and synthesizers. We offer findings from our research on student learning through programming and performance. We conclude by suggesting a number of future directions for NIME researchers interested in education.},
address = {Brisbane, Australia},
author = {Shapiro, R Benjamin and Fiebrink, Rebecca and Ahrens, Matthew and Kelly, Annie},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
isbn = {978-1-925455-13-7},
pages = {427--432},
publisher = {Queensland Conservatorium Griffith University},
series = {2220-4806},
title = {{BlockyTalky: A Physical and Distributed Computer Music Toolkit for Kids}},
url = {http://www.nime.org/proceedings/2016/nime2016{\_}paper0084.pdf},
volume = {16},
year = {2016}
}
@inproceedings{Masaok2016,
abstract = {Not Just Gadgets: Interfaces as tools towards empathy and meaning. Thoughts on building things that seek to unify an experience for the viewer, to facilitate new contemplations and viewpoints on society, the human condition and the viewers' own selves. Miya Masaoka I present a personal history of works with gestural controllers, including Pieces with Plants, Ritual with Hissing Madagascar Cockroaches, wherein interfaces provided a direct relationship to society and/or the natural world, plants and insects, and a means of tracking movement and response into musical compositions and performances. Included in this discussion will be LED KIMONO, a responsive hand sewn responsive wearable coat with hundreds of individually controlled LED's and a live presentation of the Laser Koto, an interface using midi, light optical sensors and hundreds of samples (originally created at CNMAT, UC Berkeley)},
address = {Brisbane, Australia},
author = {Masaoka, Miya},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
isbn = {978-1-925455-13-7},
pages = {1},
publisher = {Queensland Conservatorium Griffith University},
series = {2220-4806},
title = {{Not Just Gadgets: Interfaces as tools towards empathy and meaning.}},
url = {http://www.nime.org/proceedings/2016/nime2016{\_}paper0001.pdf},
volume = {16},
year = {2016}
}
@inproceedings{Becking2016,
abstract = {Most instruments traditionally used to teach music in early education, like xylophones or flutes, encumber children with the additional difficulty of an unfamiliar and unnatural interface. The most simple expressive interaction, that even the smallest children use in order to make music, is pounding at surfaces. Through the design of an instrument with a simple interface, like a drum, but which produces a melodic sound, children can be provided with an easy and intuitive means to produce consonance. This should then be further complemented with information from analysis and interpretation of childlike gestures and dance moves, reflecting their natural understanding of musical structure and motion. Based on these assumptions we propose a modular and reactive system for dynamic composition with accessible interfaces, divided into distinct plugins usable in a standard digital audio workstation. This paper describes our concept and how it can facilitate access to collaborative music making for small children. A first prototypical implementation has been designed and developed during the ongoing research project Drum-Dance-Music-Machine (DDMM), a cooperation with the local social welfare association AWO Hagen and the chair of musical education at the University of Applied Sciences Bielefeld.},
address = {Brisbane, Australia},
author = {Becking, Dominic and Steinmeier, Christine and Kroos, Philipp},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
isbn = {978-1-925455-13-7},
pages = {112--117},
publisher = {Queensland Conservatorium Griffith University},
series = {2220-4806},
title = {{Drum-Dance-Music-Machine: Construction of a Technical Toolset for Low-Threshold Access to Collaborative Musical Performance}},
url = {http://www.nime.org/proceedings/2016/nime2016{\_}paper0023.pdf},
volume = {16},
year = {2016}
}
@inproceedings{Kleinberger2016,
abstract = {The following paper documents the prototype of a musical door that interactively plays sounds, melodies, and sound textures when in use. We took the natural interactions people have with doors---grabbing and turning the knob and pushing and puling motions---and turned them into musical activities. The idea behind this project comes from the fact that the activity of using a door is almost always accompanied by a sound that is generally ignored by the user. We believe that this sound can be considered musically rich and expressive because each door has specific sound characteristics and each person makes it sound slightly different. By augmenting the door to create an unexpected sound, this project encourages us to listen to our daily lives with a musician's critical ear, and reminds us of the musicality of our everyday activities.},
address = {Brisbane, Australia},
author = {Kleinberger, R{\'{e}}becca and van Troyer, Akito},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
isbn = {978-1-925455-13-7},
pages = {160--161},
publisher = {Queensland Conservatorium Griffith University},
series = {2220-4806},
title = {{Dooremi: a Doorway to Music}},
url = {http://www.nime.org/proceedings/2016/nime2016{\_}paper0033.pdf},
volume = {16},
year = {2016}
}
@inproceedings{Bin2016,
abstract = {This paper explores the roles of technical and musical familiarity in shaping audience response to digital musical instrument (DMI) performances. In an audience study conducted during an evening concert, we examined two primary questions: first, whether a deeper understanding of how a DMI works increases an audience's enjoyment and interest in the performance; and second, given the same DMI and same performer, whether playing in a conventional (vernacular) versus an experimental musical style affects an audience's response. We held a concert in which two DMI creator-performers each played two pieces in differing styles. Before the concert, each half the 64-person audience was given a technical explanation of one of the instruments. Results showed that receiving an explanation increased the reported understanding of that instrument, but had no effect on either the reported level of interest or enjoyment. On the other hand, performances in experimental versus conventional style on the same instrument received widely divergent audience responses. We discuss implications of these findings for DMI design.},
address = {Brisbane, Australia},
author = {Bin, S Astrid and Bryan-Kinns, Nick and McPherson, Andrew},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
isbn = {978-1-925455-13-7},
pages = {200--205},
publisher = {Queensland Conservatorium Griffith University},
series = {2220-4806},
title = {{Skip the Pre-Concert Demo: How Technical Familiarity and Musical Style Affect Audience Response}},
url = {http://www.nime.org/proceedings/2016/nime2016{\_}paper0041.pdf},
volume = {16},
year = {2016}
}
@inproceedings{Reid2016,
abstract = {This paper describes the design of a Minimally Invasive Gesture Sensing Interface (MIGSI) for trumpet. The interface attaches effortlessly to any B-flat or C trumpet and requires no permanent modifications to the host-instrument. It was designed first and foremost with accessibility in mind an approach that is uncommon in augmented instrument design and seeks to strike a balance between minimal design and robust control. MIGSI uses sensor technology to capture gestural data such as valve displacement, hand tension, and instrument position, to offer extended control and expressivity to trumpet players. Several streams of continuous data are transmitted wirelessly from MIGSI to the receiving computer, where MIGSI Mapping application (a simple graphical user interface) parses the incoming data into individually accessible variables. It is our hope that MIGSI will be adopted by trumpet players and composers, and that over time a new body of repertoire for the augmented trumpet will emerge.},
address = {Brisbane, Australia},
author = {Reid, Sarah and Gaston, Ryan and Honigman, Colin and Kapur, Ajay},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
isbn = {978-1-925455-13-7},
pages = {419--424},
publisher = {Queensland Conservatorium Griffith University},
series = {2220-4806},
title = {{Minimally Invasive Gesture Sensing Interface (MIGSI) for Trumpet}},
url = {http://www.nime.org/proceedings/2016/nime2016{\_}paper0082.pdf},
volume = {16},
year = {2016}
}
@inproceedings{Baytanicode3512016,
abstract = {In this paper, we investigate how watching a live-sequenced electronic music performance, compared to merely hearing the music, contributes to spectators' experiences of tension. We also explore the role of the performers' effective and ancillary gestures in conveying tension, when they can be seen. To this end, we conducted an experiment where 30 participants heard, saw, or both heard and saw a live-sequenced techno music performance recording while they produced continuous judgments on their experience of tension. Eye tracking data was also recorded from participants who saw the visuals, to reveal aspects of the performance that influenced their tension judgments. We analysed the data to explore how auditory and visual components and the performer's movements contribute to spectators' experience of tension. Our results show that their perception of emotional intensity is consistent across hearing and sight, suggesting that gestures in live-sequencing can be a medium for expressive performance.},
address = {Brisbane, Australia},
author = {Baytas, Mehmet Aydin and G{\"{o}}ksun, Tilbe and {\"{O}}zcan, Oguzhan},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
isbn = {978-1-925455-13-7},
pages = {194--199},
publisher = {Queensland Conservatorium Griffith University},
series = {2220-4806},
title = {{The Perception of Live-sequenced Electronic Music via Hearing and Sight}},
url = {http://www.nime.org/proceedings/2016/nime2016{\_}paper0040.pdf},
volume = {16},
year = {2016}
}
@inproceedings{Nakanishi2016,
abstract = {NAKANISYNTH is a synthesiser application available on iOS devices that provides a simple and intuitive interface, allowing users to produce sound loops by freehand drawing sound waves and envelope curves. The interface provides a simple way of interacting: the only input required involves drawing two waveforms, meaning that users can easily produce various sounds intuitively without the need for complex manipulation. The application's interface comprises of an interchangeable ribbon and keyboard feature, plus two panels where users can edit waveforms, allowing users to make sounds. This simple approach to the interface means that it is easy for users to understand the relationship between a waveform and the sound that it produces.},
address = {Brisbane, Australia},
author = {Nakanishi, Kyosuke and Haimes, Paul and Baba, Tetsuaki and Kushiyama, Kumiko},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
isbn = {978-1-925455-13-7},
pages = {143--145},
publisher = {Queensland Conservatorium Griffith University},
series = {2220-4806},
title = {{NAKANISYNTH: An Intuitive Freehand Drawing Waveform Synthesiser Application for iOS Devices}},
url = {http://www.nime.org/proceedings/2016/nime2016{\_}paper0029.pdf},
volume = {16},
year = {2016}
}
@inproceedings{jfernandez:2017,
abstract = {We present here GeKiPe, a gestural interface for musical expression, combining images and sounds, generated and controlled in real time by a performer. GeKiPe is developed as part of a creation project, exploring the control of virtual instruments through the analysis of gestures specific to instrumentalists, and to percussionists in particular. GeKiPe was used for the creation of a collaborative stage performance (Sculpt), in which the musician and their movements are captured by different methods (infrared Kinect cameras and gesture-sensors on controller gloves). The use of GeKiPe as an alternate sound and image controller allowed us to combine body movement, musical gestures and audiovisual expressions to create challenging collaborative performances.},
address = {Copenhagen, Denmark},
author = {Fernandez, Jos{\'{e}} Miguel and K{\"{o}}ppel, Thomas and Verstraete, Nina and Lorieux, Gr{\'{e}}goire and Vert, Alexander and Spiesser, Philippe},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
pages = {450--455},
publisher = {Aalborg University Copenhagen},
title = {{GeKiPe, a gesture-based interface for audiovisual performance}},
url = {http://www.nime.org/proceedings/2017/nime2017{\_}paper0085.pdf},
year = {2017}
}
@inproceedings{bliang:2017,
abstract = {This paper presents the results of a study of piano pedalling techniques on the sustain pedal using a newly designed measurement system named Piano Pedaller. The system is comprised of an optical sensor mounted in the piano pedal bearing block and an embedded platform for recording audio and sensor data. This enables recording the pedalling gesture of real players and the piano sound under normal playing conditions. Using the gesture data collected from the system, the task of classifying these data by pedalling technique was undertaken using a Support Vector Machine (SVM). Results can be visualised in an audio based score following application to show pedalling together with the player's position in the score.},
address = {Copenhagen, Denmark},
author = {Liang, Beici and Fazekas, Gy{\"{o}}rgy and McPherson, Andrew and Sandler, Mark},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
pages = {325--329},
publisher = {Aalborg University Copenhagen},
title = {{Piano Pedaller: A Measurement System for Classification and Visualisation of Piano Pedalling Techniques}},
url = {http://www.nime.org/proceedings/2017/nime2017{\_}paper0062.pdf},
year = {2017}
}
@inproceedings{jvetter:2017,
abstract = {In this paper we discuss a modular instrument system for musical expression consisting of multiple devices using string detection, sound synthesis and wireless communication. The design of the system allows for different physical arrangements, which we define as topologies. In particular we will explain our concept and requirements, the system architecture including custom magnetic string sensors and our network communication and discuss its use in the performance HOMO RESTIS.},
address = {Copenhagen, Denmark},
author = {Vetter, Jens and Leimcke, Sarah},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
pages = {83--86},
publisher = {Aalborg University Copenhagen},
title = {{Homo Restis - Constructive Control Through Modular String Topologies}},
url = {http://www.nime.org/proceedings/2017/nime2017{\_}paper0017.pdf},
year = {2017}
}
@inproceedings{fkeenan:2017,
abstract = {This paper presents the next stage of an investigation into the potential of historical theatre sound effects as a resource for Sonic Interaction Design (SID). An acoustic theatre wind machine was constructed, and a digital physical modelling-based version of this specific machine was programmed using the Sound Designer{\&}{\#}8217;s Toolkit (SDT) in Max/MSP. The acoustic wind machine was fitted with 3D printed gearing to mechanically drive an optical encoder and control the digital synthesis engine in real time. The design of this system was informed by an initial comparison between the acoustic wind machine and the first iteration of its digital counterpart. To explore the main acoustic parameters and the sonic range of the acoustic and digital wind machines in operation, three simple and distinct rotational gestures were performed, with the resulting sounds recorded simultaneously, facilitating an analysis of the real-time performance of both sources. The results are reported, with an outline of future work.},
address = {Copenhagen, Denmark},
author = {Keenan, Fiona and Pauletto, Sandra},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
pages = {431--435},
publisher = {Aalborg University Copenhagen},
title = {{Design and Evaluation of a Digital Theatre Wind Machine}},
url = {http://www.nime.org/proceedings/2017/nime2017{\_}paper0081.pdf},
year = {2017}
}
@inproceedings{ahofmann:2017,
abstract = {To build electronic musical instruments, a mapping between the real-time audio processing software and the physical controllers is required. Different strategies of mapping were developed and discussed within the NIME community to improve musical expression in live performances. This paper discusses an interface focussed instrument design approach, which starts from the physical controller and its functionality. From this definition, the required, underlying software instrument is derived. A proof of concept is implemented as a framework for effect instruments. This framework comprises a library of real-time effects for Csound, a proposition for a JSON-based mapping format, and a mapping-to-instrument converter that outputs Csound instrument files. Advantages, limitations and possible future extensions are discussed.},
address = {Copenhagen, Denmark},
author = {Hofmann, Alex and Waerstad, Bernt Isak and Balasubramanian, Saranya and Koch, Kristoffer E},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
pages = {133--138},
publisher = {Aalborg University Copenhagen},
title = {{From interface design to the software instrument - Mapping as an approach to FX-instrument building}},
url = {http://www.nime.org/proceedings/2017/nime2017{\_}paper0026.pdf},
year = {2017}
}
@inproceedings{clevican:2017,
abstract = {Brain computer interfaces are being widely adopted for music creation and interpretation, and they are becoming a truly new category of musical instruments. Indeed, Miranda has coined the term Brain-computer Musical Interface (BCMI) to refer to this category. There are no "plug-n-play" solutions for a BCMI, these kinds of tools usually require the setup and implementation of particular software configurations, customized for each EEG device. The Emotiv Insight is a low-cost EEG apparatus that outputs several kinds of data, such as EEG rhythms or facial expressions, from the user's brain activity. We have developed a BCMI, in the form of a freely available middle-ware, using the Emotiv Insight for EEG input and signal processing. The obtained data, via blue-tooth is broad-casted over the network formatted for the OSC protocol. Using this software, we tested the device's adequacy as a BCMI by using the provided data in order to control different sound synthesis algorithms in MaxMSP. We conclude that the Emotiv Insight is an interesting choice for a BCMI due to its low-cost and ease of use, but we also question its reliability and robustness.},
address = {Copenhagen, Denmark},
author = {Levican, Constanza and Aparicio, Andres and Belaunde, Vernon and Cadiz, Rodrigo},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
pages = {287--290},
publisher = {Aalborg University Copenhagen},
title = {{Insight2OSC: using the brain and the body as a musical instrument with the Emotiv Insight}},
url = {http://www.nime.org/proceedings/2017/nime2017{\_}paper0055.pdf},
year = {2017}
}
@inproceedings{mkallionpaa:2017,
abstract = {{\&}{\#}8220;Climb!{\&}{\#}8221; is a musical composition that combines the ideas of a classical virtuoso piece and a computer game. We present a case study of the composition process and realization of {\&}{\#}8220;Climb!{\&}{\#}8221;, written for Disklavier and a digital interactive engine, which was co-developed together with the musical score. Specifically, the engine combines a system for recognising and responding to musical trigger phrases along with a dynamic digital score renderer. This tool chain allows for the composer{\&}{\#}8217;s original scoring to include notational elements such as trigger phrases to be automatically extracted to auto-configure the engine for live performance. We reflect holistically on the development process to date and highlight the emerging challenges and opportunities. For example, this includes the potential for further developing the workflow around the scoring process and the ways in which support for musical triggers has shaped the compositional approach.},
address = {Copenhagen, Denmark},
author = {Kallionp{\"{a}}{\"{a}}, Maria and Greenhalgh, Chris and Hazzard, Adrian and Weigl, David M and Page, Kevin R and Benford, Steve},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
pages = {464--469},
publisher = {Aalborg University Copenhagen},
title = {{Composing and Realising a Game-like Performance for Disklavier and Electronics}},
url = {http://www.nime.org/proceedings/2017/nime2017{\_}paper0088.pdf},
year = {2017}
}
@inproceedings{jwu:2017,
abstract = {This paper presents solutions to improve reliability and to work around challenges of using a Leap Motion{\&}{\#}8482; sensor as a gestural control and input device in digital music instrument (DMI) design. We implement supervised learning algorithms (k-nearest neighbors, support vector machine, binary decision tree, and artificial neural network) to estimate hand motion data, which is not typically captured by the sensor. Two problems are addressed: 1) the sensor cannot detect overlapping hands 2) The sensor's limited detection range. Training examples included 7 kinds of overlapping hand gestures as well as hand trajectories where a hand goes out of the sensor's range. The overlapping gestures were treated as a classification problem and the best performing model was k-nearest neighbors with 62{\%} accuracy. The out-of-range problem was treated first as a clustering problem to group the training examples into a small number of trajectory types, then as a classification problem to predict trajectory type based on the hand's motion before going out of range. The best performing model was k-nearest neighbors with an accuracy of 30{\%}. The prediction models were implemented in an ongoing multimedia electroacoustic vocal performance and an educational project named Embodied Sonic Meditation (ESM).},
address = {Copenhagen, Denmark},
author = {Wu, Jiayue Cecilia and Rau, Mark and Zhang, Yun and Zhou, Yijun and Wright, Matthew},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
pages = {42--47},
publisher = {Aalborg University Copenhagen},
title = {{Towards Robust Tracking with an Unreliable Motion Sensor Using Machine Learning}},
url = {http://www.nime.org/proceedings/2017/nime2017{\_}paper0009.pdf},
year = {2017}
}
@inproceedings{fheller:2017,
abstract = {Learning to play the transverse flute is not an easy task, at least not for everyone. Since the flute does not have a reed to resonate, the player must provide a steady, focused stream of air that will cause the flute to resonate and thereby produce sound. In order to achieve this, the player has to be aware of the embouchure position to generate an adequate air jet. For a beginner, this can be a difficult task due to the lack of visual cues or indicators of the air jet and lips position. This paper attempts to address this problem by presenting an augmented flute that can make the gestures related to the embouchure visible and measurable. The augmented flute shows information about the area covered by the lower lip, estimates the lip hole shape based on noise analysis, and it shows graphically the air jet direction. Additionally, the augmented flute provides directional and continuous feedback in real time, based on data acquired by experienced flutists.},
address = {Copenhagen, Denmark},
author = {Heller, Florian and Ruiz, Irene Meying Cheung and Borchers, Jan},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
pages = {34--37},
publisher = {Aalborg University Copenhagen},
title = {{An Augmented Flute for Beginners}},
url = {http://www.nime.org/proceedings/2017/nime2017{\_}paper0007.pdf},
year = {2017}
}
@inproceedings{aeldridge:2017,
abstract = {The Feedback Cello is a new electroacoustic actuated instrument in which feedback can be induced independently on each string. Built from retro-fitted acoustic cellos, the signals from electromagnetic pickups sitting under each string are passed to a speaker built into the back of the instrument and to transducers clamped in varying places across the instrument body. Placement of acoustic and mechanical actuators on the resonant body of the cello mean that this simple analogue feedback system is capable of a wide range of complex self-resonating behaviours. This paper describes the motivations for building these instruments as both a physical extension to live coding practice and an electroacoustic augmentation of cello. The design and physical construction is outlined, and modes of performance described with reference to the first six months of performances and installations. Future developments and planned investigations are outlined.},
address = {Copenhagen, Denmark},
author = {Eldridge, Alice and Kiefer, Chris},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
pages = {25--29},
publisher = {Aalborg University Copenhagen},
title = {{Self-resonating Feedback Cello: Interfacing gestural and generative processes in improvised performance}},
url = {http://www.nime.org/proceedings/2017/nime2017{\_}paper0005.pdf},
year = {2017}
}
@inproceedings{skiratli:2017,
abstract = {In this paper we present HIVE, a parametrically designed interactive sound sculpture with embedded multi-channel digital audio which explores the intersection of sculptural form and musical instrument design. We examine sculpture as an integral part of music composition and performance, expanding the definition of musical instrument to include the gestalt of loudspeakers, architectural spaces, and material form. After examining some related works, we frame HIVE as an interactive sculpture for musical expression. We then describe our design and production process, which hinges on the relationship between sound, space, and sculptural form. Finally, we discuss the installation and its implications.},
address = {Copenhagen, Denmark},
author = {Kiratli, Solen and Cadambi, Akshay and Visell, Yon},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
pages = {267--270},
publisher = {Aalborg University Copenhagen},
title = {{HIVE: An Interactive Sculpture for Musical Expression}},
url = {http://www.nime.org/proceedings/2017/nime2017{\_}paper0050.pdf},
year = {2017}
}
@inproceedings{mjensen:2017,
abstract = {Neuroimaging is a powerful tool to explore how and why humans engage in music. Magnetic resonance imaging (MRI) has allowed us to identify brain networks and regions implicated in a range of cognitive tasks including music perception and performance. However, MRI-scanners are noisy and cramped, presenting a challenging environment for playing an instrument. Here, we present an MRI-compatible polyphonic keyboard with a materials cost of 850 USD, designed and tested for safe use in 3T (three Tesla) MRI-scanners. We describe design considerations, and prior work in the field. In addition, we provide recommendations for future designs and comment on the possibility of using the keyboard in magnetoencephalography (MEG) systems. Preliminary results indicate a comfortable playing experience with no disturbance of the imaging process.},
address = {Copenhagen, Denmark},
author = {Jensen, Martin Snejbjerg and Heggli, Ole Adrian and Mota, Patricia Alves Da and Vuust, Peter},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
pages = {257--260},
publisher = {Aalborg University Copenhagen},
title = {{A low-cost MRI compatible keyboard}},
url = {http://www.nime.org/proceedings/2017/nime2017{\_}paper0048.pdf},
year = {2017}
}
@inproceedings{eknudsen:2017,
abstract = {An application for ballet training is presented that monitors the posture position (straightness of the spine and rotation of the pelvis) deviation from the ideal position in real-time. The human skeletal data is acquired through a Microsoft Kinect v2. The movement of the student is mirrored through an abstract skeletal figure and instructions are provided through a virtual teacher. Posture deviation is measured in the following way: Torso misalignment is calculated by comparing hip center joint, shoulder center joint and neck joint position with an ideal posture position retrieved in an initial calibration procedure. Pelvis deviation is expressed as the xz-rotation of the hip-center joint. The posture deviation is sonified via a varying cut-off frequency of a high-pass filter applied to floating water sound. The posture deviation is visualized via a curve and a rigged skeleton in which the misaligned torso parts are color-coded. In an experiment with 9-12 year-old dance students from a ballet school, comparing the audio-visual feedback modality with no feedback leads to an increase in posture accuracy (p {\textless} 0.001, Cohen's d = 1.047). Reaction card feedback and expert interviews indicate that the feedback is considered fun and useful for training independently from the teacher.},
address = {Copenhagen, Denmark},
author = {Knudsen, Esben W and H{\o}lledig, Malte L and Nielsen, Mads Juel and Petersen, Rikke K and Bach-Nielsen, Sebastian and Zanescu, Bogdan-Constantin and Overholt, Dan and Purwins, Hendrik and Helweg, Kim},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
pages = {71--76},
publisher = {Aalborg University Copenhagen},
title = {{Audio-Visual Feedback for Self-monitoring Posture in Ballet Training}},
url = {http://www.nime.org/proceedings/2017/nime2017{\_}paper0015.pdf},
year = {2017}
}
@inproceedings{sleitman:2017,
abstract = {This paper is an overview of the current state of a course on New Interfaces for Musical Expression taught at Stanford University. It gives an overview of the various technologies and methodologies used to teach the interdisciplinary work of new musical interfaces.},
address = {Copenhagen, Denmark},
author = {Leitman, Sasha},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
pages = {127--132},
publisher = {Aalborg University Copenhagen},
title = {{Current Iteration of a Course on Physical Interaction Design for Music}},
url = {http://www.nime.org/proceedings/2017/nime2017{\_}paper0025.pdf},
year = {2017}
}
@inproceedings{ssalazar:2017a,
abstract = {The Fragment String is a new digital musical instrument designed to reinterpret and reflect upon the sounds of the instruments it is performed in collaboration with. At its core, it samples an input audio signal and allows the performer to replay these samples through a granular resynthesizer. Normally the Fragment String samples an acoustic instrument that accompanies it, but in the absence of this input it will amplify the ambient environment and electronic noise of the input audio path to audible levels and sample these. This ability to leverage both structural, tonal sound and unstructured noise provide the instrument with multiple dimensions of musical expressivity. The relative magnitude of the physical gestures required to manipulate the instrument and control the sound also engage an audience in its performance. This straightforward yet expressive design has lent the Fragment String to a variety of performance techniques and settings. These are explored through case studies in a five year history of Fragment String-based compositions and performances, illustrating the strengths and limitations of these interactions and their sonic output.},
address = {Copenhagen, Denmark},
author = {Salazar, Spencer and Reid, Sarah and McNamara, Daniel},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
pages = {381--386},
publisher = {Aalborg University Copenhagen},
title = {{The Fragment String}},
url = {http://www.nime.org/proceedings/2017/nime2017{\_}paper0072.pdf},
year = {2017}
}
@inproceedings{gisaac:2017,
abstract = {This paper explores the idea of using virtual textural terrains as a means of generating haptic profiles for force-feedback controllers. This approach breaks from the paradigm established within audio-haptic research over the last few decades where physical models within virtual environments are designed to transduce gesture into sonic output. We outline a method for generating multimodal terrains using basis functions, which are rendered into monochromatic visual representations for inspection. This visual terrain is traversed using a haptic controller, the NovInt Falcon, which in turn receives force information based on the grayscale value of its location in this virtual space. As the image is traversed by a performer the levels of resistance vary, and the image is realized as a physical terrain. We discuss the potential of this approach to afford engaging musical experiences for both the performer and the audience as iterated through numerous performances.},
address = {Copenhagen, Denmark},
author = {Isaac, Gabriella and Hayes, Lauren and Ingalls, Todd},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
pages = {38--41},
publisher = {Aalborg University Copenhagen},
title = {{Cross-Modal Terrains: Navigating Sonic Space through Haptic Feedback}},
url = {http://www.nime.org/proceedings/2017/nime2017{\_}paper0008.pdf},
year = {2017}
}
@inproceedings{mhojlund:2017,
abstract = {This paper describes the development of a loudness-based compressor for live audio streams. The need for this device arose while developing the public sound art project The Overheard, which involves mixing together several live audio streams through a web based mixing interface. In order to preserve a natural sounding dynamic image from the varying sound sources that can be played back under varying conditions, an adaptation of the EBU R128 loudness measurement recommendation, originally developed for levelling non-real-time broadcast material, has been applied. The paper describes the Pure Data implementation and the necessary compromises enforced by the live streaming condition. Lastly observations regarding design challenges, related application areas and future goals are presented.},
address = {Copenhagen, Denmark},
author = {H{\o}jlund, Marie and Riis, Morten and Rothmann, Daniel and Kirkegaard, Jonas},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
pages = {420--425},
publisher = {Aalborg University Copenhagen},
title = {{Applying the EBU R128 Loudness Standard in live-streaming sound sculptures}},
url = {http://www.nime.org/proceedings/2017/nime2017{\_}paper0079.pdf},
year = {2017}
}
@inproceedings{mbaalman:2017,
abstract = {Academic research projects focusing on wireless sensor networks rarely live on after the funded research project has ended. In contrast, the Sense/Stage project has evolved over the past 6 years outside of an academic context and has been used in a multitude of artistic projects. This paper presents how the project has developed, the diversity of the projects that have been made with the technology, feedback from users on the system and an outline for the design of a successor to the current system.},
address = {Copenhagen, Denmark},
author = {Baalman, Marije A},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
pages = {307--312},
publisher = {Aalborg University Copenhagen},
title = {{Wireless Sensing for Artistic Applications, a Reflection on Sense/Stage to Motivate the Design of the Next Stage}},
url = {http://www.nime.org/proceedings/2017/nime2017{\_}paper0059.pdf},
year = {2017}
}
@inproceedings{crose:2017,
abstract = {Wearable sensor technology and aerial dance movement can be integrated to provide a new performance practice and perspective on interactive kinesonic composition. SALTO (Sonic Aerialist eLecTrOacoustic system), is a system which allows for the creation of collaborative works between electroacoustic composer and aerial choreographer. The system incorporates aerial dance trapeze movement, sensors, digital synthesis, and electroacoustic composition. In SALTO, the Max software programming environment employs parameters and mapping techniques for translating the performer{\&}{\#}8217;s movement and internal experience into sound. Splinter (2016), a work for aerial choreographer/performer and the SALTO system, highlights the expressive qualities of the system in a performance setting.},
address = {Copenhagen, Denmark},
author = {Rose, Christiana},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
pages = {302--306},
publisher = {Aalborg University Copenhagen},
title = {{SALTO: A System for Musical Expression in the Aerial Arts}},
url = {http://www.nime.org/proceedings/2017/nime2017{\_}paper0058.pdf},
year = {2017}
}
@inproceedings{kkonovalovs:2017,
abstract = {This paper explores a new interaction possibility for increasing performer freedom via a foot-mounted wearable, and an instrument-mounted device that maintain stomp-box styles of interactivity, but without the restrictions normally associated with the original design of guitar effect pedals. The classic foot activated effect pedals that are used to alter the sound of the instrument are stationary, forcing the performer to return to the same location in order to interact with the pedals. This paper presents a new design that enables the performer to interact with the effect pedals anywhere on the stage. By designing a foot{\&}instrument-mounted effect controller, we kept the strongest part of the classical pedal design, while allowing the activation of the effect at any location on the stage. The usability of the device has been tested on thirty experienced guitar players. Their performance has been recorded and compared, and their opinion has been investigated through questionnaire and interview. The results of the experiment showed that, in theory, foot{\&}instrument-mounted effect controller can replace standard effect pedals and at the same time provide more mobility on a stage.},
address = {Copenhagen, Denmark},
author = {Konovalovs, Kristians and Zovnercuka, Jelizaveta and Adjorlu, Ali and Overholt, Dan},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
pages = {354--357},
publisher = {Aalborg University Copenhagen},
title = {{A Wearable Foot-mounted / Instrument-mounted Effect Controller: Design and Evaluation}},
url = {http://www.nime.org/proceedings/2017/nime2017{\_}paper0067.pdf},
year = {2017}
}
@inproceedings{kbhumber:2017,
abstract = {We describe the Responsive User Body Suit (RUBS), a tactile instrument worn by performers that allows the generation and manipulation of audio output using touch triggers. The RUBS system is a responsive interface between organic touch and electronic audio, intimately located on the performer{\&}{\#}8217;s body. This system offers an entry point into a more intuitive method of music performance. A short overview of body instrument philosophy and related work is followed by the development and implementation process of the RUBS as both an interface and performance instrument. Lastly, observations, design challenges and future goals are discussed.},
address = {Copenhagen, Denmark},
author = {Bhumber, Kiran and Pritchard, Bob and Rod{\'{e}}, Kitty},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
pages = {416--419},
publisher = {Aalborg University Copenhagen},
title = {{A Responsive User Body Suit (RUBS)}},
url = {http://www.nime.org/proceedings/2017/nime2017{\_}paper0078.pdf},
year = {2017}
}
@inproceedings{dhaddad:2017,
abstract = {We introduce a family of fragile electronic musical instruments designed to be "played'' through the act of destruction. Each Fragile Instrument consists of an analog synthesizing circuit with embedded sensors that detect the destruction of an outer shell, which is destroyed and replaced for each performance. Destruction plays an integral role in both the spectacle and the generated sounds. This paper presents several variations of Fragile Instruments we have created, discussing their circuit design as well as choices of material for the outer shell and tools of destruction. We conclude by considering other approaches to create intentionally destructible electronic musical instruments.},
address = {Copenhagen, Denmark},
author = {Haddad, Donald Derek and Xiao, Xiao and Machover, Tod and Paradiso, Joseph A},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
pages = {30--33},
publisher = {Aalborg University Copenhagen},
title = {{Fragile Instruments: Constructing Destructible Musical Interfaces}},
url = {http://www.nime.org/proceedings/2017/nime2017{\_}paper0006.pdf},
year = {2017}
}
@inproceedings{ablatherwick:2017,
abstract = {Music technology can provide unique opportunities to allow access to music making for those with complex needs in special educational needs (SEN) settings. Whilst there is a growing trend of research in this area, technology has been shown to face a variety of issues leading to underuse in this context. This paper reviews issues raised in literature and in practice for the use of music technology in SEN settings. The paper then reviews existing principles and frameworks for designing digital musical instruments (DMIs.) The reviews of literature and current frameworks are then used to inform a set of design considerations for instruments for users with complex needs, and in SEN settings. 18 design considerations are presented with connections to literature and practice. An implementation example including future work is presented, and finally a conclusion is then offered.},
address = {Copenhagen, Denmark},
author = {Blatherwick, Asha and Woodbury, Luke and Davis, Tom},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
pages = {216--221},
publisher = {Aalborg University Copenhagen},
title = {{Design Considerations for Instruments for Users with Complex Needs in SEN Settings}},
url = {http://www.nime.org/proceedings/2017/nime2017{\_}paper0040.pdf},
year = {2017}
}
@inproceedings{aleslie:2017,
abstract = {This paper describes a series of fashionable sounding shoe and foot based appendages made between 2007-2017. The research attempts to demake the physical high-heeled shoe through the iterative design and fabrication of new foot based musical instruments. This process of demaking also changes the usual purpose of shoes and associated stereotypes of high heeled shoe wear. Through turning high heeled shoes into wearable musical instruments for theatrical audio visual expressivity we question why so many musical instruments are made for the hands and not the feet? With this creative work we explore ways to redress the imbalance and consider what a genuinely {\&}{\#}8220;foot based{\&}{\#}8221; expressivity could be.},
address = {Copenhagen, Denmark},
author = {Murray-Leslie, Alexandra and Johnston, Andrew},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
pages = {296--301},
publisher = {Aalborg University Copenhagen},
title = {{The Liberation of the Feet: Demaking the High Heeled Shoe For Theatrical Audio-Visual Expression}},
url = {http://www.nime.org/proceedings/2017/nime2017{\_}paper0057.pdf},
year = {2017}
}
@inproceedings{ajensenius:2017,
abstract = {This paper explores sonic microinteraction using muscle sensing through the Myo armband. The first part presents results from a small series of experiments aimed at finding the baseline micromotion and muscle activation data of people being at rest or performing short/small actions. The second part presents the prototype instrument MicroMyo, built around the concept of making sound with little motion. The instrument plays with the convention that inputting more energy into an instrument results in more sound. MicroMyo, on the other hand, is built so that the less you move, the more it sounds. Our user study shows that while such an "inverse instrument" may seem puzzling at first, it also opens a space for interesting musical interactions.},
address = {Copenhagen, Denmark},
author = {Jensenius, Alexander Refsum and Sanchez, Victor Gonzalez and Zelechowska, Agata and Bjerkestrand, Kari Anne Vadstensvik},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
pages = {442--445},
publisher = {Aalborg University Copenhagen},
title = {{Exploring the Myo controller for sonic microinteraction}},
url = {http://www.nime.org/proceedings/2017/nime2017{\_}paper0083.pdf},
year = {2017}
}
